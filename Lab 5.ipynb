{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIlMQWUiUqxO"
      },
      "source": [
        "# Computer vision and deep learning - Laboratory 5\n",
        "\n",
        "In this laboratory, you will continue the semantic segmentation mini-project that we started last time. More specifically, you will write the CNN architecture, define the training loop, perform hyperparameter search and evaluate the best segmentation module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYMNqbIyVx0R",
        "outputId": "5c4bb4bc-c33d-4707-ad2d-9f8d85cf07c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import wget\n",
        "import glob\n",
        "import wandb\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from MyUnetModel import MyUNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cNuZRCCWBQo"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "The model that will be used in this laboratory is inspired by the [U-Net](https://arxiv.org/abs/1505.04597) architecture.\n",
        "U-Net is a fully convolutional neural network comprising two symmetric paths: a contracting path (to capture context) and an expanding path  (which enables precise localization).\n",
        "The network also uses skip connections between the corresponding layers in the downsampling path to the layer in the upsampling path, and thus directly fast-forwards high-resolution feature maps from the encoder to the decoder network.\n",
        "\n",
        "The output of the model is an volume with depth C, where C is the number of pixel classes. For example, if you want to classify the pixels into person and background, the output will be a volume of depth 2.\n",
        "If you want to classify the pixels into face, hair and background the output will be a volume of depth 3.\n",
        "\n",
        "**Read the U-Net paper and try to understand the architecture.**\n",
        "\n",
        "An overview of the U-Net architecture is depicted in the figure below:\n",
        "<img src=\"https://miro.medium.com/max/1400/1*J3t2b65ufsl1x6caf6GiBA.png\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKKi5UowzvjZ"
      },
      "source": [
        "## The downsampling path\n",
        "\n",
        "\n",
        "For the downsampling path we'll use a convolutional neural network from the pretrained torchvision models.\n",
        "We'll cover this in detail in the next laboratory session.\n",
        "\n",
        "\n",
        "## The upsamping path\n",
        "\n",
        "\n",
        "In the upsampling path, we'll use transposed convolutions to progressively increase the resolution of the activation maps. The layers for the transposed convolution is [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html).\n",
        "\n",
        "Let's write a function to implement an upsampling block, consisting of a transposed convolution, a batch normalization block and a ReLu activation.\n",
        "\n",
        "Remember, the output size $W_o$ of a transposed convolutional layer is:  \n",
        "\\begin{equation}\n",
        "W_o = (W_i - 1) \\cdot S - 2P + F\n",
        "\\end{equation},\n",
        "\n",
        "where $W_i$ is the size of the input, $S$ is the stride, $P$ is the amount of padding and $F$ is the filter size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eAyhAkgfCM4O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def upsample_block(x, filters, size, stride = 2):\n",
        "  \"\"\"\n",
        "  x - the input of the upsample block\n",
        "  filters - the number of filters to be applied\n",
        "  size - the size of the filters\n",
        "  \"\"\"\n",
        "\n",
        "  # TODO your code here\n",
        "  # transposed convolution\n",
        "  # BN\n",
        "  # relu activation\n",
        "  x = torch.nn.ConvTranspose2d(x.shape[1], filters, size, stride)(x)\n",
        "  x = torch.nn.BatchNorm2d(x.shape[1])(x)\n",
        "  x = torch.nn.ReLU()(x)\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOg9FKZmj7dH"
      },
      "source": [
        "Now let's test this upsampling block.\n",
        "Change the parameters of of this function and notice how the shape of the output changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDy7Qkzmj6Tv",
        "outputId": "c66b090f-2c73-4e44-ff57-4c9935227987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in shape:  torch.Size([32, 32, 128, 128])  upsample with filter size  4 ; stride  2  -> out shape  torch.Size([32, 16, 258, 258])\n",
            "in shape:  torch.Size([32, 32, 128, 128])  upsample with filter size  4 ; stride  4  -> out shape  torch.Size([32, 16, 512, 512])\n",
            "in shape:  torch.Size([32, 32, 128, 128])  upsample with filter size  4 ; stride  8  -> out shape  torch.Size([32, 16, 1020, 1020])\n"
          ]
        }
      ],
      "source": [
        "in_layer = torch.rand((32, 32, 128, 128))\n",
        "\n",
        "filter_sz = 4\n",
        "num_filters = 16\n",
        "\n",
        "for stride in [2, 4, 8]:\n",
        "  x = upsample_block(in_layer, num_filters, filter_sz, stride)\n",
        "  print('in shape: ', in_layer.shape, ' upsample with filter size ', filter_sz, '; stride ', stride, ' -> out shape ', x.shape)\n",
        "\n",
        "  # o = (128 - 1) * 2 + 4 = 258, 254*2 = 508 + 4 = 512, 1016 + 4 = 1020 ok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RyM2bNwkMqU"
      },
      "source": [
        "## The down-sampling path\n",
        "\n",
        "\n",
        "For the down-sampling path we'll use a classical convolutional neural network.\n",
        "\n",
        "Write a class Encoder which inherits from ``torch.nn.Module`` for the down-sampling path. The model will be composed of several blocks, each block comprising two convolutional layers (with filter size of 3) with a ``ReLu`` non-linearity between them. The blocks of the encoder will be separated by max pooling layers with a size of 2 and a stride of 2.\n",
        "\n",
        " ![encoder.PNG](data:image/png;base64,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)\n",
        "\n",
        "In the constructor of the Encoder class you will pass as parameter a list containing the number of channels used for each encoder block. In the forward function you will return a list containing the outputs of all the encoder blocks (youâ€™ll be using them for the connection between the down-sampling path and the up-sampling path).\n",
        "\n",
        "You can also use a pre-trained module from torchvision for this. You'll first load the pre-trained weights on ImageNet, and \"freeze\" these weights during the training process (set required_grad=False for those tensors). The problem is that to create the skip connections required by the U-Net architecture we need access to the feature maps of some intermediate layers in the network and these are not accessible by default.\n",
        "\n",
        "You should inspect the implementation of the CNN that you chose, make the Encoder class inherit from the model you chose, and in the forward function, return the output of the layers you selected for the skip connections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxNuI5PClOkB"
      },
      "source": [
        "## The up-sampling path (Decoder)\n",
        "\n",
        "In the upsampling path, we'll use transposed convolutions to progressively increase the resolution of the activation maps. The layers for the transposed convolution is [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html).\n",
        "\n",
        "Let's write a function to implement an upsampling block, consisting of a transposed convolution, a batch normalization block and a ReLu activation.\n",
        "\n",
        "Remember, the output size $W_o$ of a transposed convolutional layer is:  \n",
        "\\begin{equation}\n",
        "W_o = (W_i - 1) \\cdot S - 2P + F\n",
        "\\end{equation},\n",
        "\n",
        "where $W_i$ is the size of the input, $S$ is the stride, $P$ is the amount of padding and $F$ is the filter size.\n",
        "\n",
        "\n",
        "Similarly to the encoder, the decoder will be composed of several blocks, and each block will consist in a transposed convolution (followed by a batch normalization layer and a ReLU non-linearity) and then by an encoder block.  \n",
        "\n",
        " ![decoder.PNG](data:image/png;base64,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)\n",
        "\n",
        "Write a class Decoder  which inherits from ``torch.nn.Module`` to implement the up-samping path; the constructor of this class will get as parameter the depth of each decoder module, and in the forward function you will have two parameters: the input feature map and a list of activations from the encoder (for the skip connections).  \n",
        "\n",
        "\n",
        "\n",
        "In the forward function, for each block:\n",
        "\n",
        "- apply the up-sampling operation (followed by batch normalization and ReLU);\n",
        "\n",
        "- crop the corresponding activation map from the encoder (use CenterCrop) such that is has the same size as the decoder block;\n",
        "\n",
        "- concatenate these two activation maps (on the channel dimension);\n",
        "\n",
        "- apply an encoder block on the result;\n",
        "\n",
        "- pass the result to the next decoder block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oARoiKOEr_38"
      },
      "source": [
        "# Simpler implementation\n",
        "\n",
        "Alternatively, you can choose to implement the entire architecture into a single class.\n",
        "\n",
        "In the UNet diagram above, the arrows represent the layers in your network.\n",
        "The blue rectangles represent the feature maps: the number on top of the rectangle is the number of channels of the feature map, while the numbers from the lower left is the spatial shape of the feature map (for example, $572 \\times 572$ - is a feature map of height and width $572$)\n",
        "\n",
        "```\n",
        "class UNet(torch.nn.Module):\n",
        "def __init__(num_classes):\n",
        "    super().__init__()\n",
        "    # TODO your code here create all the UNET layers\n",
        "    # enc_11 -> the first convolutional layer from the first encoder block\n",
        "    # the image is a color image, so the number of channels is 3, while according to the UNet architecture above, the number of output channels is 64\n",
        "    self.enc_11 = torch.nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3)\n",
        "    # enc_12 -> the second convolutional layer from the first encoder block\n",
        "    # 64 input_channels (the input of enc_11 will be the output of enc_11 that has 64 filters),  \n",
        "    self.enc_12 = torch.nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n",
        "    # etc etc\n",
        "   \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading dataset, creating dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "147 37\n",
            "torch.Size([3, 250, 250])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\ML\\ml_venv_python3.8\\lib\\site-packages\\torch\\utils\\data\\dataset.py:414: UserWarning: Length of split at index 2 is 0. This might result in an empty dataset.\n",
            "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
          ]
        }
      ],
      "source": [
        "from LFWDataset import LFWDataset\n",
        "from pathlib import Path\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "dataset = LFWDataset(\n",
        "    Path('D:\\ML\\cv_dl\\lfw_dataset'), \n",
        "    transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.to(\"cuda\")),\n",
        "    ])\n",
        ")\n",
        "\n",
        "random_gen = torch.Generator().manual_seed(69)\n",
        "train_data, test_data, _= random_split(dataset, [.8, .2, .0], random_gen)\n",
        "\n",
        "batch_size = 16\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False) \n",
        "\n",
        "print(len(train_dataloader), len(test_dataloader))\n",
        "print(dataset.__getitem__(0)[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Unet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([15, 3, 250, 250])\n",
            "MyUNet(\n",
            "  (encoder): Encoder(\n",
            "    (blocks): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (1): ReLU()\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (3): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (1): ReLU()\n",
            "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (3): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (1): ReLU()\n",
            "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (3): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (1): ReLU()\n",
            "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (3): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (1): ReLU()\n",
            "          (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "          (3): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (decoder_blocks): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (encoder_blocks): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (finalFC): Sequential(\n",
            "    (0): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from MyUnetModel import MyUNet\n",
        "\n",
        "num_classes = 3\n",
        "encoder_channels = [3, 64, 128, 256, 512, 1024]\n",
        "decoder_channels = [1024, 512, 256, 128, 64]\n",
        "\n",
        "unet_model = MyUNet(num_classes, encoder_channels, decoder_channels)\n",
        "unet_model.to('cuda')\n",
        "\n",
        "testX = torch.rand(15, 3, 250, 250).to('cuda')\n",
        "output = unet_model(testX)\n",
        "print(output.shape)\n",
        "print(unet_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([15, 3, 250, 250])\n"
          ]
        }
      ],
      "source": [
        "class StupidNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 32, padding='same', kernel_size=3),\n",
        "            torch.nn.Conv2d(32, 32, padding='same', kernel_size=3)\n",
        "        )\n",
        "        self.finalFC = torch.nn.Conv2d(32, 3, padding='same', kernel_size=3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = self.finalFC(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "stupid_model = StupidNet()\n",
        "stupid_model.to('cuda')\n",
        "\n",
        "testX = torch.rand(15, 3, 250, 250).to('cuda')\n",
        "output = stupid_model(testX)\n",
        "print(output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh3gmb0HlmmW"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "Finally, you should write the UNet class which defines the semantic segmentation model. In this class you will use and connect the Encoder and Decoder classes that you previously wrote. The output of the segmentation module will be a volume with as many channels as the number of classes from the segmentation problem. Basically, each channel c from the output will be a map that stores the probability of each pixel to belong to the class c.\n",
        "\n",
        "Apply a 1x1 convolution with c channels on the decoder output to obtain the segmentation map.\n",
        "\n",
        "Finally, to have matching shapes between the network output (segmentation map) and the ground truth data, resize the segmentation map using ``torch.nn.functional.interpolate``.\n",
        "\n",
        "\n",
        "### The training loop\n",
        "\n",
        "\n",
        "Now, itâ€™s time to write the training loop.  \n",
        "\n",
        "You remember the steps from the previous labs. You need a train and a test DataLoader and you must first define the loss function cross entropy and select an optimizer.\n",
        "\n",
        "Then:\n",
        "\n",
        "- Get a batch of training data from the DataLoader\n",
        "- Zero out the optimizerâ€™s gradients\n",
        "- Perform the forward pass\n",
        "- Calculate and store the loss and the accuracy based on the predictions and the labels from the dataset\n",
        "- Tell the optimizer to perform one learning step - that is, adjust the modelâ€™s learning weights based on the observed gradients for this batch, according to the optimization algorithm we chose\n",
        "- Evaluate the model on the validation/test set. Store the loss and the accuracy on the validation/test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbejan-andrei-personal\u001b[0m (\u001b[33mdeiubejan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>d:\\ML\\cv_dl\\wandb\\run-20240110_234451-l5gaf73s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deiubejan/cv-dl/runs/l5gaf73s' target=\"_blank\">rich-hill-22</a></strong> to <a href='https://wandb.ai/deiubejan/cv-dl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/deiubejan/cv-dl' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/deiubejan/cv-dl/runs/l5gaf73s' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl/runs/l5gaf73s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0:\n",
            "batch: 0 loss: 1.7481364011764526\n",
            "batch: 1 loss: 1.5688786506652832\n",
            "batch: 2 loss: 1.3305222988128662\n",
            "batch: 3 loss: 1.3387815952301025\n",
            "batch: 4 loss: 1.27056884765625\n",
            "batch: 5 loss: 1.3115684986114502\n",
            "batch: 6 loss: 1.2546135187149048\n",
            "batch: 7 loss: 1.2546207904815674\n",
            "batch: 8 loss: 1.2819997072219849\n",
            "batch: 9 loss: 1.1591191291809082\n",
            "batch: 10 loss: 1.2005635499954224\n",
            "batch: 11 loss: 1.2345664501190186\n",
            "batch: 12 loss: 1.0641651153564453\n",
            "batch: 13 loss: 1.093583345413208\n",
            "batch: 14 loss: 1.1422653198242188\n",
            "batch: 15 loss: 1.033637285232544\n",
            "batch: 16 loss: 1.2008408308029175\n",
            "batch: 17 loss: 1.0517992973327637\n",
            "batch: 18 loss: 1.0581486225128174\n",
            "batch: 19 loss: 0.9113483428955078\n",
            "batch: 20 loss: 1.1418979167938232\n",
            "batch: 21 loss: 0.951263964176178\n",
            "batch: 22 loss: 0.9863956570625305\n",
            "batch: 23 loss: 0.9943786263465881\n",
            "batch: 24 loss: 0.929864764213562\n",
            "batch: 25 loss: 1.0062952041625977\n",
            "batch: 26 loss: 0.8595654964447021\n",
            "batch: 27 loss: 0.8106796145439148\n",
            "batch: 28 loss: 1.0908088684082031\n",
            "batch: 29 loss: 0.9360780119895935\n",
            "batch: 30 loss: 1.0162878036499023\n",
            "batch: 31 loss: 0.9580169320106506\n",
            "batch: 32 loss: 0.8704209327697754\n",
            "batch: 33 loss: 1.096267580986023\n",
            "batch: 34 loss: 0.9272079467773438\n",
            "batch: 35 loss: 1.0206658840179443\n",
            "batch: 36 loss: 0.8497157692909241\n",
            "batch: 37 loss: 0.8562936186790466\n",
            "batch: 38 loss: 0.8131203651428223\n",
            "batch: 39 loss: 0.8301880359649658\n",
            "batch: 40 loss: 0.9970679879188538\n",
            "batch: 41 loss: 0.954330563545227\n",
            "batch: 42 loss: 0.9781354665756226\n",
            "batch: 43 loss: 0.9236586689949036\n",
            "batch: 44 loss: 0.9057925343513489\n",
            "batch: 45 loss: 0.7951846122741699\n",
            "batch: 46 loss: 0.843531608581543\n",
            "batch: 47 loss: 0.885986864566803\n",
            "batch: 48 loss: 0.6447810530662537\n",
            "batch: 49 loss: 0.7680118083953857\n",
            "batch: 50 loss: 0.7470039129257202\n",
            "batch: 51 loss: 0.89545077085495\n",
            "batch: 52 loss: 0.8097110390663147\n",
            "batch: 53 loss: 0.9622520208358765\n",
            "batch: 54 loss: 0.8466214537620544\n",
            "batch: 55 loss: 0.7823652625083923\n",
            "batch: 56 loss: 0.931391179561615\n",
            "batch: 57 loss: 0.8990829586982727\n",
            "batch: 58 loss: 0.859656810760498\n",
            "batch: 59 loss: 0.7527669072151184\n",
            "batch: 60 loss: 0.6260203123092651\n",
            "batch: 61 loss: 0.7972658276557922\n",
            "batch: 62 loss: 0.7294290065765381\n",
            "batch: 63 loss: 0.8139252662658691\n",
            "batch: 64 loss: 0.6652653217315674\n",
            "batch: 65 loss: 0.9223365187644958\n",
            "batch: 66 loss: 1.0985729694366455\n",
            "batch: 67 loss: 0.7626522779464722\n",
            "batch: 68 loss: 0.9619947671890259\n",
            "batch: 69 loss: 0.6346522569656372\n",
            "batch: 70 loss: 0.6417958736419678\n",
            "batch: 71 loss: 0.7706934809684753\n",
            "batch: 72 loss: 0.693211019039154\n",
            "batch: 73 loss: 0.7726681232452393\n",
            "batch: 74 loss: 0.6577092409133911\n",
            "batch: 75 loss: 0.6742088794708252\n",
            "batch: 76 loss: 0.6734025478363037\n",
            "batch: 77 loss: 0.7929169535636902\n",
            "batch: 78 loss: 0.5641186237335205\n",
            "batch: 79 loss: 0.8442238569259644\n",
            "batch: 80 loss: 0.6951611042022705\n",
            "batch: 81 loss: 0.6742459535598755\n",
            "batch: 82 loss: 0.667544960975647\n",
            "batch: 83 loss: 0.7340328097343445\n",
            "batch: 84 loss: 0.5823568105697632\n",
            "batch: 85 loss: 0.5749632716178894\n",
            "batch: 86 loss: 0.6629407405853271\n",
            "batch: 87 loss: 0.5636192560195923\n",
            "batch: 88 loss: 0.6439376473426819\n",
            "batch: 89 loss: 0.7146521806716919\n",
            "batch: 90 loss: 0.7192360758781433\n",
            "batch: 91 loss: 0.6519222259521484\n",
            "batch: 92 loss: 0.6614827513694763\n",
            "batch: 93 loss: 0.7183215022087097\n",
            "batch: 94 loss: 0.6387189626693726\n",
            "batch: 95 loss: 0.6657689809799194\n",
            "batch: 96 loss: 0.5545127391815186\n",
            "batch: 97 loss: 0.6997774839401245\n",
            "batch: 98 loss: 0.5587550401687622\n",
            "batch: 99 loss: 0.6656218767166138\n",
            "batch: 100 loss: 0.5622893571853638\n",
            "batch: 101 loss: 0.5723928213119507\n",
            "batch: 102 loss: 0.4541915953159332\n",
            "batch: 103 loss: 0.49256670475006104\n",
            "batch: 104 loss: 0.6139464974403381\n",
            "batch: 105 loss: 0.585375189781189\n",
            "batch: 106 loss: 0.6179811954498291\n",
            "batch: 107 loss: 0.6231966614723206\n",
            "batch: 108 loss: 0.6000446081161499\n",
            "batch: 109 loss: 0.6948533654212952\n",
            "batch: 110 loss: 0.6034121513366699\n",
            "batch: 111 loss: 0.6924660205841064\n",
            "batch: 112 loss: 0.5766851902008057\n",
            "batch: 113 loss: 0.692283034324646\n",
            "batch: 114 loss: 0.6251800656318665\n",
            "batch: 115 loss: 0.6165364980697632\n",
            "batch: 116 loss: 0.5310811996459961\n",
            "batch: 117 loss: 0.5991190671920776\n",
            "batch: 118 loss: 0.5712961554527283\n",
            "batch: 119 loss: 0.5046572685241699\n",
            "batch: 120 loss: 0.5342028737068176\n",
            "batch: 121 loss: 0.6583309173583984\n",
            "batch: 122 loss: 0.7035965919494629\n",
            "batch: 123 loss: 0.6604111790657043\n",
            "batch: 124 loss: 0.782526969909668\n",
            "batch: 125 loss: 0.5651935935020447\n",
            "batch: 126 loss: 0.5711183547973633\n",
            "batch: 127 loss: 0.7097387313842773\n",
            "batch: 128 loss: 0.5940414071083069\n",
            "batch: 129 loss: 0.5045408606529236\n",
            "batch: 130 loss: 0.7600936889648438\n",
            "batch: 131 loss: 0.6610223054885864\n",
            "batch: 132 loss: 0.5397295355796814\n",
            "batch: 133 loss: 0.6719081401824951\n",
            "batch: 134 loss: 0.5770455002784729\n",
            "batch: 135 loss: 0.5537810325622559\n",
            "batch: 136 loss: 0.7138640284538269\n",
            "batch: 137 loss: 0.6148345470428467\n",
            "batch: 138 loss: 0.7098425626754761\n",
            "batch: 139 loss: 0.6429412364959717\n",
            "batch: 140 loss: 0.5942658185958862\n",
            "batch: 141 loss: 0.6251643896102905\n",
            "batch: 142 loss: 0.5922738313674927\n",
            "batch: 143 loss: 0.6037566661834717\n",
            "batch: 144 loss: 0.6324060559272766\n",
            "batch: 145 loss: 0.6909070611000061\n",
            "batch: 146 loss: 0.9132952094078064\n",
            "LOSS train 0.7182210683822632 valid 0.7221839427947998\n",
            "EPOCH 1:\n",
            "batch: 0 loss: 0.5907822251319885\n",
            "batch: 1 loss: 0.5313572883605957\n",
            "batch: 2 loss: 0.5775410532951355\n",
            "batch: 3 loss: 0.6532972455024719\n",
            "batch: 4 loss: 0.7110241651535034\n",
            "batch: 5 loss: 0.565361499786377\n",
            "batch: 6 loss: 0.5998884439468384\n",
            "batch: 7 loss: 0.49629902839660645\n",
            "batch: 8 loss: 0.45707017183303833\n",
            "batch: 9 loss: 0.5442903637886047\n",
            "batch: 10 loss: 0.41803571581840515\n",
            "batch: 11 loss: 0.5141209363937378\n",
            "batch: 12 loss: 0.7412782311439514\n",
            "batch: 13 loss: 0.5877121686935425\n",
            "batch: 14 loss: 0.42785605788230896\n",
            "batch: 15 loss: 0.489284873008728\n",
            "batch: 16 loss: 0.5815774202346802\n",
            "batch: 17 loss: 0.5009372234344482\n",
            "batch: 18 loss: 0.6540402173995972\n",
            "batch: 19 loss: 0.5784437656402588\n",
            "batch: 20 loss: 0.5234634876251221\n",
            "batch: 21 loss: 0.5266505479812622\n",
            "batch: 22 loss: 0.6836962699890137\n",
            "batch: 23 loss: 0.5229768753051758\n",
            "batch: 24 loss: 0.49148988723754883\n",
            "batch: 25 loss: 0.5589174032211304\n",
            "batch: 26 loss: 0.5477489233016968\n",
            "batch: 27 loss: 0.5164065957069397\n",
            "batch: 28 loss: 0.6455070972442627\n",
            "batch: 29 loss: 0.5533304214477539\n",
            "batch: 30 loss: 0.5411062240600586\n",
            "batch: 31 loss: 0.463702529668808\n",
            "batch: 32 loss: 0.6377832889556885\n",
            "batch: 33 loss: 0.506513774394989\n",
            "batch: 34 loss: 0.5587708950042725\n",
            "batch: 35 loss: 0.670218288898468\n",
            "batch: 36 loss: 0.5264958739280701\n",
            "batch: 37 loss: 0.5392385721206665\n",
            "batch: 38 loss: 0.5006467700004578\n",
            "batch: 39 loss: 0.5760351419448853\n",
            "batch: 40 loss: 0.45365461707115173\n",
            "batch: 41 loss: 0.6710034608840942\n",
            "batch: 42 loss: 0.48822587728500366\n",
            "batch: 43 loss: 0.685673713684082\n",
            "batch: 44 loss: 0.5558087825775146\n",
            "batch: 45 loss: 0.6774032115936279\n",
            "batch: 46 loss: 0.4845305383205414\n",
            "batch: 47 loss: 0.508493185043335\n",
            "batch: 48 loss: 0.6557159423828125\n",
            "batch: 49 loss: 0.6517323851585388\n",
            "batch: 50 loss: 0.5198720693588257\n",
            "batch: 51 loss: 0.5521826148033142\n",
            "batch: 52 loss: 0.6079256534576416\n",
            "batch: 53 loss: 0.5183171033859253\n",
            "batch: 54 loss: 0.5896787047386169\n",
            "batch: 55 loss: 0.6449539661407471\n",
            "batch: 56 loss: 0.45437270402908325\n",
            "batch: 57 loss: 0.5740755796432495\n",
            "batch: 58 loss: 0.5616910457611084\n",
            "batch: 59 loss: 0.6232002377510071\n",
            "batch: 60 loss: 0.484153151512146\n",
            "batch: 61 loss: 0.6578480005264282\n",
            "batch: 62 loss: 0.5595667958259583\n",
            "batch: 63 loss: 0.6001709699630737\n",
            "batch: 64 loss: 0.4397954046726227\n",
            "batch: 65 loss: 0.5735853910446167\n",
            "batch: 66 loss: 0.501915693283081\n",
            "batch: 67 loss: 0.8249284029006958\n",
            "batch: 68 loss: 0.5787080526351929\n",
            "batch: 69 loss: 0.8389637470245361\n",
            "batch: 70 loss: 0.6288248300552368\n",
            "batch: 71 loss: 0.6220797300338745\n",
            "batch: 72 loss: 0.4840317964553833\n",
            "batch: 73 loss: 0.4805896282196045\n",
            "batch: 74 loss: 0.5968046188354492\n",
            "batch: 75 loss: 0.6110076904296875\n",
            "batch: 76 loss: 0.48474788665771484\n",
            "batch: 77 loss: 0.6070814728736877\n",
            "batch: 78 loss: 0.5141370296478271\n",
            "batch: 79 loss: 0.5639947056770325\n",
            "batch: 80 loss: 0.5015203952789307\n",
            "batch: 81 loss: 0.5535228252410889\n",
            "batch: 82 loss: 0.4551820456981659\n",
            "batch: 83 loss: 0.4891652464866638\n",
            "batch: 84 loss: 0.44322657585144043\n",
            "batch: 85 loss: 0.528871476650238\n",
            "batch: 86 loss: 0.4441553056240082\n",
            "batch: 87 loss: 0.3944662809371948\n",
            "batch: 88 loss: 0.4866562783718109\n",
            "batch: 89 loss: 0.5925290584564209\n",
            "batch: 90 loss: 0.6143416166305542\n",
            "batch: 91 loss: 0.6271150708198547\n",
            "batch: 92 loss: 0.4526771008968353\n",
            "batch: 93 loss: 0.4218142628669739\n",
            "batch: 94 loss: 0.5162135362625122\n",
            "batch: 95 loss: 0.40930452942848206\n",
            "batch: 96 loss: 0.5465219020843506\n",
            "batch: 97 loss: 0.5252600908279419\n",
            "batch: 98 loss: 0.5793103575706482\n",
            "batch: 99 loss: 0.5033949613571167\n",
            "batch: 100 loss: 0.39849454164505005\n",
            "batch: 101 loss: 0.5050206184387207\n",
            "batch: 102 loss: 0.48962318897247314\n",
            "batch: 103 loss: 0.5377907156944275\n",
            "batch: 104 loss: 0.5434179306030273\n",
            "batch: 105 loss: 0.42223086953163147\n",
            "batch: 106 loss: 0.5013701319694519\n",
            "batch: 107 loss: 0.5089010000228882\n",
            "batch: 108 loss: 0.4581180214881897\n",
            "batch: 109 loss: 0.45561644434928894\n",
            "batch: 110 loss: 0.7418304681777954\n",
            "batch: 111 loss: 0.560598611831665\n",
            "batch: 112 loss: 0.46917301416397095\n",
            "batch: 113 loss: 0.5235069990158081\n",
            "batch: 114 loss: 0.5733299255371094\n",
            "batch: 115 loss: 0.6505581140518188\n",
            "batch: 116 loss: 0.48299896717071533\n",
            "batch: 117 loss: 0.5521876811981201\n",
            "batch: 118 loss: 0.39937883615493774\n",
            "batch: 119 loss: 0.37464815378189087\n",
            "batch: 120 loss: 0.6007527709007263\n",
            "batch: 121 loss: 0.5003609657287598\n",
            "batch: 122 loss: 0.4671213924884796\n",
            "batch: 123 loss: 0.3999880850315094\n",
            "batch: 124 loss: 0.4989185631275177\n",
            "batch: 125 loss: 0.4410683512687683\n",
            "batch: 126 loss: 0.5264695882797241\n",
            "batch: 127 loss: 0.4695558249950409\n",
            "batch: 128 loss: 0.5062476396560669\n",
            "batch: 129 loss: 0.5247635841369629\n",
            "batch: 130 loss: 0.48493656516075134\n",
            "batch: 131 loss: 0.5073453187942505\n",
            "batch: 132 loss: 0.5143958926200867\n",
            "batch: 133 loss: 0.34823113679885864\n",
            "batch: 134 loss: 0.9177761077880859\n",
            "batch: 135 loss: 0.4837411046028137\n",
            "batch: 136 loss: 0.4412878751754761\n",
            "batch: 137 loss: 0.5970250368118286\n",
            "batch: 138 loss: 0.5157853364944458\n",
            "batch: 139 loss: 0.5734328031539917\n",
            "batch: 140 loss: 0.5964442491531372\n",
            "batch: 141 loss: 0.4303230047225952\n",
            "batch: 142 loss: 0.5965056419372559\n",
            "batch: 143 loss: 0.46567800641059875\n",
            "batch: 144 loss: 0.5973895192146301\n",
            "batch: 145 loss: 0.4280455410480499\n",
            "batch: 146 loss: 0.5785551071166992\n",
            "LOSS train 0.5108136534690857 valid 0.5063004493713379\n",
            "EPOCH 2:\n",
            "batch: 0 loss: 0.46384426951408386\n",
            "batch: 1 loss: 0.552103579044342\n",
            "batch: 2 loss: 0.5227252244949341\n",
            "batch: 3 loss: 0.47157782316207886\n",
            "batch: 4 loss: 0.4928562641143799\n",
            "batch: 5 loss: 0.40159276127815247\n",
            "batch: 6 loss: 0.5607951283454895\n",
            "batch: 7 loss: 0.3580704927444458\n",
            "batch: 8 loss: 0.4703138768672943\n",
            "batch: 9 loss: 0.39606955647468567\n",
            "batch: 10 loss: 0.700995922088623\n",
            "batch: 11 loss: 0.48276081681251526\n",
            "batch: 12 loss: 0.42633599042892456\n",
            "batch: 13 loss: 0.4669366180896759\n",
            "batch: 14 loss: 0.5103551149368286\n",
            "batch: 15 loss: 0.488564670085907\n",
            "batch: 16 loss: 0.5392807722091675\n",
            "batch: 17 loss: 0.4949265718460083\n",
            "batch: 18 loss: 0.4739682078361511\n",
            "batch: 19 loss: 0.4727489650249481\n",
            "batch: 20 loss: 0.41085562109947205\n",
            "batch: 21 loss: 0.3780291974544525\n",
            "batch: 22 loss: 0.4825000762939453\n",
            "batch: 23 loss: 0.49715176224708557\n",
            "batch: 24 loss: 0.37128788232803345\n",
            "batch: 25 loss: 0.5257735252380371\n",
            "batch: 26 loss: 0.44921573996543884\n",
            "batch: 27 loss: 0.5061184167861938\n",
            "batch: 28 loss: 0.4722903072834015\n",
            "batch: 29 loss: 0.3719213008880615\n",
            "batch: 30 loss: 0.4728342592716217\n",
            "batch: 31 loss: 0.5224745273590088\n",
            "batch: 32 loss: 0.5698933005332947\n",
            "batch: 33 loss: 0.3910646140575409\n",
            "batch: 34 loss: 0.5950710773468018\n",
            "batch: 35 loss: 0.509548008441925\n",
            "batch: 36 loss: 0.6066190004348755\n",
            "batch: 37 loss: 0.543881893157959\n",
            "batch: 38 loss: 0.4912336766719818\n",
            "batch: 39 loss: 0.5016437768936157\n",
            "batch: 40 loss: 0.45879098773002625\n",
            "batch: 41 loss: 0.5256325006484985\n",
            "batch: 42 loss: 0.4359683394432068\n",
            "batch: 43 loss: 0.36508554220199585\n",
            "batch: 44 loss: 0.3953252136707306\n",
            "batch: 45 loss: 0.45407435297966003\n",
            "batch: 46 loss: 0.42169636487960815\n",
            "batch: 47 loss: 0.5830776691436768\n",
            "batch: 48 loss: 0.34833091497421265\n",
            "batch: 49 loss: 0.39352744817733765\n",
            "batch: 50 loss: 0.4108234941959381\n",
            "batch: 51 loss: 0.4563056230545044\n",
            "batch: 52 loss: 0.4312315583229065\n",
            "batch: 53 loss: 0.499256432056427\n",
            "batch: 54 loss: 0.4406995177268982\n",
            "batch: 55 loss: 0.2973639667034149\n",
            "batch: 56 loss: 0.4572758674621582\n",
            "batch: 57 loss: 0.4174809753894806\n",
            "batch: 58 loss: 0.3837006688117981\n",
            "batch: 59 loss: 0.6615458726882935\n",
            "batch: 60 loss: 0.4750889539718628\n",
            "batch: 61 loss: 0.3704644441604614\n",
            "batch: 62 loss: 0.4778512418270111\n",
            "batch: 63 loss: 0.47624891996383667\n",
            "batch: 64 loss: 0.5387742519378662\n",
            "batch: 65 loss: 0.48518621921539307\n",
            "batch: 66 loss: 0.439792275428772\n",
            "batch: 67 loss: 0.36571255326271057\n",
            "batch: 68 loss: 0.4161168038845062\n",
            "batch: 69 loss: 0.4275088608264923\n",
            "batch: 70 loss: 0.5725136995315552\n",
            "batch: 71 loss: 0.4257606863975525\n",
            "batch: 72 loss: 0.4873163104057312\n",
            "batch: 73 loss: 0.5258256196975708\n",
            "batch: 74 loss: 0.3505934774875641\n",
            "batch: 75 loss: 0.5860140323638916\n",
            "batch: 76 loss: 0.3367396593093872\n",
            "batch: 77 loss: 0.5210707187652588\n",
            "batch: 78 loss: 0.5474203824996948\n",
            "batch: 79 loss: 0.38479676842689514\n",
            "batch: 80 loss: 0.4455745220184326\n",
            "batch: 81 loss: 0.44377854466438293\n",
            "batch: 82 loss: 0.5794384479522705\n",
            "batch: 83 loss: 0.4807993471622467\n",
            "batch: 84 loss: 0.4571632742881775\n",
            "batch: 85 loss: 0.4390488862991333\n",
            "batch: 86 loss: 0.5416277647018433\n",
            "batch: 87 loss: 0.424602746963501\n",
            "batch: 88 loss: 0.4742972254753113\n",
            "batch: 89 loss: 0.5085701942443848\n",
            "batch: 90 loss: 0.537718653678894\n",
            "batch: 91 loss: 0.38631758093833923\n",
            "batch: 92 loss: 0.47751104831695557\n",
            "batch: 93 loss: 0.3361686170101166\n",
            "batch: 94 loss: 0.4260481297969818\n",
            "batch: 95 loss: 0.4121803641319275\n",
            "batch: 96 loss: 0.621465802192688\n",
            "batch: 97 loss: 0.49050280451774597\n",
            "batch: 98 loss: 0.4651438295841217\n",
            "batch: 99 loss: 0.45550164580345154\n",
            "batch: 100 loss: 0.39742371439933777\n",
            "batch: 101 loss: 0.5370770692825317\n",
            "batch: 102 loss: 0.4590717852115631\n",
            "batch: 103 loss: 0.47213679552078247\n",
            "batch: 104 loss: 0.3586912155151367\n",
            "batch: 105 loss: 0.3655499219894409\n",
            "batch: 106 loss: 0.38985300064086914\n",
            "batch: 107 loss: 0.38185417652130127\n",
            "batch: 108 loss: 0.37620797753334045\n",
            "batch: 109 loss: 0.38411232829093933\n",
            "batch: 110 loss: 0.4899199903011322\n",
            "batch: 111 loss: 0.49836283922195435\n",
            "batch: 112 loss: 0.44879350066185\n",
            "batch: 113 loss: 0.49393731355667114\n",
            "batch: 114 loss: 0.3832639455795288\n",
            "batch: 115 loss: 0.5866031050682068\n",
            "batch: 116 loss: 0.42543521523475647\n",
            "batch: 117 loss: 0.5848022699356079\n",
            "batch: 118 loss: 0.32043588161468506\n",
            "batch: 119 loss: 0.3850022554397583\n",
            "batch: 120 loss: 0.42427289485931396\n",
            "batch: 121 loss: 0.42039623856544495\n",
            "batch: 122 loss: 0.4236697852611542\n",
            "batch: 123 loss: 0.5508543252944946\n",
            "batch: 124 loss: 0.4029248058795929\n",
            "batch: 125 loss: 0.44335103034973145\n",
            "batch: 126 loss: 0.3914634585380554\n",
            "batch: 127 loss: 0.3919867277145386\n",
            "batch: 128 loss: 0.39931443333625793\n",
            "batch: 129 loss: 0.4591692090034485\n",
            "batch: 130 loss: 0.4296153485774994\n",
            "batch: 131 loss: 0.557023823261261\n",
            "batch: 132 loss: 0.40110743045806885\n",
            "batch: 133 loss: 0.4641885757446289\n",
            "batch: 134 loss: 0.47941145300865173\n",
            "batch: 135 loss: 0.5661360621452332\n",
            "batch: 136 loss: 0.4780336320400238\n",
            "batch: 137 loss: 0.48645201325416565\n",
            "batch: 138 loss: 0.4226394593715668\n",
            "batch: 139 loss: 0.7347748875617981\n",
            "batch: 140 loss: 0.5189228057861328\n",
            "batch: 141 loss: 0.4263326823711395\n",
            "batch: 142 loss: 0.5163060426712036\n",
            "batch: 143 loss: 0.4318007528781891\n",
            "batch: 144 loss: 0.6626471281051636\n",
            "batch: 145 loss: 0.5551046133041382\n",
            "batch: 146 loss: 0.7905524969100952\n",
            "LOSS train 0.545279860496521 valid 0.544617772102356\n",
            "EPOCH 3:\n",
            "batch: 0 loss: 0.4157980978488922\n",
            "batch: 1 loss: 0.4364863932132721\n",
            "batch: 2 loss: 0.5376702547073364\n",
            "batch: 3 loss: 0.490810751914978\n",
            "batch: 4 loss: 0.4395560026168823\n",
            "batch: 5 loss: 0.3438596725463867\n",
            "batch: 6 loss: 0.3854103982448578\n",
            "batch: 7 loss: 0.594228208065033\n",
            "batch: 8 loss: 0.4460385739803314\n",
            "batch: 9 loss: 0.42089203000068665\n",
            "batch: 10 loss: 0.47477707266807556\n",
            "batch: 11 loss: 0.3782581686973572\n",
            "batch: 12 loss: 0.434011846780777\n",
            "batch: 13 loss: 0.47459936141967773\n",
            "batch: 14 loss: 0.42520153522491455\n",
            "batch: 15 loss: 0.5937941670417786\n",
            "batch: 16 loss: 0.4858838617801666\n",
            "batch: 17 loss: 0.45477423071861267\n",
            "batch: 18 loss: 0.4860951006412506\n",
            "batch: 19 loss: 0.3829008936882019\n",
            "batch: 20 loss: 0.4233970642089844\n",
            "batch: 21 loss: 0.4132764935493469\n",
            "batch: 22 loss: 0.4283425807952881\n",
            "batch: 23 loss: 0.43797484040260315\n",
            "batch: 24 loss: 0.48342716693878174\n",
            "batch: 25 loss: 0.3331301808357239\n",
            "batch: 26 loss: 0.33176544308662415\n",
            "batch: 27 loss: 0.6389333009719849\n",
            "batch: 28 loss: 0.4407710134983063\n",
            "batch: 29 loss: 0.4344339072704315\n",
            "batch: 30 loss: 0.603711724281311\n",
            "batch: 31 loss: 0.37124893069267273\n",
            "batch: 32 loss: 0.4144097864627838\n",
            "batch: 33 loss: 0.4395657777786255\n",
            "batch: 34 loss: 0.4247170388698578\n",
            "batch: 35 loss: 0.3573610186576843\n",
            "batch: 36 loss: 0.49098196625709534\n",
            "batch: 37 loss: 0.41870537400245667\n",
            "batch: 38 loss: 0.407725065946579\n",
            "batch: 39 loss: 0.44497159123420715\n",
            "batch: 40 loss: 0.4851215183734894\n",
            "batch: 41 loss: 0.388923317193985\n",
            "batch: 42 loss: 0.43379053473472595\n",
            "batch: 43 loss: 0.4261203110218048\n",
            "batch: 44 loss: 0.525973379611969\n",
            "batch: 45 loss: 0.34435901045799255\n",
            "batch: 46 loss: 0.5251861810684204\n",
            "batch: 47 loss: 0.42560991644859314\n",
            "batch: 48 loss: 0.5458637475967407\n",
            "batch: 49 loss: 0.43554410338401794\n",
            "batch: 50 loss: 0.40841686725616455\n",
            "batch: 51 loss: 0.41852113604545593\n",
            "batch: 52 loss: 0.37825649976730347\n",
            "batch: 53 loss: 0.32607072591781616\n",
            "batch: 54 loss: 0.4459168016910553\n",
            "batch: 55 loss: 0.4499494433403015\n",
            "batch: 56 loss: 0.39679908752441406\n",
            "batch: 57 loss: 0.442096471786499\n",
            "batch: 58 loss: 0.5317755341529846\n",
            "batch: 59 loss: 0.47663414478302\n",
            "batch: 60 loss: 0.3880384862422943\n",
            "batch: 61 loss: 0.40087229013442993\n",
            "batch: 62 loss: 0.41006746888160706\n",
            "batch: 63 loss: 0.5052077174186707\n",
            "batch: 64 loss: 0.4461357295513153\n",
            "batch: 65 loss: 0.3906407058238983\n",
            "batch: 66 loss: 0.42788243293762207\n",
            "batch: 67 loss: 0.4380936622619629\n",
            "batch: 68 loss: 0.4570040702819824\n",
            "batch: 69 loss: 0.3802696466445923\n",
            "batch: 70 loss: 0.5482855439186096\n",
            "batch: 71 loss: 0.4004245102405548\n",
            "batch: 72 loss: 0.3782728612422943\n",
            "batch: 73 loss: 0.4370640516281128\n",
            "batch: 74 loss: 0.3100087642669678\n",
            "batch: 75 loss: 0.5168421864509583\n",
            "batch: 76 loss: 0.3418143689632416\n",
            "batch: 77 loss: 0.43072739243507385\n",
            "batch: 78 loss: 0.3648412823677063\n",
            "batch: 79 loss: 0.4640054702758789\n",
            "batch: 80 loss: 0.42160749435424805\n",
            "batch: 81 loss: 0.43597933650016785\n",
            "batch: 82 loss: 0.3969581425189972\n",
            "batch: 83 loss: 0.43465086817741394\n",
            "batch: 84 loss: 0.5234717726707458\n",
            "batch: 85 loss: 0.3505406975746155\n",
            "batch: 86 loss: 0.40844491124153137\n",
            "batch: 87 loss: 0.365620493888855\n",
            "batch: 88 loss: 0.7627385854721069\n",
            "batch: 89 loss: 0.37663644552230835\n",
            "batch: 90 loss: 0.5647193193435669\n",
            "batch: 91 loss: 0.525992751121521\n",
            "batch: 92 loss: 0.44758057594299316\n",
            "batch: 93 loss: 0.42949071526527405\n",
            "batch: 94 loss: 0.5504962205886841\n",
            "batch: 95 loss: 0.40539664030075073\n",
            "batch: 96 loss: 0.4305684566497803\n",
            "batch: 97 loss: 0.3251258432865143\n",
            "batch: 98 loss: 0.3348158597946167\n",
            "batch: 99 loss: 0.3914792835712433\n",
            "batch: 100 loss: 0.5432010889053345\n",
            "batch: 101 loss: 0.3280717134475708\n",
            "batch: 102 loss: 0.4027581214904785\n",
            "batch: 103 loss: 0.348074734210968\n",
            "batch: 104 loss: 0.4887009859085083\n",
            "batch: 105 loss: 0.4283774495124817\n",
            "batch: 106 loss: 0.35400426387786865\n",
            "batch: 107 loss: 0.4114646911621094\n",
            "batch: 108 loss: 0.3419570326805115\n",
            "batch: 109 loss: 0.3827161192893982\n",
            "batch: 110 loss: 0.3553465008735657\n",
            "batch: 111 loss: 0.5234023332595825\n",
            "batch: 112 loss: 0.5417187213897705\n",
            "batch: 113 loss: 0.42202967405319214\n",
            "batch: 114 loss: 0.46708768606185913\n",
            "batch: 115 loss: 0.5106279850006104\n",
            "batch: 116 loss: 0.3909548223018646\n",
            "batch: 117 loss: 0.6006433963775635\n",
            "batch: 118 loss: 0.43643832206726074\n",
            "batch: 119 loss: 0.39601126313209534\n",
            "batch: 120 loss: 0.47725552320480347\n",
            "batch: 121 loss: 0.3472687602043152\n",
            "batch: 122 loss: 0.6281930208206177\n",
            "batch: 123 loss: 0.4749824106693268\n",
            "batch: 124 loss: 0.4107586741447449\n",
            "batch: 125 loss: 0.39994269609451294\n",
            "batch: 126 loss: 0.5183539390563965\n",
            "batch: 127 loss: 0.5169799327850342\n",
            "batch: 128 loss: 0.3998180329799652\n",
            "batch: 129 loss: 0.4106333553791046\n",
            "batch: 130 loss: 0.4331319034099579\n",
            "batch: 131 loss: 0.3715705871582031\n",
            "batch: 132 loss: 0.39640453457832336\n",
            "batch: 133 loss: 0.38119423389434814\n",
            "batch: 134 loss: 0.42903780937194824\n",
            "batch: 135 loss: 0.392076313495636\n",
            "batch: 136 loss: 0.32052505016326904\n",
            "batch: 137 loss: 0.38717320561408997\n",
            "batch: 138 loss: 0.33501291275024414\n",
            "batch: 139 loss: 0.5325213074684143\n",
            "batch: 140 loss: 0.4284344017505646\n",
            "batch: 141 loss: 0.311347097158432\n",
            "batch: 142 loss: 0.428323358297348\n",
            "batch: 143 loss: 0.41076210141181946\n",
            "batch: 144 loss: 0.41309863328933716\n",
            "batch: 145 loss: 0.5517135858535767\n",
            "batch: 146 loss: 0.6082540154457092\n",
            "LOSS train 0.46921801567077637 valid 0.47553038597106934\n",
            "EPOCH 4:\n",
            "batch: 0 loss: 0.4607747495174408\n",
            "batch: 1 loss: 0.3769381642341614\n",
            "batch: 2 loss: 0.534203052520752\n",
            "batch: 3 loss: 0.3816750943660736\n",
            "batch: 4 loss: 0.4174570143222809\n",
            "batch: 5 loss: 0.45947954058647156\n",
            "batch: 6 loss: 0.4123997092247009\n",
            "batch: 7 loss: 0.4838266372680664\n",
            "batch: 8 loss: 0.5218795537948608\n",
            "batch: 9 loss: 0.3886567950248718\n",
            "batch: 10 loss: 0.33866190910339355\n",
            "batch: 11 loss: 0.44317182898521423\n",
            "batch: 12 loss: 0.44155359268188477\n",
            "batch: 13 loss: 0.47261106967926025\n",
            "batch: 14 loss: 0.43066704273223877\n",
            "batch: 15 loss: 0.4086981415748596\n",
            "batch: 16 loss: 0.4003885090351105\n",
            "batch: 17 loss: 0.4033023715019226\n",
            "batch: 18 loss: 0.38538822531700134\n",
            "batch: 19 loss: 0.35909080505371094\n",
            "batch: 20 loss: 0.5635748505592346\n",
            "batch: 21 loss: 0.3959863781929016\n",
            "batch: 22 loss: 0.5811305642127991\n",
            "batch: 23 loss: 0.36926159262657166\n",
            "batch: 24 loss: 0.39360830187797546\n",
            "batch: 25 loss: 0.5507526993751526\n",
            "batch: 26 loss: 0.413524329662323\n",
            "batch: 27 loss: 0.47303685545921326\n",
            "batch: 28 loss: 0.39909258484840393\n",
            "batch: 29 loss: 0.30384892225265503\n",
            "batch: 30 loss: 0.40281248092651367\n",
            "batch: 31 loss: 0.2949284315109253\n",
            "batch: 32 loss: 0.4571702778339386\n",
            "batch: 33 loss: 0.604395866394043\n",
            "batch: 34 loss: 0.41999900341033936\n",
            "batch: 35 loss: 0.35269254446029663\n",
            "batch: 36 loss: 0.3727225661277771\n",
            "batch: 37 loss: 0.5135189890861511\n",
            "batch: 38 loss: 0.31748008728027344\n",
            "batch: 39 loss: 0.36165934801101685\n",
            "batch: 40 loss: 0.44736820459365845\n",
            "batch: 41 loss: 0.43661126494407654\n",
            "batch: 42 loss: 0.4354991018772125\n",
            "batch: 43 loss: 0.3734560012817383\n",
            "batch: 44 loss: 0.39012786746025085\n",
            "batch: 45 loss: 0.33992302417755127\n",
            "batch: 46 loss: 0.4331384003162384\n",
            "batch: 47 loss: 0.4346086084842682\n",
            "batch: 48 loss: 0.2802114486694336\n",
            "batch: 49 loss: 0.6439857482910156\n",
            "batch: 50 loss: 0.3309323787689209\n",
            "batch: 51 loss: 0.43643084168434143\n",
            "batch: 52 loss: 0.3267342150211334\n",
            "batch: 53 loss: 0.44819509983062744\n",
            "batch: 54 loss: 0.42751002311706543\n",
            "batch: 55 loss: 0.34183746576309204\n",
            "batch: 56 loss: 0.4461468458175659\n",
            "batch: 57 loss: 0.6250438094139099\n",
            "batch: 58 loss: 0.3795897662639618\n",
            "batch: 59 loss: 0.40487140417099\n",
            "batch: 60 loss: 0.4293885827064514\n",
            "batch: 61 loss: 0.3485087752342224\n",
            "batch: 62 loss: 0.4265068769454956\n",
            "batch: 63 loss: 0.3992881774902344\n",
            "batch: 64 loss: 0.38111647963523865\n",
            "batch: 65 loss: 0.4173542261123657\n",
            "batch: 66 loss: 0.4877144396305084\n",
            "batch: 67 loss: 0.4466555118560791\n",
            "batch: 68 loss: 0.31184402108192444\n",
            "batch: 69 loss: 0.33277517557144165\n",
            "batch: 70 loss: 0.5462179183959961\n",
            "batch: 71 loss: 0.3837164640426636\n",
            "batch: 72 loss: 0.4063112139701843\n",
            "batch: 73 loss: 0.39892223477363586\n",
            "batch: 74 loss: 0.35783588886260986\n",
            "batch: 75 loss: 0.376555860042572\n",
            "batch: 76 loss: 0.4810726046562195\n",
            "batch: 77 loss: 0.44958028197288513\n",
            "batch: 78 loss: 0.576933741569519\n",
            "batch: 79 loss: 0.614122748374939\n",
            "batch: 80 loss: 0.43998855352401733\n",
            "batch: 81 loss: 0.49773678183555603\n",
            "batch: 82 loss: 0.7069537043571472\n",
            "batch: 83 loss: 0.35781747102737427\n",
            "batch: 84 loss: 0.43060067296028137\n",
            "batch: 85 loss: 0.3444291353225708\n",
            "batch: 86 loss: 0.5349535942077637\n",
            "batch: 87 loss: 0.40016430616378784\n",
            "batch: 88 loss: 0.4053618907928467\n",
            "batch: 89 loss: 0.48890504240989685\n",
            "batch: 90 loss: 0.350410521030426\n",
            "batch: 91 loss: 0.4342494606971741\n",
            "batch: 92 loss: 0.2889077365398407\n",
            "batch: 93 loss: 0.5345467329025269\n",
            "batch: 94 loss: 0.3762055039405823\n",
            "batch: 95 loss: 0.44303250312805176\n",
            "batch: 96 loss: 0.5306840538978577\n",
            "batch: 97 loss: 0.3648090958595276\n",
            "batch: 98 loss: 0.3842483162879944\n",
            "batch: 99 loss: 0.39661821722984314\n",
            "batch: 100 loss: 0.4587799310684204\n",
            "batch: 101 loss: 0.48996391892433167\n",
            "batch: 102 loss: 0.3693161606788635\n",
            "batch: 103 loss: 0.3967505991458893\n",
            "batch: 104 loss: 0.5030415058135986\n",
            "batch: 105 loss: 0.4069300889968872\n",
            "batch: 106 loss: 0.3472358286380768\n",
            "batch: 107 loss: 0.3241139054298401\n",
            "batch: 108 loss: 0.48803776502609253\n",
            "batch: 109 loss: 0.36423930525779724\n",
            "batch: 110 loss: 0.35102128982543945\n",
            "batch: 111 loss: 0.44567054510116577\n",
            "batch: 112 loss: 0.5402929782867432\n",
            "batch: 113 loss: 0.3393554985523224\n",
            "batch: 114 loss: 0.5179888010025024\n",
            "batch: 115 loss: 0.4105037450790405\n",
            "batch: 116 loss: 0.33088254928588867\n",
            "batch: 117 loss: 0.5132566690444946\n",
            "batch: 118 loss: 0.34416791796684265\n",
            "batch: 119 loss: 0.32022956013679504\n",
            "batch: 120 loss: 0.3803738057613373\n",
            "batch: 121 loss: 0.46742650866508484\n",
            "batch: 122 loss: 0.4545971751213074\n",
            "batch: 123 loss: 0.40179377794265747\n",
            "batch: 124 loss: 0.5046065449714661\n",
            "batch: 125 loss: 0.3717001974582672\n",
            "batch: 126 loss: 0.37238189578056335\n",
            "batch: 127 loss: 0.4061641991138458\n",
            "batch: 128 loss: 0.43565845489501953\n",
            "batch: 129 loss: 0.5263029336929321\n",
            "batch: 130 loss: 0.45348018407821655\n",
            "batch: 131 loss: 0.4410868287086487\n",
            "batch: 132 loss: 0.3910679519176483\n",
            "batch: 133 loss: 0.34598809480667114\n",
            "batch: 134 loss: 0.38078927993774414\n",
            "batch: 135 loss: 0.40065979957580566\n",
            "batch: 136 loss: 0.4108678996562958\n",
            "batch: 137 loss: 0.288600891828537\n",
            "batch: 138 loss: 0.27379971742630005\n",
            "batch: 139 loss: 0.339099645614624\n",
            "batch: 140 loss: 0.45897457003593445\n",
            "batch: 141 loss: 0.3571200966835022\n",
            "batch: 142 loss: 0.5772625803947449\n",
            "batch: 143 loss: 0.319572389125824\n",
            "batch: 144 loss: 0.3497738838195801\n",
            "batch: 145 loss: 0.4531122148036957\n",
            "batch: 146 loss: 0.2948768138885498\n",
            "LOSS train 0.510226309299469 valid 0.4992540180683136\n",
            "EPOCH 5:\n",
            "batch: 0 loss: 0.5048969388008118\n",
            "batch: 1 loss: 0.5685807466506958\n",
            "batch: 2 loss: 0.3798835277557373\n",
            "batch: 3 loss: 0.36400461196899414\n",
            "batch: 4 loss: 0.34279516339302063\n",
            "batch: 5 loss: 0.42996516823768616\n",
            "batch: 6 loss: 0.34734779596328735\n",
            "batch: 7 loss: 0.30094680190086365\n",
            "batch: 8 loss: 0.4904783070087433\n",
            "batch: 9 loss: 0.3292767107486725\n",
            "batch: 10 loss: 0.335286408662796\n",
            "batch: 11 loss: 0.5571889281272888\n",
            "batch: 12 loss: 0.3835600018501282\n",
            "batch: 13 loss: 0.44847288727760315\n",
            "batch: 14 loss: 0.31814226508140564\n",
            "batch: 15 loss: 0.44361600279808044\n",
            "batch: 16 loss: 0.357944130897522\n",
            "batch: 17 loss: 0.34949564933776855\n",
            "batch: 18 loss: 0.45569315552711487\n",
            "batch: 19 loss: 0.34198063611984253\n",
            "batch: 20 loss: 0.32637637853622437\n",
            "batch: 21 loss: 0.5041037797927856\n",
            "batch: 22 loss: 0.3152008652687073\n",
            "batch: 23 loss: 0.3561666011810303\n",
            "batch: 24 loss: 0.4658094048500061\n",
            "batch: 25 loss: 0.41986221075057983\n",
            "batch: 26 loss: 0.34170398116111755\n",
            "batch: 27 loss: 0.3090084493160248\n",
            "batch: 28 loss: 0.36272478103637695\n",
            "batch: 29 loss: 0.3758225739002228\n",
            "batch: 30 loss: 0.3721270263195038\n",
            "batch: 31 loss: 0.4340006709098816\n",
            "batch: 32 loss: 0.28361189365386963\n",
            "batch: 33 loss: 0.39241665601730347\n",
            "batch: 34 loss: 0.44511905312538147\n",
            "batch: 35 loss: 0.35461413860321045\n",
            "batch: 36 loss: 0.447696715593338\n",
            "batch: 37 loss: 0.37786442041397095\n",
            "batch: 38 loss: 0.48408499360084534\n",
            "batch: 39 loss: 0.6240420341491699\n",
            "batch: 40 loss: 0.3647034764289856\n",
            "batch: 41 loss: 0.4064006507396698\n",
            "batch: 42 loss: 0.4268924593925476\n",
            "batch: 43 loss: 0.39462926983833313\n",
            "batch: 44 loss: 0.35433530807495117\n",
            "batch: 45 loss: 0.4872989058494568\n",
            "batch: 46 loss: 0.6489940881729126\n",
            "batch: 47 loss: 0.31867533922195435\n",
            "batch: 48 loss: 0.44037923216819763\n",
            "batch: 49 loss: 0.4182819724082947\n",
            "batch: 50 loss: 0.44388464093208313\n",
            "batch: 51 loss: 0.395582377910614\n",
            "batch: 52 loss: 0.33541446924209595\n",
            "batch: 53 loss: 0.28585949540138245\n",
            "batch: 54 loss: 0.43782347440719604\n",
            "batch: 55 loss: 0.35826143622398376\n",
            "batch: 56 loss: 0.33431628346443176\n",
            "batch: 57 loss: 0.4515497088432312\n",
            "batch: 58 loss: 0.39579471945762634\n",
            "batch: 59 loss: 0.31209826469421387\n",
            "batch: 60 loss: 0.3909342288970947\n",
            "batch: 61 loss: 0.3643420338630676\n",
            "batch: 62 loss: 0.4617238938808441\n",
            "batch: 63 loss: 0.38594046235084534\n",
            "batch: 64 loss: 0.2798522114753723\n",
            "batch: 65 loss: 0.5019739866256714\n",
            "batch: 66 loss: 0.32446524500846863\n",
            "batch: 67 loss: 0.38246989250183105\n",
            "batch: 68 loss: 0.6056346297264099\n",
            "batch: 69 loss: 0.3638607859611511\n",
            "batch: 70 loss: 0.3169112205505371\n",
            "batch: 71 loss: 0.31676241755485535\n",
            "batch: 72 loss: 0.48681387305259705\n",
            "batch: 73 loss: 0.40864911675453186\n",
            "batch: 74 loss: 0.372191846370697\n",
            "batch: 75 loss: 0.4318658709526062\n",
            "batch: 76 loss: 0.46158158779144287\n",
            "batch: 77 loss: 0.3937396705150604\n",
            "batch: 78 loss: 0.3338702619075775\n",
            "batch: 79 loss: 0.451675683259964\n",
            "batch: 80 loss: 0.365738183259964\n",
            "batch: 81 loss: 0.3939058780670166\n",
            "batch: 82 loss: 0.32539382576942444\n",
            "batch: 83 loss: 0.44101712107658386\n",
            "batch: 84 loss: 0.36852508783340454\n",
            "batch: 85 loss: 0.44211769104003906\n",
            "batch: 86 loss: 0.4294971525669098\n",
            "batch: 87 loss: 0.38769346475601196\n",
            "batch: 88 loss: 0.33563512563705444\n",
            "batch: 89 loss: 0.2526699900627136\n",
            "batch: 90 loss: 0.33759814500808716\n",
            "batch: 91 loss: 0.3546055555343628\n",
            "batch: 92 loss: 0.5388312339782715\n",
            "batch: 93 loss: 0.430277556180954\n",
            "batch: 94 loss: 0.3359670639038086\n",
            "batch: 95 loss: 0.3795129954814911\n",
            "batch: 96 loss: 0.3359954059123993\n",
            "batch: 97 loss: 0.3795733153820038\n",
            "batch: 98 loss: 0.3220427632331848\n",
            "batch: 99 loss: 0.5765891075134277\n",
            "batch: 100 loss: 0.2958976626396179\n",
            "batch: 101 loss: 0.32698944211006165\n",
            "batch: 102 loss: 0.5055272579193115\n",
            "batch: 103 loss: 0.3866710066795349\n",
            "batch: 104 loss: 0.3752690255641937\n",
            "batch: 105 loss: 0.4103643298149109\n",
            "batch: 106 loss: 0.408532053232193\n",
            "batch: 107 loss: 0.44975021481513977\n",
            "batch: 108 loss: 0.39865460991859436\n",
            "batch: 109 loss: 0.3613830804824829\n",
            "batch: 110 loss: 0.34328803420066833\n",
            "batch: 111 loss: 0.37839820981025696\n",
            "batch: 112 loss: 0.4612669348716736\n",
            "batch: 113 loss: 0.5312971472740173\n",
            "batch: 114 loss: 0.4426114857196808\n",
            "batch: 115 loss: 0.47590139508247375\n",
            "batch: 116 loss: 0.3669271171092987\n",
            "batch: 117 loss: 0.36313971877098083\n",
            "batch: 118 loss: 0.3472023010253906\n",
            "batch: 119 loss: 0.3521302342414856\n",
            "batch: 120 loss: 0.5505095720291138\n",
            "batch: 121 loss: 0.5158059597015381\n",
            "batch: 122 loss: 0.3782985210418701\n",
            "batch: 123 loss: 0.4289941191673279\n",
            "batch: 124 loss: 0.5398714542388916\n",
            "batch: 125 loss: 0.3734967112541199\n",
            "batch: 126 loss: 0.35701510310173035\n",
            "batch: 127 loss: 0.44109898805618286\n",
            "batch: 128 loss: 0.5113697052001953\n",
            "batch: 129 loss: 0.3524058759212494\n",
            "batch: 130 loss: 0.4550374746322632\n",
            "batch: 131 loss: 0.38885703682899475\n",
            "batch: 132 loss: 0.34168869256973267\n",
            "batch: 133 loss: 0.35381388664245605\n",
            "batch: 134 loss: 0.37782320380210876\n",
            "batch: 135 loss: 0.2865906059741974\n",
            "batch: 136 loss: 0.32765719294548035\n",
            "batch: 137 loss: 0.31255894899368286\n",
            "batch: 138 loss: 0.32795602083206177\n",
            "batch: 139 loss: 0.4199369549751282\n",
            "batch: 140 loss: 0.315215140581131\n",
            "batch: 141 loss: 0.45879557728767395\n",
            "batch: 142 loss: 0.493897408246994\n",
            "batch: 143 loss: 0.5087759494781494\n",
            "batch: 144 loss: 0.5417221784591675\n",
            "batch: 145 loss: 0.49780261516571045\n",
            "batch: 146 loss: 0.29755690693855286\n",
            "LOSS train 0.3964671492576599 valid 0.39096176624298096\n",
            "EPOCH 6:\n",
            "batch: 0 loss: 0.4355495572090149\n",
            "batch: 1 loss: 0.3749984800815582\n",
            "batch: 2 loss: 0.32112371921539307\n",
            "batch: 3 loss: 0.42261242866516113\n",
            "batch: 4 loss: 0.33476176857948303\n",
            "batch: 5 loss: 0.42685040831565857\n",
            "batch: 6 loss: 0.3898835778236389\n",
            "batch: 7 loss: 0.41383588314056396\n",
            "batch: 8 loss: 0.3015868663787842\n",
            "batch: 9 loss: 0.44460535049438477\n",
            "batch: 10 loss: 0.4461018443107605\n",
            "batch: 11 loss: 0.38485774397850037\n",
            "batch: 12 loss: 0.41742441058158875\n",
            "batch: 13 loss: 0.4198053777217865\n",
            "batch: 14 loss: 0.42203524708747864\n",
            "batch: 15 loss: 0.4961753487586975\n",
            "batch: 16 loss: 0.4223271906375885\n",
            "batch: 17 loss: 0.4187161326408386\n",
            "batch: 18 loss: 0.3653491139411926\n",
            "batch: 19 loss: 0.4840640723705292\n",
            "batch: 20 loss: 0.38058653473854065\n",
            "batch: 21 loss: 0.48144838213920593\n",
            "batch: 22 loss: 0.40167585015296936\n",
            "batch: 23 loss: 0.32464170455932617\n",
            "batch: 24 loss: 0.4034835398197174\n",
            "batch: 25 loss: 0.4288663864135742\n",
            "batch: 26 loss: 0.4479096531867981\n",
            "batch: 27 loss: 0.49575042724609375\n",
            "batch: 28 loss: 0.38740411400794983\n",
            "batch: 29 loss: 0.4710509181022644\n",
            "batch: 30 loss: 0.5153096914291382\n",
            "batch: 31 loss: 0.2738577127456665\n",
            "batch: 32 loss: 0.3577815592288971\n",
            "batch: 33 loss: 0.4502640664577484\n",
            "batch: 34 loss: 0.3330160975456238\n",
            "batch: 35 loss: 0.4139476716518402\n",
            "batch: 36 loss: 0.47640615701675415\n",
            "batch: 37 loss: 0.4580923318862915\n",
            "batch: 38 loss: 0.33189573884010315\n",
            "batch: 39 loss: 0.36019790172576904\n",
            "batch: 40 loss: 0.4443332254886627\n",
            "batch: 41 loss: 0.3436398208141327\n",
            "batch: 42 loss: 0.35284000635147095\n",
            "batch: 43 loss: 0.5036865472793579\n",
            "batch: 44 loss: 0.4600484371185303\n",
            "batch: 45 loss: 0.3199654221534729\n",
            "batch: 46 loss: 0.3689132034778595\n",
            "batch: 47 loss: 0.4702405035495758\n",
            "batch: 48 loss: 0.30420464277267456\n",
            "batch: 49 loss: 0.43672487139701843\n",
            "batch: 50 loss: 0.3282455801963806\n",
            "batch: 51 loss: 0.316169798374176\n",
            "batch: 52 loss: 0.3376842141151428\n",
            "batch: 53 loss: 0.3517444133758545\n",
            "batch: 54 loss: 0.4435786306858063\n",
            "batch: 55 loss: 0.34291917085647583\n",
            "batch: 56 loss: 0.31040918827056885\n",
            "batch: 57 loss: 0.2521818280220032\n",
            "batch: 58 loss: 0.4018193781375885\n",
            "batch: 59 loss: 0.3153362572193146\n",
            "batch: 60 loss: 0.3080028295516968\n",
            "batch: 61 loss: 0.2592412531375885\n",
            "batch: 62 loss: 0.3612655997276306\n",
            "batch: 63 loss: 0.42353343963623047\n",
            "batch: 64 loss: 0.3411716818809509\n",
            "batch: 65 loss: 0.3340214192867279\n",
            "batch: 66 loss: 0.37708771228790283\n",
            "batch: 67 loss: 0.35666579008102417\n",
            "batch: 68 loss: 0.2832387089729309\n",
            "batch: 69 loss: 0.47300779819488525\n",
            "batch: 70 loss: 0.3154503107070923\n",
            "batch: 71 loss: 0.3275349736213684\n",
            "batch: 72 loss: 0.31945234537124634\n",
            "batch: 73 loss: 0.3880678117275238\n",
            "batch: 74 loss: 0.43033868074417114\n",
            "batch: 75 loss: 0.34260162711143494\n",
            "batch: 76 loss: 0.3962055444717407\n",
            "batch: 77 loss: 0.38361695408821106\n",
            "batch: 78 loss: 0.3897729218006134\n",
            "batch: 79 loss: 0.3126148581504822\n",
            "batch: 80 loss: 0.30678439140319824\n",
            "batch: 81 loss: 0.2595362067222595\n",
            "batch: 82 loss: 0.3630673289299011\n",
            "batch: 83 loss: 0.5147190093994141\n",
            "batch: 84 loss: 0.34931325912475586\n",
            "batch: 85 loss: 0.49141502380371094\n",
            "batch: 86 loss: 0.3548736572265625\n",
            "batch: 87 loss: 0.35636618733406067\n",
            "batch: 88 loss: 0.37858009338378906\n",
            "batch: 89 loss: 0.38388171792030334\n",
            "batch: 90 loss: 0.31780505180358887\n",
            "batch: 91 loss: 0.27423182129859924\n",
            "batch: 92 loss: 0.35745835304260254\n",
            "batch: 93 loss: 0.27335935831069946\n",
            "batch: 94 loss: 0.4147130250930786\n",
            "batch: 95 loss: 0.5367968678474426\n",
            "batch: 96 loss: 0.36171630024909973\n",
            "batch: 97 loss: 0.4681325852870941\n",
            "batch: 98 loss: 0.35450100898742676\n",
            "batch: 99 loss: 0.3459431231021881\n",
            "batch: 100 loss: 0.33719033002853394\n",
            "batch: 101 loss: 0.4475261867046356\n",
            "batch: 102 loss: 0.4182738661766052\n",
            "batch: 103 loss: 0.4549456834793091\n",
            "batch: 104 loss: 0.49598050117492676\n",
            "batch: 105 loss: 0.44519656896591187\n",
            "batch: 106 loss: 0.3861714005470276\n",
            "batch: 107 loss: 0.39396122097969055\n",
            "batch: 108 loss: 0.41189324855804443\n",
            "batch: 109 loss: 0.4496670663356781\n",
            "batch: 110 loss: 0.31812185049057007\n",
            "batch: 111 loss: 0.4825921952724457\n",
            "batch: 112 loss: 0.2792479991912842\n",
            "batch: 113 loss: 0.421455442905426\n",
            "batch: 114 loss: 0.40717869997024536\n",
            "batch: 115 loss: 0.30892473459243774\n",
            "batch: 116 loss: 0.44760099053382874\n",
            "batch: 117 loss: 0.5185689926147461\n",
            "batch: 118 loss: 0.35574764013290405\n",
            "batch: 119 loss: 0.36068975925445557\n",
            "batch: 120 loss: 0.435124009847641\n",
            "batch: 121 loss: 0.41399329900741577\n",
            "batch: 122 loss: 0.4250674545764923\n",
            "batch: 123 loss: 0.337923526763916\n",
            "batch: 124 loss: 0.28698456287384033\n",
            "batch: 125 loss: 0.4098392426967621\n",
            "batch: 126 loss: 0.31885695457458496\n",
            "batch: 127 loss: 0.568659245967865\n",
            "batch: 128 loss: 0.419236421585083\n",
            "batch: 129 loss: 0.2944642901420593\n",
            "batch: 130 loss: 0.468018501996994\n",
            "batch: 131 loss: 0.47505947947502136\n",
            "batch: 132 loss: 0.40001794695854187\n",
            "batch: 133 loss: 0.4259294867515564\n",
            "batch: 134 loss: 0.3027271032333374\n",
            "batch: 135 loss: 0.35031038522720337\n",
            "batch: 136 loss: 0.3287501335144043\n",
            "batch: 137 loss: 0.25227582454681396\n",
            "batch: 138 loss: 0.3560003638267517\n",
            "batch: 139 loss: 0.276511013507843\n",
            "batch: 140 loss: 0.3709957003593445\n",
            "batch: 141 loss: 0.4093397557735443\n",
            "batch: 142 loss: 0.3083331286907196\n",
            "batch: 143 loss: 0.42750874161720276\n",
            "batch: 144 loss: 0.32935386896133423\n",
            "batch: 145 loss: 0.3512594997882843\n",
            "batch: 146 loss: 0.3472203314304352\n",
            "LOSS train 0.37869909405708313 valid 0.3854597508907318\n",
            "EPOCH 7:\n",
            "batch: 0 loss: 0.43322816491127014\n",
            "batch: 1 loss: 0.4492247700691223\n",
            "batch: 2 loss: 0.24636143445968628\n",
            "batch: 3 loss: 0.35190582275390625\n",
            "batch: 4 loss: 0.3227607309818268\n",
            "batch: 5 loss: 0.4999817907810211\n",
            "batch: 6 loss: 0.3536835312843323\n",
            "batch: 7 loss: 0.3734341561794281\n",
            "batch: 8 loss: 0.3629293441772461\n",
            "batch: 9 loss: 0.3440791070461273\n",
            "batch: 10 loss: 0.35965418815612793\n",
            "batch: 11 loss: 0.4559536874294281\n",
            "batch: 12 loss: 0.35088711977005005\n",
            "batch: 13 loss: 0.4790019690990448\n",
            "batch: 14 loss: 0.35751593112945557\n",
            "batch: 15 loss: 0.3326951563358307\n",
            "batch: 16 loss: 0.3739055097103119\n",
            "batch: 17 loss: 0.26540523767471313\n",
            "batch: 18 loss: 0.3239790201187134\n",
            "batch: 19 loss: 0.3319828510284424\n",
            "batch: 20 loss: 0.3502500653266907\n",
            "batch: 21 loss: 0.3906567692756653\n",
            "batch: 22 loss: 0.4160660207271576\n",
            "batch: 23 loss: 0.3380957245826721\n",
            "batch: 24 loss: 0.3756657838821411\n",
            "batch: 25 loss: 0.3537102937698364\n",
            "batch: 26 loss: 0.31579285860061646\n",
            "batch: 27 loss: 0.4118933081626892\n",
            "batch: 28 loss: 0.4449869990348816\n",
            "batch: 29 loss: 0.3432427644729614\n",
            "batch: 30 loss: 0.284413605928421\n",
            "batch: 31 loss: 0.40255144238471985\n",
            "batch: 32 loss: 0.3116881549358368\n",
            "batch: 33 loss: 0.395429790019989\n",
            "batch: 34 loss: 0.34695515036582947\n",
            "batch: 35 loss: 0.315728098154068\n",
            "batch: 36 loss: 0.3956198990345001\n",
            "batch: 37 loss: 0.3348979949951172\n",
            "batch: 38 loss: 0.3311140239238739\n",
            "batch: 39 loss: 0.4143485128879547\n",
            "batch: 40 loss: 0.3369094729423523\n",
            "batch: 41 loss: 0.31829532980918884\n",
            "batch: 42 loss: 0.34577277302742004\n",
            "batch: 43 loss: 0.43828025460243225\n",
            "batch: 44 loss: 0.32657134532928467\n",
            "batch: 45 loss: 0.36143404245376587\n",
            "batch: 46 loss: 0.3372644782066345\n",
            "batch: 47 loss: 0.3786628246307373\n",
            "batch: 48 loss: 0.4376533329486847\n",
            "batch: 49 loss: 0.3659764230251312\n",
            "batch: 50 loss: 0.42357611656188965\n",
            "batch: 51 loss: 0.4136253595352173\n",
            "batch: 52 loss: 0.36462345719337463\n",
            "batch: 53 loss: 0.3272648751735687\n",
            "batch: 54 loss: 0.3851280212402344\n",
            "batch: 55 loss: 0.3120015561580658\n",
            "batch: 56 loss: 0.31377315521240234\n",
            "batch: 57 loss: 0.31549033522605896\n",
            "batch: 58 loss: 0.4870527684688568\n",
            "batch: 59 loss: 0.29714518785476685\n",
            "batch: 60 loss: 0.3529808521270752\n",
            "batch: 61 loss: 0.31294146180152893\n",
            "batch: 62 loss: 0.3357270359992981\n",
            "batch: 63 loss: 0.33088722825050354\n",
            "batch: 64 loss: 0.2832931876182556\n",
            "batch: 65 loss: 0.5037870407104492\n",
            "batch: 66 loss: 0.4124828577041626\n",
            "batch: 67 loss: 0.3127509355545044\n",
            "batch: 68 loss: 0.319919228553772\n",
            "batch: 69 loss: 0.29395556449890137\n",
            "batch: 70 loss: 0.37211525440216064\n",
            "batch: 71 loss: 0.3734021484851837\n",
            "batch: 72 loss: 0.49777689576148987\n",
            "batch: 73 loss: 0.40883973240852356\n",
            "batch: 74 loss: 0.31519460678100586\n",
            "batch: 75 loss: 0.3622538447380066\n",
            "batch: 76 loss: 0.3346617817878723\n",
            "batch: 77 loss: 0.35813450813293457\n",
            "batch: 78 loss: 0.2949286103248596\n",
            "batch: 79 loss: 0.35354697704315186\n",
            "batch: 80 loss: 0.32449233531951904\n",
            "batch: 81 loss: 0.3044736385345459\n",
            "batch: 82 loss: 0.4964216649532318\n",
            "batch: 83 loss: 0.3114772439002991\n",
            "batch: 84 loss: 0.30597564578056335\n",
            "batch: 85 loss: 0.33504945039749146\n",
            "batch: 86 loss: 0.33556652069091797\n",
            "batch: 87 loss: 0.339435875415802\n",
            "batch: 88 loss: 0.4031723439693451\n",
            "batch: 89 loss: 0.26978611946105957\n",
            "batch: 90 loss: 0.3612734079360962\n",
            "batch: 91 loss: 0.36636656522750854\n",
            "batch: 92 loss: 0.3629347085952759\n",
            "batch: 93 loss: 0.3346351087093353\n",
            "batch: 94 loss: 0.3314908742904663\n",
            "batch: 95 loss: 0.45113202929496765\n",
            "batch: 96 loss: 0.32885754108428955\n",
            "batch: 97 loss: 0.37959033250808716\n",
            "batch: 98 loss: 0.3132450580596924\n",
            "batch: 99 loss: 0.38992151618003845\n",
            "batch: 100 loss: 0.4284357726573944\n",
            "batch: 101 loss: 0.3570626974105835\n",
            "batch: 102 loss: 0.2949944734573364\n",
            "batch: 103 loss: 0.35242828726768494\n",
            "batch: 104 loss: 0.3913726210594177\n",
            "batch: 105 loss: 0.3138348460197449\n",
            "batch: 106 loss: 0.5123990774154663\n",
            "batch: 107 loss: 0.32097339630126953\n",
            "batch: 108 loss: 0.3347325921058655\n",
            "batch: 109 loss: 0.34181368350982666\n",
            "batch: 110 loss: 0.26280516386032104\n",
            "batch: 111 loss: 0.317094087600708\n",
            "batch: 112 loss: 0.3401946425437927\n",
            "batch: 113 loss: 0.3442597985267639\n",
            "batch: 114 loss: 0.40812617540359497\n",
            "batch: 115 loss: 0.338762104511261\n",
            "batch: 116 loss: 0.35092687606811523\n",
            "batch: 117 loss: 0.2600429356098175\n",
            "batch: 118 loss: 0.3325921297073364\n",
            "batch: 119 loss: 0.5138504505157471\n",
            "batch: 120 loss: 0.25988367199897766\n",
            "batch: 121 loss: 0.29831767082214355\n",
            "batch: 122 loss: 0.3550585210323334\n",
            "batch: 123 loss: 0.3442397713661194\n",
            "batch: 124 loss: 0.3619573712348938\n",
            "batch: 125 loss: 0.4489782154560089\n",
            "batch: 126 loss: 0.39512935280799866\n",
            "batch: 127 loss: 0.4014807641506195\n",
            "batch: 128 loss: 0.34614092111587524\n",
            "batch: 129 loss: 0.3637387752532959\n",
            "batch: 130 loss: 0.5922847986221313\n",
            "batch: 131 loss: 0.3472468852996826\n",
            "batch: 132 loss: 0.27475881576538086\n",
            "batch: 133 loss: 0.30580300092697144\n",
            "batch: 134 loss: 0.3368481993675232\n",
            "batch: 135 loss: 0.3772599995136261\n",
            "batch: 136 loss: 0.28470104932785034\n",
            "batch: 137 loss: 0.2959338426589966\n",
            "batch: 138 loss: 0.3041669428348541\n",
            "batch: 139 loss: 0.33876100182533264\n",
            "batch: 140 loss: 0.4042297899723053\n",
            "batch: 141 loss: 0.3288777470588684\n",
            "batch: 142 loss: 0.4334743320941925\n",
            "batch: 143 loss: 0.31530749797821045\n",
            "batch: 144 loss: 0.3192744255065918\n",
            "batch: 145 loss: 0.3934319317340851\n",
            "batch: 146 loss: 0.43044739961624146\n",
            "LOSS train 0.4158635437488556 valid 0.41380172967910767\n",
            "EPOCH 8:\n",
            "batch: 0 loss: 0.3545687198638916\n",
            "batch: 1 loss: 0.3001485764980316\n",
            "batch: 2 loss: 0.40887999534606934\n",
            "batch: 3 loss: 0.5666399002075195\n",
            "batch: 4 loss: 0.3199150860309601\n",
            "batch: 5 loss: 0.29776662588119507\n",
            "batch: 6 loss: 0.4109591841697693\n",
            "batch: 7 loss: 0.30394160747528076\n",
            "batch: 8 loss: 0.2884002923965454\n",
            "batch: 9 loss: 0.3436305820941925\n",
            "batch: 10 loss: 0.310754656791687\n",
            "batch: 11 loss: 0.3059779107570648\n",
            "batch: 12 loss: 0.39461278915405273\n",
            "batch: 13 loss: 0.25232186913490295\n",
            "batch: 14 loss: 0.316694974899292\n",
            "batch: 15 loss: 0.3826977014541626\n",
            "batch: 16 loss: 0.35245048999786377\n",
            "batch: 17 loss: 0.3731512427330017\n",
            "batch: 18 loss: 0.28551894426345825\n",
            "batch: 19 loss: 0.2915595769882202\n",
            "batch: 20 loss: 0.3136911392211914\n",
            "batch: 21 loss: 0.2970055937767029\n",
            "batch: 22 loss: 0.41239914298057556\n",
            "batch: 23 loss: 0.33939558267593384\n",
            "batch: 24 loss: 0.3517134189605713\n",
            "batch: 25 loss: 0.24916785955429077\n",
            "batch: 26 loss: 0.39991846680641174\n",
            "batch: 27 loss: 0.3231567144393921\n",
            "batch: 28 loss: 0.38300496339797974\n",
            "batch: 29 loss: 0.38665980100631714\n",
            "batch: 30 loss: 0.3225400149822235\n",
            "batch: 31 loss: 0.4431142807006836\n",
            "batch: 32 loss: 0.30313757061958313\n",
            "batch: 33 loss: 0.40089529752731323\n",
            "batch: 34 loss: 0.368814617395401\n",
            "batch: 35 loss: 0.3944982588291168\n",
            "batch: 36 loss: 0.34170234203338623\n",
            "batch: 37 loss: 0.39820319414138794\n",
            "batch: 38 loss: 0.38974329829216003\n",
            "batch: 39 loss: 0.27941879630088806\n",
            "batch: 40 loss: 0.318656325340271\n",
            "batch: 41 loss: 0.28758755326271057\n",
            "batch: 42 loss: 0.3868384063243866\n",
            "batch: 43 loss: 0.3089902997016907\n",
            "batch: 44 loss: 0.31144580245018005\n",
            "batch: 45 loss: 0.2779509425163269\n",
            "batch: 46 loss: 0.338859498500824\n",
            "batch: 47 loss: 0.35378921031951904\n",
            "batch: 48 loss: 0.29430273175239563\n",
            "batch: 49 loss: 0.32344508171081543\n",
            "batch: 50 loss: 0.30779969692230225\n",
            "batch: 51 loss: 0.39641228318214417\n",
            "batch: 52 loss: 0.3373038172721863\n",
            "batch: 53 loss: 0.4122977554798126\n",
            "batch: 54 loss: 0.28654229640960693\n",
            "batch: 55 loss: 0.3939445912837982\n",
            "batch: 56 loss: 0.3967283070087433\n",
            "batch: 57 loss: 0.31681162118911743\n",
            "batch: 58 loss: 0.31260615587234497\n",
            "batch: 59 loss: 0.29231807589530945\n",
            "batch: 60 loss: 0.36590760946273804\n",
            "batch: 61 loss: 0.35614514350891113\n",
            "batch: 62 loss: 0.29403603076934814\n",
            "batch: 63 loss: 0.3987423777580261\n",
            "batch: 64 loss: 0.3065648674964905\n",
            "batch: 65 loss: 0.3269619047641754\n",
            "batch: 66 loss: 0.3236522674560547\n",
            "batch: 67 loss: 0.2784489393234253\n",
            "batch: 68 loss: 0.2548620104789734\n",
            "batch: 69 loss: 0.41432297229766846\n",
            "batch: 70 loss: 0.3898490369319916\n",
            "batch: 71 loss: 0.2722058892250061\n",
            "batch: 72 loss: 0.280147910118103\n",
            "batch: 73 loss: 0.30534639954566956\n",
            "batch: 74 loss: 0.2507808208465576\n",
            "batch: 75 loss: 0.4115959703922272\n",
            "batch: 76 loss: 0.28918004035949707\n",
            "batch: 77 loss: 0.24656298756599426\n",
            "batch: 78 loss: 0.34655696153640747\n",
            "batch: 79 loss: 0.3292786180973053\n",
            "batch: 80 loss: 0.3792364299297333\n",
            "batch: 81 loss: 0.3008398711681366\n",
            "batch: 82 loss: 0.27575767040252686\n",
            "batch: 83 loss: 0.3654235899448395\n",
            "batch: 84 loss: 0.2716670334339142\n",
            "batch: 85 loss: 0.3076895475387573\n",
            "batch: 86 loss: 0.3244588077068329\n",
            "batch: 87 loss: 0.36353009939193726\n",
            "batch: 88 loss: 0.34019821882247925\n",
            "batch: 89 loss: 0.2716992199420929\n",
            "batch: 90 loss: 0.3088153004646301\n",
            "batch: 91 loss: 0.253911554813385\n",
            "batch: 92 loss: 0.31878602504730225\n",
            "batch: 93 loss: 0.3482658267021179\n",
            "batch: 94 loss: 0.31061142683029175\n",
            "batch: 95 loss: 0.3495585322380066\n",
            "batch: 96 loss: 0.33433207869529724\n",
            "batch: 97 loss: 0.3455921411514282\n",
            "batch: 98 loss: 0.3232564926147461\n",
            "batch: 99 loss: 0.2652670443058014\n",
            "batch: 100 loss: 0.3641533851623535\n",
            "batch: 101 loss: 0.34550681710243225\n",
            "batch: 102 loss: 0.293567955493927\n",
            "batch: 103 loss: 0.5049031972885132\n",
            "batch: 104 loss: 0.32933616638183594\n",
            "batch: 105 loss: 0.3532337546348572\n",
            "batch: 106 loss: 0.37107712030410767\n",
            "batch: 107 loss: 0.37408211827278137\n",
            "batch: 108 loss: 0.3711240887641907\n",
            "batch: 109 loss: 0.32409632205963135\n",
            "batch: 110 loss: 0.40293392539024353\n",
            "batch: 111 loss: 0.30712705850601196\n",
            "batch: 112 loss: 0.32998722791671753\n",
            "batch: 113 loss: 0.3188929557800293\n",
            "batch: 114 loss: 0.2946189045906067\n",
            "batch: 115 loss: 0.25578033924102783\n",
            "batch: 116 loss: 0.4917491376399994\n",
            "batch: 117 loss: 0.23461157083511353\n",
            "batch: 118 loss: 0.3798627555370331\n",
            "batch: 119 loss: 0.41511839628219604\n",
            "batch: 120 loss: 0.26055261492729187\n",
            "batch: 121 loss: 0.3592750132083893\n",
            "batch: 122 loss: 0.440476655960083\n",
            "batch: 123 loss: 0.3038803040981293\n",
            "batch: 124 loss: 0.28159472346305847\n",
            "batch: 125 loss: 0.3514977991580963\n",
            "batch: 126 loss: 0.2984483242034912\n",
            "batch: 127 loss: 0.45530033111572266\n",
            "batch: 128 loss: 0.28998005390167236\n",
            "batch: 129 loss: 0.34575343132019043\n",
            "batch: 130 loss: 0.25816798210144043\n",
            "batch: 131 loss: 0.3013368248939514\n",
            "batch: 132 loss: 0.2824702858924866\n",
            "batch: 133 loss: 0.30038562417030334\n",
            "batch: 134 loss: 0.3840653896331787\n",
            "batch: 135 loss: 0.3360021710395813\n",
            "batch: 136 loss: 0.3287377953529358\n",
            "batch: 137 loss: 0.41402432322502136\n",
            "batch: 138 loss: 0.24872638285160065\n",
            "batch: 139 loss: 0.43500813841819763\n",
            "batch: 140 loss: 0.2663518488407135\n",
            "batch: 141 loss: 0.30693620443344116\n",
            "batch: 142 loss: 0.27409717440605164\n",
            "batch: 143 loss: 0.335925817489624\n",
            "batch: 144 loss: 0.428041011095047\n",
            "batch: 145 loss: 0.4166066646575928\n",
            "batch: 146 loss: 0.2597261071205139\n",
            "LOSS train 0.7747933268547058 valid 0.772731363773346\n",
            "EPOCH 9:\n",
            "batch: 0 loss: 0.38912898302078247\n",
            "batch: 1 loss: 0.30232614278793335\n",
            "batch: 2 loss: 0.34197360277175903\n",
            "batch: 3 loss: 0.3095629811286926\n",
            "batch: 4 loss: 0.2966328561306\n",
            "batch: 5 loss: 0.2659982442855835\n",
            "batch: 6 loss: 0.32714974880218506\n",
            "batch: 7 loss: 0.3366750180721283\n",
            "batch: 8 loss: 0.43991583585739136\n",
            "batch: 9 loss: 0.3581494092941284\n",
            "batch: 10 loss: 0.34861892461776733\n",
            "batch: 11 loss: 0.30007678270339966\n",
            "batch: 12 loss: 0.3453497290611267\n",
            "batch: 13 loss: 0.2986479103565216\n",
            "batch: 14 loss: 0.3423507809638977\n",
            "batch: 15 loss: 0.28084611892700195\n",
            "batch: 16 loss: 0.30818408727645874\n",
            "batch: 17 loss: 0.304429829120636\n",
            "batch: 18 loss: 0.436230331659317\n",
            "batch: 19 loss: 0.33030062913894653\n",
            "batch: 20 loss: 0.3549039363861084\n",
            "batch: 21 loss: 0.38353291153907776\n",
            "batch: 22 loss: 0.4066818654537201\n",
            "batch: 23 loss: 0.29709333181381226\n",
            "batch: 24 loss: 0.30046430230140686\n",
            "batch: 25 loss: 0.4509848356246948\n",
            "batch: 26 loss: 0.4038085341453552\n",
            "batch: 27 loss: 0.2948395609855652\n",
            "batch: 28 loss: 0.2615707516670227\n",
            "batch: 29 loss: 0.379063218832016\n",
            "batch: 30 loss: 0.2611755132675171\n",
            "batch: 31 loss: 0.35555317997932434\n",
            "batch: 32 loss: 0.304492712020874\n",
            "batch: 33 loss: 0.3264317214488983\n",
            "batch: 34 loss: 0.3717469573020935\n",
            "batch: 35 loss: 0.2879030704498291\n",
            "batch: 36 loss: 0.2467411756515503\n",
            "batch: 37 loss: 0.27754971385002136\n",
            "batch: 38 loss: 0.39389511942863464\n",
            "batch: 39 loss: 0.27256280183792114\n",
            "batch: 40 loss: 0.2839636504650116\n",
            "batch: 41 loss: 0.3744502067565918\n",
            "batch: 42 loss: 0.28662019968032837\n",
            "batch: 43 loss: 0.25019755959510803\n",
            "batch: 44 loss: 0.2650953531265259\n",
            "batch: 45 loss: 0.35771799087524414\n",
            "batch: 46 loss: 0.318043053150177\n",
            "batch: 47 loss: 0.31556662917137146\n",
            "batch: 48 loss: 0.35038694739341736\n",
            "batch: 49 loss: 0.29049772024154663\n",
            "batch: 50 loss: 0.3190508186817169\n",
            "batch: 51 loss: 0.2795887887477875\n",
            "batch: 52 loss: 0.3125746250152588\n",
            "batch: 53 loss: 0.25069552659988403\n",
            "batch: 54 loss: 0.2797035574913025\n",
            "batch: 55 loss: 0.32442760467529297\n",
            "batch: 56 loss: 0.23930783569812775\n",
            "batch: 57 loss: 0.28407564759254456\n",
            "batch: 58 loss: 0.5198208093643188\n",
            "batch: 59 loss: 0.3247033953666687\n",
            "batch: 60 loss: 0.29252010583877563\n",
            "batch: 61 loss: 0.2733384370803833\n",
            "batch: 62 loss: 0.4251701533794403\n",
            "batch: 63 loss: 0.3018341064453125\n",
            "batch: 64 loss: 0.3233276605606079\n",
            "batch: 65 loss: 0.2813045084476471\n",
            "batch: 66 loss: 0.29510587453842163\n",
            "batch: 67 loss: 0.3122570514678955\n",
            "batch: 68 loss: 0.252782940864563\n",
            "batch: 69 loss: 0.3873652517795563\n",
            "batch: 70 loss: 0.3449283242225647\n",
            "batch: 71 loss: 0.392835795879364\n",
            "batch: 72 loss: 0.31724846363067627\n",
            "batch: 73 loss: 0.3839695155620575\n",
            "batch: 74 loss: 0.3412516117095947\n",
            "batch: 75 loss: 0.3874315321445465\n",
            "batch: 76 loss: 0.34055662155151367\n",
            "batch: 77 loss: 0.25280314683914185\n",
            "batch: 78 loss: 0.36608025431632996\n",
            "batch: 79 loss: 0.2863510847091675\n",
            "batch: 80 loss: 0.38493382930755615\n",
            "batch: 81 loss: 0.33535104990005493\n",
            "batch: 82 loss: 0.2681037187576294\n",
            "batch: 83 loss: 0.4532720446586609\n",
            "batch: 84 loss: 0.3617531955242157\n",
            "batch: 85 loss: 0.2349138706922531\n",
            "batch: 86 loss: 0.3603094220161438\n",
            "batch: 87 loss: 0.3032008409500122\n",
            "batch: 88 loss: 0.2609982490539551\n",
            "batch: 89 loss: 0.3442544937133789\n",
            "batch: 90 loss: 0.2883259057998657\n",
            "batch: 91 loss: 0.3426276445388794\n",
            "batch: 92 loss: 0.28103888034820557\n",
            "batch: 93 loss: 0.2810591459274292\n",
            "batch: 94 loss: 0.33777329325675964\n",
            "batch: 95 loss: 0.3060074746608734\n",
            "batch: 96 loss: 0.2590819299221039\n",
            "batch: 97 loss: 0.28500813245773315\n",
            "batch: 98 loss: 0.29269763827323914\n",
            "batch: 99 loss: 0.3773961663246155\n",
            "batch: 100 loss: 0.4683343768119812\n",
            "batch: 101 loss: 0.4673154652118683\n",
            "batch: 102 loss: 0.26296180486679077\n",
            "batch: 103 loss: 0.3056507706642151\n",
            "batch: 104 loss: 0.42419570684432983\n",
            "batch: 105 loss: 0.4846388101577759\n",
            "batch: 106 loss: 0.3498690128326416\n",
            "batch: 107 loss: 0.3572746515274048\n",
            "batch: 108 loss: 0.2996301054954529\n",
            "batch: 109 loss: 0.26788148283958435\n",
            "batch: 110 loss: 0.2971310615539551\n",
            "batch: 111 loss: 0.35379722714424133\n",
            "batch: 112 loss: 0.31921863555908203\n",
            "batch: 113 loss: 0.233592227101326\n",
            "batch: 114 loss: 0.26041311025619507\n",
            "batch: 115 loss: 0.35781726241111755\n",
            "batch: 116 loss: 0.2556886672973633\n",
            "batch: 117 loss: 0.35115334391593933\n",
            "batch: 118 loss: 0.25607097148895264\n",
            "batch: 119 loss: 0.26511162519454956\n",
            "batch: 120 loss: 0.37229907512664795\n",
            "batch: 121 loss: 0.3462381064891815\n",
            "batch: 122 loss: 0.3516564965248108\n",
            "batch: 123 loss: 0.3508445620536804\n",
            "batch: 124 loss: 0.26387616991996765\n",
            "batch: 125 loss: 0.2934257686138153\n",
            "batch: 126 loss: 0.24491021037101746\n",
            "batch: 127 loss: 0.3621530532836914\n",
            "batch: 128 loss: 0.3681394159793854\n",
            "batch: 129 loss: 0.23709158599376678\n",
            "batch: 130 loss: 0.28353819251060486\n",
            "batch: 131 loss: 0.4418943226337433\n",
            "batch: 132 loss: 0.3031795620918274\n",
            "batch: 133 loss: 0.3249194622039795\n",
            "batch: 134 loss: 0.31562283635139465\n",
            "batch: 135 loss: 0.27431926131248474\n",
            "batch: 136 loss: 0.29439806938171387\n",
            "batch: 137 loss: 0.30022579431533813\n",
            "batch: 138 loss: 0.26188111305236816\n",
            "batch: 139 loss: 0.3358466327190399\n",
            "batch: 140 loss: 0.3277273178100586\n",
            "batch: 141 loss: 0.5278759002685547\n",
            "batch: 142 loss: 0.26646900177001953\n",
            "batch: 143 loss: 0.29190197587013245\n",
            "batch: 144 loss: 0.28553593158721924\n",
            "batch: 145 loss: 0.2896665334701538\n",
            "batch: 146 loss: 0.29626619815826416\n",
            "LOSS train 0.30785030126571655 valid 0.311808705329895\n",
            "EPOCH 10:\n",
            "batch: 0 loss: 0.29411759972572327\n",
            "batch: 1 loss: 0.28210318088531494\n",
            "batch: 2 loss: 0.4267236590385437\n",
            "batch: 3 loss: 0.26698487997055054\n",
            "batch: 4 loss: 0.2405014932155609\n",
            "batch: 5 loss: 0.39335179328918457\n",
            "batch: 6 loss: 0.2584904432296753\n",
            "batch: 7 loss: 0.28614842891693115\n",
            "batch: 8 loss: 0.24907957017421722\n",
            "batch: 9 loss: 0.27951347827911377\n",
            "batch: 10 loss: 0.21480391919612885\n",
            "batch: 11 loss: 0.37972378730773926\n",
            "batch: 12 loss: 0.3579583168029785\n",
            "batch: 13 loss: 0.268090158700943\n",
            "batch: 14 loss: 0.31763458251953125\n",
            "batch: 15 loss: 0.33327075839042664\n",
            "batch: 16 loss: 0.38023772835731506\n",
            "batch: 17 loss: 0.4021858274936676\n",
            "batch: 18 loss: 0.3325960040092468\n",
            "batch: 19 loss: 0.3276924192905426\n",
            "batch: 20 loss: 0.3083302974700928\n",
            "batch: 21 loss: 0.3982742130756378\n",
            "batch: 22 loss: 0.3295775055885315\n",
            "batch: 23 loss: 0.31001490354537964\n",
            "batch: 24 loss: 0.2749796509742737\n",
            "batch: 25 loss: 0.23290301859378815\n",
            "batch: 26 loss: 0.32206210494041443\n",
            "batch: 27 loss: 0.270751953125\n",
            "batch: 28 loss: 0.24079173803329468\n",
            "batch: 29 loss: 0.25632038712501526\n",
            "batch: 30 loss: 0.2501804530620575\n",
            "batch: 31 loss: 0.3206249475479126\n",
            "batch: 32 loss: 0.30252760648727417\n",
            "batch: 33 loss: 0.45283809304237366\n",
            "batch: 34 loss: 0.2982920706272125\n",
            "batch: 35 loss: 0.28039664030075073\n",
            "batch: 36 loss: 0.27982380986213684\n",
            "batch: 37 loss: 0.28737956285476685\n",
            "batch: 38 loss: 0.3671032190322876\n",
            "batch: 39 loss: 0.25601160526275635\n",
            "batch: 40 loss: 0.3239254057407379\n",
            "batch: 41 loss: 0.2518753409385681\n",
            "batch: 42 loss: 0.3051135540008545\n",
            "batch: 43 loss: 0.3430442810058594\n",
            "batch: 44 loss: 0.2306477576494217\n",
            "batch: 45 loss: 0.2769957482814789\n",
            "batch: 46 loss: 0.30582910776138306\n",
            "batch: 47 loss: 0.2459876537322998\n",
            "batch: 48 loss: 0.2771550416946411\n",
            "batch: 49 loss: 0.31569141149520874\n",
            "batch: 50 loss: 0.26145440340042114\n",
            "batch: 51 loss: 0.29901641607284546\n",
            "batch: 52 loss: 0.296358585357666\n",
            "batch: 53 loss: 0.3169326186180115\n",
            "batch: 54 loss: 0.25247469544410706\n",
            "batch: 55 loss: 0.3000447452068329\n",
            "batch: 56 loss: 0.2923715114593506\n",
            "batch: 57 loss: 0.291057288646698\n",
            "batch: 58 loss: 0.35916072130203247\n",
            "batch: 59 loss: 0.29804039001464844\n",
            "batch: 60 loss: 0.2717922329902649\n",
            "batch: 61 loss: 0.3467084765434265\n",
            "batch: 62 loss: 0.36023417115211487\n",
            "batch: 63 loss: 0.30611246824264526\n",
            "batch: 64 loss: 0.3221491575241089\n",
            "batch: 65 loss: 0.3482873737812042\n",
            "batch: 66 loss: 0.24213150143623352\n",
            "batch: 67 loss: 0.24106010794639587\n",
            "batch: 68 loss: 0.3918628394603729\n",
            "batch: 69 loss: 0.2563096284866333\n",
            "batch: 70 loss: 0.35167592763900757\n",
            "batch: 71 loss: 0.26140666007995605\n",
            "batch: 72 loss: 0.322309672832489\n",
            "batch: 73 loss: 0.24780432879924774\n",
            "batch: 74 loss: 0.2967521846294403\n",
            "batch: 75 loss: 0.3065853714942932\n",
            "batch: 76 loss: 0.3344038128852844\n",
            "batch: 77 loss: 0.23613278567790985\n",
            "batch: 78 loss: 0.31011879444122314\n",
            "batch: 79 loss: 0.24036704003810883\n",
            "batch: 80 loss: 0.24203185737133026\n",
            "batch: 81 loss: 0.3703638017177582\n",
            "batch: 82 loss: 0.2702566981315613\n",
            "batch: 83 loss: 0.22845056653022766\n",
            "batch: 84 loss: 0.3093845248222351\n",
            "batch: 85 loss: 0.2701082229614258\n",
            "batch: 86 loss: 0.21877828240394592\n",
            "batch: 87 loss: 0.39915409684181213\n",
            "batch: 88 loss: 0.33402037620544434\n",
            "batch: 89 loss: 0.264856219291687\n",
            "batch: 90 loss: 0.37178704142570496\n",
            "batch: 91 loss: 0.3447679281234741\n",
            "batch: 92 loss: 0.3790833055973053\n",
            "batch: 93 loss: 0.2697879672050476\n",
            "batch: 94 loss: 0.30444037914276123\n",
            "batch: 95 loss: 0.2579494118690491\n",
            "batch: 96 loss: 0.3091040253639221\n",
            "batch: 97 loss: 0.2688029408454895\n",
            "batch: 98 loss: 0.2632295489311218\n",
            "batch: 99 loss: 0.2596473693847656\n",
            "batch: 100 loss: 0.23205460608005524\n",
            "batch: 101 loss: 0.5941505432128906\n",
            "batch: 102 loss: 0.5059331059455872\n",
            "batch: 103 loss: 0.2974422574043274\n",
            "batch: 104 loss: 0.3192247748374939\n",
            "batch: 105 loss: 0.346290647983551\n",
            "batch: 106 loss: 0.30468684434890747\n",
            "batch: 107 loss: 0.24862779676914215\n",
            "batch: 108 loss: 0.2508849799633026\n",
            "batch: 109 loss: 0.49446386098861694\n",
            "batch: 110 loss: 0.38440221548080444\n",
            "batch: 111 loss: 0.2765142321586609\n",
            "batch: 112 loss: 0.2827019989490509\n",
            "batch: 113 loss: 0.25428760051727295\n",
            "batch: 114 loss: 0.3110998868942261\n",
            "batch: 115 loss: 0.324146568775177\n",
            "batch: 116 loss: 0.346069872379303\n",
            "batch: 117 loss: 0.299946129322052\n",
            "batch: 118 loss: 0.3208533823490143\n",
            "batch: 119 loss: 0.27740487456321716\n",
            "batch: 120 loss: 0.2513895034790039\n",
            "batch: 121 loss: 0.3009084463119507\n",
            "batch: 122 loss: 0.282598614692688\n",
            "batch: 123 loss: 0.404641330242157\n",
            "batch: 124 loss: 0.24438874423503876\n",
            "batch: 125 loss: 0.24517354369163513\n",
            "batch: 126 loss: 0.2643698751926422\n",
            "batch: 127 loss: 0.39756470918655396\n",
            "batch: 128 loss: 0.2797277569770813\n",
            "batch: 129 loss: 0.22580626606941223\n",
            "batch: 130 loss: 0.25569164752960205\n",
            "batch: 131 loss: 0.27072715759277344\n",
            "batch: 132 loss: 0.3755779266357422\n",
            "batch: 133 loss: 0.31639739871025085\n",
            "batch: 134 loss: 0.3253626823425293\n",
            "batch: 135 loss: 0.3319307565689087\n",
            "batch: 136 loss: 0.3244408071041107\n",
            "batch: 137 loss: 0.2948502004146576\n",
            "batch: 138 loss: 0.2818072736263275\n",
            "batch: 139 loss: 0.2501378655433655\n",
            "batch: 140 loss: 0.33823907375335693\n",
            "batch: 141 loss: 0.40354618430137634\n",
            "batch: 142 loss: 0.3398047089576721\n",
            "batch: 143 loss: 0.21185627579689026\n",
            "batch: 144 loss: 0.31742632389068604\n",
            "batch: 145 loss: 0.2775827646255493\n",
            "batch: 146 loss: 0.3663182258605957\n",
            "LOSS train 0.32540664076805115 valid 0.33809342980384827\n",
            "EPOCH 11:\n",
            "batch: 0 loss: 0.3864970803260803\n",
            "batch: 1 loss: 0.26346999406814575\n",
            "batch: 2 loss: 0.2567370533943176\n",
            "batch: 3 loss: 0.26221656799316406\n",
            "batch: 4 loss: 0.2842137813568115\n",
            "batch: 5 loss: 0.25556057691574097\n",
            "batch: 6 loss: 0.2995184659957886\n",
            "batch: 7 loss: 0.2882218360900879\n",
            "batch: 8 loss: 0.3405138850212097\n",
            "batch: 9 loss: 0.2831525206565857\n",
            "batch: 10 loss: 0.41460826992988586\n",
            "batch: 11 loss: 0.2175988405942917\n",
            "batch: 12 loss: 0.31065601110458374\n",
            "batch: 13 loss: 0.3814701437950134\n",
            "batch: 14 loss: 0.3113734722137451\n",
            "batch: 15 loss: 0.2664147615432739\n",
            "batch: 16 loss: 0.4030958116054535\n",
            "batch: 17 loss: 0.2889671325683594\n",
            "batch: 18 loss: 0.3445703387260437\n",
            "batch: 19 loss: 0.37678536772727966\n",
            "batch: 20 loss: 0.2817785143852234\n",
            "batch: 21 loss: 0.35980188846588135\n",
            "batch: 22 loss: 0.47383546829223633\n",
            "batch: 23 loss: 0.2799268662929535\n",
            "batch: 24 loss: 0.24858298897743225\n",
            "batch: 25 loss: 0.28281769156455994\n",
            "batch: 26 loss: 0.2471398115158081\n",
            "batch: 27 loss: 0.3458137512207031\n",
            "batch: 28 loss: 0.2458372414112091\n",
            "batch: 29 loss: 0.31251609325408936\n",
            "batch: 30 loss: 0.2386280596256256\n",
            "batch: 31 loss: 0.325656920671463\n",
            "batch: 32 loss: 0.31253862380981445\n",
            "batch: 33 loss: 0.23260250687599182\n",
            "batch: 34 loss: 0.2894447445869446\n",
            "batch: 35 loss: 0.34646520018577576\n",
            "batch: 36 loss: 0.3671395778656006\n",
            "batch: 37 loss: 0.25502943992614746\n",
            "batch: 38 loss: 0.2809153199195862\n",
            "batch: 39 loss: 0.2706371545791626\n",
            "batch: 40 loss: 0.25592994689941406\n",
            "batch: 41 loss: 0.3360792398452759\n",
            "batch: 42 loss: 0.23299363255500793\n",
            "batch: 43 loss: 0.3677099347114563\n",
            "batch: 44 loss: 0.25507432222366333\n",
            "batch: 45 loss: 0.24453124403953552\n",
            "batch: 46 loss: 0.2958410680294037\n",
            "batch: 47 loss: 0.32721203565597534\n",
            "batch: 48 loss: 0.223555326461792\n",
            "batch: 49 loss: 0.2764282524585724\n",
            "batch: 50 loss: 0.33807727694511414\n",
            "batch: 51 loss: 0.3343702554702759\n",
            "batch: 52 loss: 0.3062615692615509\n",
            "batch: 53 loss: 0.27953118085861206\n",
            "batch: 54 loss: 0.29495948553085327\n",
            "batch: 55 loss: 0.2749735116958618\n",
            "batch: 56 loss: 0.2355482280254364\n",
            "batch: 57 loss: 0.2519228458404541\n",
            "batch: 58 loss: 0.33909058570861816\n",
            "batch: 59 loss: 0.18815520405769348\n",
            "batch: 60 loss: 0.2823861837387085\n",
            "batch: 61 loss: 0.2843058109283447\n",
            "batch: 62 loss: 0.2765779495239258\n",
            "batch: 63 loss: 0.21585692465305328\n",
            "batch: 64 loss: 0.235280379652977\n",
            "batch: 65 loss: 0.2577999234199524\n",
            "batch: 66 loss: 0.2549172043800354\n",
            "batch: 67 loss: 0.2125910371541977\n",
            "batch: 68 loss: 0.28848502039909363\n",
            "batch: 69 loss: 0.24106484651565552\n",
            "batch: 70 loss: 0.2588934302330017\n",
            "batch: 71 loss: 0.2945839762687683\n",
            "batch: 72 loss: 0.2768712639808655\n",
            "batch: 73 loss: 0.34929510951042175\n",
            "batch: 74 loss: 0.27781105041503906\n",
            "batch: 75 loss: 0.2391793429851532\n",
            "batch: 76 loss: 0.33481311798095703\n",
            "batch: 77 loss: 0.3396357297897339\n",
            "batch: 78 loss: 0.28975027799606323\n",
            "batch: 79 loss: 0.30563077330589294\n",
            "batch: 80 loss: 0.2072029560804367\n",
            "batch: 81 loss: 0.3084511458873749\n",
            "batch: 82 loss: 0.27212756872177124\n",
            "batch: 83 loss: 0.3582279086112976\n",
            "batch: 84 loss: 0.31176671385765076\n",
            "batch: 85 loss: 0.27180320024490356\n",
            "batch: 86 loss: 0.3074535131454468\n",
            "batch: 87 loss: 0.2706117033958435\n",
            "batch: 88 loss: 0.3198489546775818\n",
            "batch: 89 loss: 0.19883792102336884\n",
            "batch: 90 loss: 0.2675197124481201\n",
            "batch: 91 loss: 0.2401624619960785\n",
            "batch: 92 loss: 0.30564016103744507\n",
            "batch: 93 loss: 0.2380281388759613\n",
            "batch: 94 loss: 0.4306366741657257\n",
            "batch: 95 loss: 0.2488493025302887\n",
            "batch: 96 loss: 0.2713809609413147\n",
            "batch: 97 loss: 0.28564009070396423\n",
            "batch: 98 loss: 0.2757706940174103\n",
            "batch: 99 loss: 0.22335273027420044\n",
            "batch: 100 loss: 0.39815080165863037\n",
            "batch: 101 loss: 0.3712388575077057\n",
            "batch: 102 loss: 0.3373613655567169\n",
            "batch: 103 loss: 0.32825177907943726\n",
            "batch: 104 loss: 0.3005833327770233\n",
            "batch: 105 loss: 0.26431533694267273\n",
            "batch: 106 loss: 0.2699814438819885\n",
            "batch: 107 loss: 0.3529146909713745\n",
            "batch: 108 loss: 0.36160004138946533\n",
            "batch: 109 loss: 0.2748451232910156\n",
            "batch: 110 loss: 0.31518030166625977\n",
            "batch: 111 loss: 0.309305340051651\n",
            "batch: 112 loss: 0.40572023391723633\n",
            "batch: 113 loss: 0.2444351464509964\n",
            "batch: 114 loss: 0.2977405786514282\n",
            "batch: 115 loss: 0.2557276487350464\n",
            "batch: 116 loss: 0.24168884754180908\n",
            "batch: 117 loss: 0.3184492290019989\n",
            "batch: 118 loss: 0.2509347200393677\n",
            "batch: 119 loss: 0.252865731716156\n",
            "batch: 120 loss: 0.31335821747779846\n",
            "batch: 121 loss: 0.37133365869522095\n",
            "batch: 122 loss: 0.2372063398361206\n",
            "batch: 123 loss: 0.2906374931335449\n",
            "batch: 124 loss: 0.24351078271865845\n",
            "batch: 125 loss: 0.3685983419418335\n",
            "batch: 126 loss: 0.2695084810256958\n",
            "batch: 127 loss: 0.24357669055461884\n",
            "batch: 128 loss: 0.27475014328956604\n",
            "batch: 129 loss: 0.2920020520687103\n",
            "batch: 130 loss: 0.2213352769613266\n",
            "batch: 131 loss: 0.34238114953041077\n",
            "batch: 132 loss: 0.3465489149093628\n",
            "batch: 133 loss: 0.3789884150028229\n",
            "batch: 134 loss: 0.2093445509672165\n",
            "batch: 135 loss: 0.2542794942855835\n",
            "batch: 136 loss: 0.25922784209251404\n",
            "batch: 137 loss: 0.2573910057544708\n",
            "batch: 138 loss: 0.36189383268356323\n",
            "batch: 139 loss: 0.22851109504699707\n",
            "batch: 140 loss: 0.26456111669540405\n",
            "batch: 141 loss: 0.3043721914291382\n",
            "batch: 142 loss: 0.2689860463142395\n",
            "batch: 143 loss: 0.29251790046691895\n",
            "batch: 144 loss: 0.36410194635391235\n",
            "batch: 145 loss: 0.2270302027463913\n",
            "batch: 146 loss: 0.29663050174713135\n",
            "LOSS train 0.27996891736984253 valid 0.2878045439720154\n",
            "EPOCH 12:\n",
            "batch: 0 loss: 0.2418603152036667\n",
            "batch: 1 loss: 0.2591189742088318\n",
            "batch: 2 loss: 0.2651292681694031\n",
            "batch: 3 loss: 0.25977635383605957\n",
            "batch: 4 loss: 0.2603859305381775\n",
            "batch: 5 loss: 0.30528655648231506\n",
            "batch: 6 loss: 0.26912206411361694\n",
            "batch: 7 loss: 0.26343607902526855\n",
            "batch: 8 loss: 0.37092074751853943\n",
            "batch: 9 loss: 0.25715792179107666\n",
            "batch: 10 loss: 0.26109570264816284\n",
            "batch: 11 loss: 0.2891066074371338\n",
            "batch: 12 loss: 0.23439887166023254\n",
            "batch: 13 loss: 0.2414505034685135\n",
            "batch: 14 loss: 0.26068466901779175\n",
            "batch: 15 loss: 0.23162208497524261\n",
            "batch: 16 loss: 0.3947542607784271\n",
            "batch: 17 loss: 0.2972561717033386\n",
            "batch: 18 loss: 0.3320366144180298\n",
            "batch: 19 loss: 0.23966924846172333\n",
            "batch: 20 loss: 0.29507359862327576\n",
            "batch: 21 loss: 0.28621524572372437\n",
            "batch: 22 loss: 0.3718458414077759\n",
            "batch: 23 loss: 0.2762056589126587\n",
            "batch: 24 loss: 0.2584160566329956\n",
            "batch: 25 loss: 0.3012511730194092\n",
            "batch: 26 loss: 0.2444630116224289\n",
            "batch: 27 loss: 0.22741056978702545\n",
            "batch: 28 loss: 0.28715288639068604\n",
            "batch: 29 loss: 0.2551630437374115\n",
            "batch: 30 loss: 0.3380952477455139\n",
            "batch: 31 loss: 0.35839593410491943\n",
            "batch: 32 loss: 0.2734643816947937\n",
            "batch: 33 loss: 0.25562477111816406\n",
            "batch: 34 loss: 0.2696968913078308\n",
            "batch: 35 loss: 0.25995010137557983\n",
            "batch: 36 loss: 0.3135344982147217\n",
            "batch: 37 loss: 0.3225076198577881\n",
            "batch: 38 loss: 0.20393432676792145\n",
            "batch: 39 loss: 0.4218662679195404\n",
            "batch: 40 loss: 0.28687745332717896\n",
            "batch: 41 loss: 0.4020041823387146\n",
            "batch: 42 loss: 0.23834170401096344\n",
            "batch: 43 loss: 0.33450278639793396\n",
            "batch: 44 loss: 0.32275956869125366\n",
            "batch: 45 loss: 0.3569256663322449\n",
            "batch: 46 loss: 0.275126576423645\n",
            "batch: 47 loss: 0.3126377463340759\n",
            "batch: 48 loss: 0.2679066061973572\n",
            "batch: 49 loss: 0.2848584055900574\n",
            "batch: 50 loss: 0.3093160092830658\n",
            "batch: 51 loss: 0.3759806752204895\n",
            "batch: 52 loss: 0.2844085097312927\n",
            "batch: 53 loss: 0.2606928050518036\n",
            "batch: 54 loss: 0.2428749054670334\n",
            "batch: 55 loss: 0.2846783399581909\n",
            "batch: 56 loss: 0.33337900042533875\n",
            "batch: 57 loss: 0.24462604522705078\n",
            "batch: 58 loss: 0.29321718215942383\n",
            "batch: 59 loss: 0.2679438591003418\n",
            "batch: 60 loss: 0.3205220699310303\n",
            "batch: 61 loss: 0.28109097480773926\n",
            "batch: 62 loss: 0.2394152134656906\n",
            "batch: 63 loss: 0.25891879200935364\n",
            "batch: 64 loss: 0.27901846170425415\n",
            "batch: 65 loss: 0.30401915311813354\n",
            "batch: 66 loss: 0.284224271774292\n",
            "batch: 67 loss: 0.2889172434806824\n",
            "batch: 68 loss: 0.3053639531135559\n",
            "batch: 69 loss: 0.25094330310821533\n",
            "batch: 70 loss: 0.34287941455841064\n",
            "batch: 71 loss: 0.3003627359867096\n",
            "batch: 72 loss: 0.4117584228515625\n",
            "batch: 73 loss: 0.3503897786140442\n",
            "batch: 74 loss: 0.3116535544395447\n",
            "batch: 75 loss: 0.2823152244091034\n",
            "batch: 76 loss: 0.24370424449443817\n",
            "batch: 77 loss: 0.2621411383152008\n",
            "batch: 78 loss: 0.25547704100608826\n",
            "batch: 79 loss: 0.24804756045341492\n",
            "batch: 80 loss: 0.23065246641635895\n",
            "batch: 81 loss: 0.2343813180923462\n",
            "batch: 82 loss: 0.27681270241737366\n",
            "batch: 83 loss: 0.27197712659835815\n",
            "batch: 84 loss: 0.2212342768907547\n",
            "batch: 85 loss: 0.2617969810962677\n",
            "batch: 86 loss: 0.258047491312027\n",
            "batch: 87 loss: 0.291643351316452\n",
            "batch: 88 loss: 0.35067737102508545\n",
            "batch: 89 loss: 0.32750242948532104\n",
            "batch: 90 loss: 0.29267793893814087\n",
            "batch: 91 loss: 0.24032220244407654\n",
            "batch: 92 loss: 0.2932603359222412\n",
            "batch: 93 loss: 0.3259658217430115\n",
            "batch: 94 loss: 0.287908136844635\n",
            "batch: 95 loss: 0.2945941388607025\n",
            "batch: 96 loss: 0.2761959433555603\n",
            "batch: 97 loss: 0.24334929883480072\n",
            "batch: 98 loss: 0.25786638259887695\n",
            "batch: 99 loss: 0.22286231815814972\n",
            "batch: 100 loss: 0.29897570610046387\n",
            "batch: 101 loss: 0.24620166420936584\n",
            "batch: 102 loss: 0.24677202105522156\n",
            "batch: 103 loss: 0.30008137226104736\n",
            "batch: 104 loss: 0.2568050026893616\n",
            "batch: 105 loss: 0.22223243117332458\n",
            "batch: 106 loss: 0.27458059787750244\n",
            "batch: 107 loss: 0.26695185899734497\n",
            "batch: 108 loss: 0.2579598128795624\n",
            "batch: 109 loss: 0.3419106602668762\n",
            "batch: 110 loss: 0.2615325450897217\n",
            "batch: 111 loss: 0.24336442351341248\n",
            "batch: 112 loss: 0.23054197430610657\n",
            "batch: 113 loss: 0.36097100377082825\n",
            "batch: 114 loss: 0.3623783588409424\n",
            "batch: 115 loss: 0.3260498642921448\n",
            "batch: 116 loss: 0.26870912313461304\n",
            "batch: 117 loss: 0.2676889896392822\n",
            "batch: 118 loss: 0.2690160274505615\n",
            "batch: 119 loss: 0.2782271206378937\n",
            "batch: 120 loss: 0.2562642991542816\n",
            "batch: 121 loss: 0.3058721423149109\n",
            "batch: 122 loss: 0.2415464073419571\n",
            "batch: 123 loss: 0.28335440158843994\n",
            "batch: 124 loss: 0.3278948962688446\n",
            "batch: 125 loss: 0.28132790327072144\n",
            "batch: 126 loss: 0.2263266146183014\n",
            "batch: 127 loss: 0.3198849558830261\n",
            "batch: 128 loss: 0.3458162844181061\n",
            "batch: 129 loss: 0.2524297833442688\n",
            "batch: 130 loss: 0.2903634309768677\n",
            "batch: 131 loss: 0.24547794461250305\n",
            "batch: 132 loss: 0.3744291365146637\n",
            "batch: 133 loss: 0.27319812774658203\n",
            "batch: 134 loss: 0.2394780069589615\n",
            "batch: 135 loss: 0.2862284183502197\n",
            "batch: 136 loss: 0.24872751533985138\n",
            "batch: 137 loss: 0.2713419795036316\n",
            "batch: 138 loss: 0.2657780349254608\n",
            "batch: 139 loss: 0.3183901906013489\n",
            "batch: 140 loss: 0.2639007568359375\n",
            "batch: 141 loss: 0.24254198372364044\n",
            "batch: 142 loss: 0.24293681979179382\n",
            "batch: 143 loss: 0.26050907373428345\n",
            "batch: 144 loss: 0.26825836300849915\n",
            "batch: 145 loss: 0.29989194869995117\n",
            "batch: 146 loss: 0.2712787091732025\n",
            "LOSS train 0.27396541833877563 valid 0.2834395468235016\n",
            "EPOCH 13:\n",
            "batch: 0 loss: 0.24611492455005646\n",
            "batch: 1 loss: 0.2570515275001526\n",
            "batch: 2 loss: 0.2510630488395691\n",
            "batch: 3 loss: 0.31997859477996826\n",
            "batch: 4 loss: 0.22947107255458832\n",
            "batch: 5 loss: 0.21140934526920319\n",
            "batch: 6 loss: 0.2972952723503113\n",
            "batch: 7 loss: 0.3897703289985657\n",
            "batch: 8 loss: 0.29512351751327515\n",
            "batch: 9 loss: 0.30957749485969543\n",
            "batch: 10 loss: 0.313356876373291\n",
            "batch: 11 loss: 0.2474963665008545\n",
            "batch: 12 loss: 0.33599478006362915\n",
            "batch: 13 loss: 0.29780733585357666\n",
            "batch: 14 loss: 0.28072959184646606\n",
            "batch: 15 loss: 0.29614385962486267\n",
            "batch: 16 loss: 0.3061445951461792\n",
            "batch: 17 loss: 0.28693825006484985\n",
            "batch: 18 loss: 0.24205133318901062\n",
            "batch: 19 loss: 0.2246417999267578\n",
            "batch: 20 loss: 0.31602704524993896\n",
            "batch: 21 loss: 0.2524777352809906\n",
            "batch: 22 loss: 0.31338900327682495\n",
            "batch: 23 loss: 0.33798128366470337\n",
            "batch: 24 loss: 0.22833651304244995\n",
            "batch: 25 loss: 0.23106740415096283\n",
            "batch: 26 loss: 0.29224592447280884\n",
            "batch: 27 loss: 0.25824058055877686\n",
            "batch: 28 loss: 0.3056780993938446\n",
            "batch: 29 loss: 0.213889479637146\n",
            "batch: 30 loss: 0.22127729654312134\n",
            "batch: 31 loss: 0.24625243246555328\n",
            "batch: 32 loss: 0.2813093662261963\n",
            "batch: 33 loss: 0.2767573893070221\n",
            "batch: 34 loss: 0.29123902320861816\n",
            "batch: 35 loss: 0.24899353086948395\n",
            "batch: 36 loss: 0.24933265149593353\n",
            "batch: 37 loss: 0.24167200922966003\n",
            "batch: 38 loss: 0.28389209508895874\n",
            "batch: 39 loss: 0.2503032982349396\n",
            "batch: 40 loss: 0.23989500105381012\n",
            "batch: 41 loss: 0.2897270917892456\n",
            "batch: 42 loss: 0.20262698829174042\n",
            "batch: 43 loss: 0.24850688874721527\n",
            "batch: 44 loss: 0.2637239694595337\n",
            "batch: 45 loss: 0.2805578112602234\n",
            "batch: 46 loss: 0.27218469977378845\n",
            "batch: 47 loss: 0.2219531536102295\n",
            "batch: 48 loss: 0.29323163628578186\n",
            "batch: 49 loss: 0.34199294447898865\n",
            "batch: 50 loss: 0.29508358240127563\n",
            "batch: 51 loss: 0.27329397201538086\n",
            "batch: 52 loss: 0.2354985773563385\n",
            "batch: 53 loss: 0.3181314766407013\n",
            "batch: 54 loss: 0.36226198077201843\n",
            "batch: 55 loss: 0.2805485725402832\n",
            "batch: 56 loss: 0.26230454444885254\n",
            "batch: 57 loss: 0.3105633854866028\n",
            "batch: 58 loss: 0.2812986373901367\n",
            "batch: 59 loss: 0.24037036299705505\n",
            "batch: 60 loss: 0.30484530329704285\n",
            "batch: 61 loss: 0.30106645822525024\n",
            "batch: 62 loss: 0.2989357113838196\n",
            "batch: 63 loss: 0.2384660392999649\n",
            "batch: 64 loss: 0.33702877163887024\n",
            "batch: 65 loss: 0.32064393162727356\n",
            "batch: 66 loss: 0.2003021240234375\n",
            "batch: 67 loss: 0.28955191373825073\n",
            "batch: 68 loss: 0.24608851969242096\n",
            "batch: 69 loss: 0.22416403889656067\n",
            "batch: 70 loss: 0.23899388313293457\n",
            "batch: 71 loss: 0.2914237380027771\n",
            "batch: 72 loss: 0.23655389249324799\n",
            "batch: 73 loss: 0.36673638224601746\n",
            "batch: 74 loss: 0.2902127802371979\n",
            "batch: 75 loss: 0.22421248257160187\n",
            "batch: 76 loss: 0.2388067990541458\n",
            "batch: 77 loss: 0.19659771025180817\n",
            "batch: 78 loss: 0.21386520564556122\n",
            "batch: 79 loss: 0.24341049790382385\n",
            "batch: 80 loss: 0.24730980396270752\n",
            "batch: 81 loss: 0.2446409910917282\n",
            "batch: 82 loss: 0.2340448498725891\n",
            "batch: 83 loss: 0.22677238285541534\n",
            "batch: 84 loss: 0.2504204511642456\n",
            "batch: 85 loss: 0.29616886377334595\n",
            "batch: 86 loss: 0.25547558069229126\n",
            "batch: 87 loss: 0.23100212216377258\n",
            "batch: 88 loss: 0.32576632499694824\n",
            "batch: 89 loss: 0.2874528169631958\n",
            "batch: 90 loss: 0.2503461241722107\n",
            "batch: 91 loss: 0.2738179564476013\n",
            "batch: 92 loss: 0.21822872757911682\n",
            "batch: 93 loss: 0.3472668528556824\n",
            "batch: 94 loss: 0.2662736177444458\n",
            "batch: 95 loss: 0.2613540291786194\n",
            "batch: 96 loss: 0.276958703994751\n",
            "batch: 97 loss: 0.31374600529670715\n",
            "batch: 98 loss: 0.221432164311409\n",
            "batch: 99 loss: 0.2919304668903351\n",
            "batch: 100 loss: 0.26436638832092285\n",
            "batch: 101 loss: 0.3557731509208679\n",
            "batch: 102 loss: 0.3385903239250183\n",
            "batch: 103 loss: 0.3410753905773163\n",
            "batch: 104 loss: 0.31688424944877625\n",
            "batch: 105 loss: 0.3026401996612549\n",
            "batch: 106 loss: 0.2390967607498169\n",
            "batch: 107 loss: 0.2736753821372986\n",
            "batch: 108 loss: 0.277819961309433\n",
            "batch: 109 loss: 0.2330918163061142\n",
            "batch: 110 loss: 0.1671966165304184\n",
            "batch: 111 loss: 0.25247418880462646\n",
            "batch: 112 loss: 0.25521185994148254\n",
            "batch: 113 loss: 0.2568904757499695\n",
            "batch: 114 loss: 0.24435187876224518\n",
            "batch: 115 loss: 0.2823474407196045\n",
            "batch: 116 loss: 0.2424992173910141\n",
            "batch: 117 loss: 0.2758824825286865\n",
            "batch: 118 loss: 0.29753565788269043\n",
            "batch: 119 loss: 0.2835443615913391\n",
            "batch: 120 loss: 0.27848607301712036\n",
            "batch: 121 loss: 0.2833397388458252\n",
            "batch: 122 loss: 0.29911136627197266\n",
            "batch: 123 loss: 0.22463388741016388\n",
            "batch: 124 loss: 0.2969881296157837\n",
            "batch: 125 loss: 0.27176594734191895\n",
            "batch: 126 loss: 0.2672739028930664\n",
            "batch: 127 loss: 0.20437325537204742\n",
            "batch: 128 loss: 0.3024279773235321\n",
            "batch: 129 loss: 0.30695241689682007\n",
            "batch: 130 loss: 0.2617502212524414\n",
            "batch: 131 loss: 0.3632831573486328\n",
            "batch: 132 loss: 0.337323933839798\n",
            "batch: 133 loss: 0.259970098733902\n",
            "batch: 134 loss: 0.27121174335479736\n",
            "batch: 135 loss: 0.24219205975532532\n",
            "batch: 136 loss: 0.2228500097990036\n",
            "batch: 137 loss: 0.18852797150611877\n",
            "batch: 138 loss: 0.22960762679576874\n",
            "batch: 139 loss: 0.2609703838825226\n",
            "batch: 140 loss: 0.2747536897659302\n",
            "batch: 141 loss: 0.33548760414123535\n",
            "batch: 142 loss: 0.2790990471839905\n",
            "batch: 143 loss: 0.30056220293045044\n",
            "batch: 144 loss: 0.3187277615070343\n",
            "batch: 145 loss: 0.4871968626976013\n",
            "batch: 146 loss: 0.31007224321365356\n",
            "LOSS train 0.329874724149704 valid 0.33686932921409607\n",
            "EPOCH 14:\n",
            "batch: 0 loss: 0.28383541107177734\n",
            "batch: 1 loss: 0.2113640457391739\n",
            "batch: 2 loss: 0.2842181324958801\n",
            "batch: 3 loss: 0.18059974908828735\n",
            "batch: 4 loss: 0.3649980127811432\n",
            "batch: 5 loss: 0.2882457971572876\n",
            "batch: 6 loss: 0.30890822410583496\n",
            "batch: 7 loss: 0.32911157608032227\n",
            "batch: 8 loss: 0.2799527645111084\n",
            "batch: 9 loss: 0.2968970537185669\n",
            "batch: 10 loss: 0.3141404092311859\n",
            "batch: 11 loss: 0.24237561225891113\n",
            "batch: 12 loss: 0.21659760177135468\n",
            "batch: 13 loss: 0.2914295196533203\n",
            "batch: 14 loss: 0.26021477580070496\n",
            "batch: 15 loss: 0.17524221539497375\n",
            "batch: 16 loss: 0.27005821466445923\n",
            "batch: 17 loss: 0.262847363948822\n",
            "batch: 18 loss: 0.28891903162002563\n",
            "batch: 19 loss: 0.3750261962413788\n",
            "batch: 20 loss: 0.3598192036151886\n",
            "batch: 21 loss: 0.24526330828666687\n",
            "batch: 22 loss: 0.2634163200855255\n",
            "batch: 23 loss: 0.2421293556690216\n",
            "batch: 24 loss: 0.2469673603773117\n",
            "batch: 25 loss: 0.2874174118041992\n",
            "batch: 26 loss: 0.20467188954353333\n",
            "batch: 27 loss: 0.2997514009475708\n",
            "batch: 28 loss: 0.21387217938899994\n",
            "batch: 29 loss: 0.2382878214120865\n",
            "batch: 30 loss: 0.1893477439880371\n",
            "batch: 31 loss: 0.3105070888996124\n",
            "batch: 32 loss: 0.3856813907623291\n",
            "batch: 33 loss: 0.3062712252140045\n",
            "batch: 34 loss: 0.23894281685352325\n",
            "batch: 35 loss: 0.3627648651599884\n",
            "batch: 36 loss: 0.2028571367263794\n",
            "batch: 37 loss: 0.3001866936683655\n",
            "batch: 38 loss: 0.23959355056285858\n",
            "batch: 39 loss: 0.18735021352767944\n",
            "batch: 40 loss: 0.24692538380622864\n",
            "batch: 41 loss: 0.30815473198890686\n",
            "batch: 42 loss: 0.28279146552085876\n",
            "batch: 43 loss: 0.27263277769088745\n",
            "batch: 44 loss: 0.23104146122932434\n",
            "batch: 45 loss: 0.232203409075737\n",
            "batch: 46 loss: 0.3126641511917114\n",
            "batch: 47 loss: 0.23185297846794128\n",
            "batch: 48 loss: 0.24933789670467377\n",
            "batch: 49 loss: 0.2563663125038147\n",
            "batch: 50 loss: 0.25384023785591125\n",
            "batch: 51 loss: 0.30479806661605835\n",
            "batch: 52 loss: 0.2334621548652649\n",
            "batch: 53 loss: 0.3067515194416046\n",
            "batch: 54 loss: 0.2761157155036926\n",
            "batch: 55 loss: 0.23840443789958954\n",
            "batch: 56 loss: 0.2806578278541565\n",
            "batch: 57 loss: 0.29669636487960815\n",
            "batch: 58 loss: 0.23672543466091156\n",
            "batch: 59 loss: 0.22365929186344147\n",
            "batch: 60 loss: 0.30532407760620117\n",
            "batch: 61 loss: 0.25431323051452637\n",
            "batch: 62 loss: 0.21142709255218506\n",
            "batch: 63 loss: 0.2520183324813843\n",
            "batch: 64 loss: 0.221795916557312\n",
            "batch: 65 loss: 0.29232388734817505\n",
            "batch: 66 loss: 0.3338594138622284\n",
            "batch: 67 loss: 0.3221225440502167\n",
            "batch: 68 loss: 0.2341664880514145\n",
            "batch: 69 loss: 0.20156912505626678\n",
            "batch: 70 loss: 0.27923038601875305\n",
            "batch: 71 loss: 0.28170737624168396\n",
            "batch: 72 loss: 0.2744133472442627\n",
            "batch: 73 loss: 0.19804076850414276\n",
            "batch: 74 loss: 0.2327243685722351\n",
            "batch: 75 loss: 0.25808262825012207\n",
            "batch: 76 loss: 0.25988131761550903\n",
            "batch: 77 loss: 0.29523158073425293\n",
            "batch: 78 loss: 0.2183070331811905\n",
            "batch: 79 loss: 0.21560177206993103\n",
            "batch: 80 loss: 0.3383222222328186\n",
            "batch: 81 loss: 0.2445516586303711\n",
            "batch: 82 loss: 0.23489509522914886\n",
            "batch: 83 loss: 0.31232163310050964\n",
            "batch: 84 loss: 0.22096693515777588\n",
            "batch: 85 loss: 0.25323641300201416\n",
            "batch: 86 loss: 0.2787668704986572\n",
            "batch: 87 loss: 0.24111302196979523\n",
            "batch: 88 loss: 0.33888208866119385\n",
            "batch: 89 loss: 0.2237386852502823\n",
            "batch: 90 loss: 0.2548930048942566\n",
            "batch: 91 loss: 0.25135475397109985\n",
            "batch: 92 loss: 0.3253301680088043\n",
            "batch: 93 loss: 0.22205817699432373\n",
            "batch: 94 loss: 0.3570963144302368\n",
            "batch: 95 loss: 0.2850647568702698\n",
            "batch: 96 loss: 0.23213933408260345\n",
            "batch: 97 loss: 0.24141468107700348\n",
            "batch: 98 loss: 0.2845960855484009\n",
            "batch: 99 loss: 0.22927573323249817\n",
            "batch: 100 loss: 0.2535581588745117\n",
            "batch: 101 loss: 0.23156635463237762\n",
            "batch: 102 loss: 0.20306876301765442\n",
            "batch: 103 loss: 0.22063544392585754\n",
            "batch: 104 loss: 0.21563434600830078\n",
            "batch: 105 loss: 0.2877534031867981\n",
            "batch: 106 loss: 0.28501927852630615\n",
            "batch: 107 loss: 0.1875166893005371\n",
            "batch: 108 loss: 0.20692558586597443\n",
            "batch: 109 loss: 0.270757257938385\n",
            "batch: 110 loss: 0.3072080612182617\n",
            "batch: 111 loss: 0.24787983298301697\n",
            "batch: 112 loss: 0.27584195137023926\n",
            "batch: 113 loss: 0.2598510980606079\n",
            "batch: 114 loss: 0.19237466156482697\n",
            "batch: 115 loss: 0.340202271938324\n",
            "batch: 116 loss: 0.2945595979690552\n",
            "batch: 117 loss: 0.19637545943260193\n",
            "batch: 118 loss: 0.21825242042541504\n",
            "batch: 119 loss: 0.3918553292751312\n",
            "batch: 120 loss: 0.22781556844711304\n",
            "batch: 121 loss: 0.24840186536312103\n",
            "batch: 122 loss: 0.23382551968097687\n",
            "batch: 123 loss: 0.2633318305015564\n",
            "batch: 124 loss: 0.22709138691425323\n",
            "batch: 125 loss: 0.34376585483551025\n",
            "batch: 126 loss: 0.2040795385837555\n",
            "batch: 127 loss: 0.24251295626163483\n",
            "batch: 128 loss: 0.31367379426956177\n",
            "batch: 129 loss: 0.20575495064258575\n",
            "batch: 130 loss: 0.20827528834342957\n",
            "batch: 131 loss: 0.24161982536315918\n",
            "batch: 132 loss: 0.3787512481212616\n",
            "batch: 133 loss: 0.3567018508911133\n",
            "batch: 134 loss: 0.2906973361968994\n",
            "batch: 135 loss: 0.3211878538131714\n",
            "batch: 136 loss: 0.23051470518112183\n",
            "batch: 137 loss: 0.28554195165634155\n",
            "batch: 138 loss: 0.2673192024230957\n",
            "batch: 139 loss: 0.29620158672332764\n",
            "batch: 140 loss: 0.2505449950695038\n",
            "batch: 141 loss: 0.28773289918899536\n",
            "batch: 142 loss: 0.3041737377643585\n",
            "batch: 143 loss: 0.34113749861717224\n",
            "batch: 144 loss: 0.2689690589904785\n",
            "batch: 145 loss: 0.2342631071805954\n",
            "batch: 146 loss: 0.23083102703094482\n",
            "LOSS train 0.2793094217777252 valid 0.2862350344657898\n",
            "EPOCH 15:\n",
            "batch: 0 loss: 0.26924678683280945\n",
            "batch: 1 loss: 0.33196043968200684\n",
            "batch: 2 loss: 0.23478221893310547\n",
            "batch: 3 loss: 0.2135889083147049\n",
            "batch: 4 loss: 0.25087353587150574\n",
            "batch: 5 loss: 0.23399518430233002\n",
            "batch: 6 loss: 0.22245854139328003\n",
            "batch: 7 loss: 0.2667713165283203\n",
            "batch: 8 loss: 0.24171726405620575\n",
            "batch: 9 loss: 0.2983663082122803\n",
            "batch: 10 loss: 0.213392436504364\n",
            "batch: 11 loss: 0.23813126981258392\n",
            "batch: 12 loss: 0.3043990433216095\n",
            "batch: 13 loss: 0.2766833007335663\n",
            "batch: 14 loss: 0.2032846212387085\n",
            "batch: 15 loss: 0.23395287990570068\n",
            "batch: 16 loss: 0.24216632544994354\n",
            "batch: 17 loss: 0.24328868091106415\n",
            "batch: 18 loss: 0.39222919940948486\n",
            "batch: 19 loss: 0.22741709649562836\n",
            "batch: 20 loss: 0.2656201720237732\n",
            "batch: 21 loss: 0.2715563476085663\n",
            "batch: 22 loss: 0.16358423233032227\n",
            "batch: 23 loss: 0.23523984849452972\n",
            "batch: 24 loss: 0.2812332510948181\n",
            "batch: 25 loss: 0.2509506344795227\n",
            "batch: 26 loss: 0.22193941473960876\n",
            "batch: 27 loss: 0.2397293895483017\n",
            "batch: 28 loss: 0.2121434211730957\n",
            "batch: 29 loss: 0.3389650881290436\n",
            "batch: 30 loss: 0.24367952346801758\n",
            "batch: 31 loss: 0.20983393490314484\n",
            "batch: 32 loss: 0.29068946838378906\n",
            "batch: 33 loss: 0.26105839014053345\n",
            "batch: 34 loss: 0.25411543250083923\n",
            "batch: 35 loss: 0.2781279385089874\n",
            "batch: 36 loss: 0.20272184908390045\n",
            "batch: 37 loss: 0.2718513607978821\n",
            "batch: 38 loss: 0.3267970383167267\n",
            "batch: 39 loss: 0.25406134128570557\n",
            "batch: 40 loss: 0.18342171609401703\n",
            "batch: 41 loss: 0.3419634699821472\n",
            "batch: 42 loss: 0.24543800950050354\n",
            "batch: 43 loss: 0.24313689768314362\n",
            "batch: 44 loss: 0.24373863637447357\n",
            "batch: 45 loss: 0.27646639943122864\n",
            "batch: 46 loss: 0.22483259439468384\n",
            "batch: 47 loss: 0.2203194946050644\n",
            "batch: 48 loss: 0.26874423027038574\n",
            "batch: 49 loss: 0.15623541176319122\n",
            "batch: 50 loss: 0.23715080320835114\n",
            "batch: 51 loss: 0.29500505328178406\n",
            "batch: 52 loss: 0.2603297829627991\n",
            "batch: 53 loss: 0.22334033250808716\n",
            "batch: 54 loss: 0.2192123830318451\n",
            "batch: 55 loss: 0.26028257608413696\n",
            "batch: 56 loss: 0.21559327840805054\n",
            "batch: 57 loss: 0.20753277838230133\n",
            "batch: 58 loss: 0.2596864700317383\n",
            "batch: 59 loss: 0.2996973693370819\n",
            "batch: 60 loss: 0.2851746082305908\n",
            "batch: 61 loss: 0.19212476909160614\n",
            "batch: 62 loss: 0.25881892442703247\n",
            "batch: 63 loss: 0.22422398626804352\n",
            "batch: 64 loss: 0.20475134253501892\n",
            "batch: 65 loss: 0.23212207853794098\n",
            "batch: 66 loss: 0.29368406534194946\n",
            "batch: 67 loss: 0.2768141031265259\n",
            "batch: 68 loss: 0.27706295251846313\n",
            "batch: 69 loss: 0.22672154009342194\n",
            "batch: 70 loss: 0.2662595510482788\n",
            "batch: 71 loss: 0.24175181984901428\n",
            "batch: 72 loss: 0.26764994859695435\n",
            "batch: 73 loss: 0.3006862998008728\n",
            "batch: 74 loss: 0.25808364152908325\n",
            "batch: 75 loss: 0.2364780604839325\n",
            "batch: 76 loss: 0.2214081585407257\n",
            "batch: 77 loss: 0.33119475841522217\n",
            "batch: 78 loss: 0.30843794345855713\n",
            "batch: 79 loss: 0.27266204357147217\n",
            "batch: 80 loss: 0.17161650955677032\n",
            "batch: 81 loss: 0.2562903165817261\n",
            "batch: 82 loss: 0.23912855982780457\n",
            "batch: 83 loss: 0.271745890378952\n",
            "batch: 84 loss: 0.2876238226890564\n",
            "batch: 85 loss: 0.26773667335510254\n",
            "batch: 86 loss: 0.2209024578332901\n",
            "batch: 87 loss: 0.3167632818222046\n",
            "batch: 88 loss: 0.2467365711927414\n",
            "batch: 89 loss: 0.3227289021015167\n",
            "batch: 90 loss: 0.2542300224304199\n",
            "batch: 91 loss: 0.20518937706947327\n",
            "batch: 92 loss: 0.2641019821166992\n",
            "batch: 93 loss: 0.2898341417312622\n",
            "batch: 94 loss: 0.2480129897594452\n",
            "batch: 95 loss: 0.2796081602573395\n",
            "batch: 96 loss: 0.2771584391593933\n",
            "batch: 97 loss: 0.22902292013168335\n",
            "batch: 98 loss: 0.22598545253276825\n",
            "batch: 99 loss: 0.3355952501296997\n",
            "batch: 100 loss: 0.291412353515625\n",
            "batch: 101 loss: 0.2585299611091614\n",
            "batch: 102 loss: 0.21202750504016876\n",
            "batch: 103 loss: 0.23869873583316803\n",
            "batch: 104 loss: 0.23440173268318176\n",
            "batch: 105 loss: 0.31103914976119995\n",
            "batch: 106 loss: 0.2086063027381897\n",
            "batch: 107 loss: 0.19196604192256927\n",
            "batch: 108 loss: 0.22790846228599548\n",
            "batch: 109 loss: 0.3333248198032379\n",
            "batch: 110 loss: 0.29562586545944214\n",
            "batch: 111 loss: 0.24655182659626007\n",
            "batch: 112 loss: 0.28114068508148193\n",
            "batch: 113 loss: 0.20452725887298584\n",
            "batch: 114 loss: 0.2839839458465576\n",
            "batch: 115 loss: 0.2368098795413971\n",
            "batch: 116 loss: 0.2132364809513092\n",
            "batch: 117 loss: 0.3383171558380127\n",
            "batch: 118 loss: 0.2703169584274292\n",
            "batch: 119 loss: 0.2612141966819763\n",
            "batch: 120 loss: 0.2786475121974945\n",
            "batch: 121 loss: 0.21653059124946594\n",
            "batch: 122 loss: 0.22021009027957916\n",
            "batch: 123 loss: 0.26377779245376587\n",
            "batch: 124 loss: 0.21701680123806\n",
            "batch: 125 loss: 0.311093807220459\n",
            "batch: 126 loss: 0.25588923692703247\n",
            "batch: 127 loss: 0.24939051270484924\n",
            "batch: 128 loss: 0.22991764545440674\n",
            "batch: 129 loss: 0.21358081698417664\n",
            "batch: 130 loss: 0.16258132457733154\n",
            "batch: 131 loss: 0.2526978850364685\n",
            "batch: 132 loss: 0.24321679770946503\n",
            "batch: 133 loss: 0.2352064996957779\n",
            "batch: 134 loss: 0.2990792393684387\n",
            "batch: 135 loss: 0.2939133644104004\n",
            "batch: 136 loss: 0.3235284984111786\n",
            "batch: 137 loss: 0.24014708399772644\n",
            "batch: 138 loss: 0.33861494064331055\n",
            "batch: 139 loss: 0.22488726675510406\n",
            "batch: 140 loss: 0.2683728337287903\n",
            "batch: 141 loss: 0.307667076587677\n",
            "batch: 142 loss: 0.26744210720062256\n",
            "batch: 143 loss: 0.2665529251098633\n",
            "batch: 144 loss: 0.24216607213020325\n",
            "batch: 145 loss: 0.25797390937805176\n",
            "batch: 146 loss: 0.18697576224803925\n",
            "LOSS train 0.2664508521556854 valid 0.2801847457885742\n",
            "EPOCH 16:\n",
            "batch: 0 loss: 0.29608988761901855\n",
            "batch: 1 loss: 0.26299387216567993\n",
            "batch: 2 loss: 0.24353612959384918\n",
            "batch: 3 loss: 0.27739572525024414\n",
            "batch: 4 loss: 0.22390983998775482\n",
            "batch: 5 loss: 0.2504861354827881\n",
            "batch: 6 loss: 0.265388548374176\n",
            "batch: 7 loss: 0.24937178194522858\n",
            "batch: 8 loss: 0.2681056261062622\n",
            "batch: 9 loss: 0.21598079800605774\n",
            "batch: 10 loss: 0.25568127632141113\n",
            "batch: 11 loss: 0.27871057391166687\n",
            "batch: 12 loss: 0.2868427038192749\n",
            "batch: 13 loss: 0.20692896842956543\n",
            "batch: 14 loss: 0.2624695599079132\n",
            "batch: 15 loss: 0.24586118757724762\n",
            "batch: 16 loss: 0.21535232663154602\n",
            "batch: 17 loss: 0.29172712564468384\n",
            "batch: 18 loss: 0.22148951888084412\n",
            "batch: 19 loss: 0.26421302556991577\n",
            "batch: 20 loss: 0.2015533745288849\n",
            "batch: 21 loss: 0.22569474577903748\n",
            "batch: 22 loss: 0.29223698377609253\n",
            "batch: 23 loss: 0.20048388838768005\n",
            "batch: 24 loss: 0.212467223405838\n",
            "batch: 25 loss: 0.23070324957370758\n",
            "batch: 26 loss: 0.3354804515838623\n",
            "batch: 27 loss: 0.21763335168361664\n",
            "batch: 28 loss: 0.3217739760875702\n",
            "batch: 29 loss: 0.2798699736595154\n",
            "batch: 30 loss: 0.23327547311782837\n",
            "batch: 31 loss: 0.2629808783531189\n",
            "batch: 32 loss: 0.21889476478099823\n",
            "batch: 33 loss: 0.338111937046051\n",
            "batch: 34 loss: 0.20637096464633942\n",
            "batch: 35 loss: 0.22229434549808502\n",
            "batch: 36 loss: 0.2640400528907776\n",
            "batch: 37 loss: 0.1799127459526062\n",
            "batch: 38 loss: 0.20043036341667175\n",
            "batch: 39 loss: 0.3002779483795166\n",
            "batch: 40 loss: 0.19587677717208862\n",
            "batch: 41 loss: 0.30919331312179565\n",
            "batch: 42 loss: 0.2628898620605469\n",
            "batch: 43 loss: 0.23983807861804962\n",
            "batch: 44 loss: 0.3091471493244171\n",
            "batch: 45 loss: 0.2217722088098526\n",
            "batch: 46 loss: 0.2391481250524521\n",
            "batch: 47 loss: 0.2502744793891907\n",
            "batch: 48 loss: 0.21242494881153107\n",
            "batch: 49 loss: 0.34336310625076294\n",
            "batch: 50 loss: 0.2376628965139389\n",
            "batch: 51 loss: 0.261213093996048\n",
            "batch: 52 loss: 0.2710724472999573\n",
            "batch: 53 loss: 0.21191340684890747\n",
            "batch: 54 loss: 0.22481286525726318\n",
            "batch: 55 loss: 0.28339141607284546\n",
            "batch: 56 loss: 0.26985660195350647\n",
            "batch: 57 loss: 0.22559796273708344\n",
            "batch: 58 loss: 0.21887536346912384\n",
            "batch: 59 loss: 0.2589166462421417\n",
            "batch: 60 loss: 0.18778641521930695\n",
            "batch: 61 loss: 0.23291437327861786\n",
            "batch: 62 loss: 0.37280645966529846\n",
            "batch: 63 loss: 0.2636977732181549\n",
            "batch: 64 loss: 0.18245843052864075\n",
            "batch: 65 loss: 0.2831062078475952\n",
            "batch: 66 loss: 0.20719803869724274\n",
            "batch: 67 loss: 0.24343305826187134\n",
            "batch: 68 loss: 0.28169602155685425\n",
            "batch: 69 loss: 0.1940099596977234\n",
            "batch: 70 loss: 0.27464592456817627\n",
            "batch: 71 loss: 0.2272477000951767\n",
            "batch: 72 loss: 0.2658368945121765\n",
            "batch: 73 loss: 0.3185461163520813\n",
            "batch: 74 loss: 0.2854107618331909\n",
            "batch: 75 loss: 0.24730826914310455\n",
            "batch: 76 loss: 0.2559497058391571\n",
            "batch: 77 loss: 0.19809302687644958\n",
            "batch: 78 loss: 0.2613949775695801\n",
            "batch: 79 loss: 0.2762574553489685\n",
            "batch: 80 loss: 0.2816110849380493\n",
            "batch: 81 loss: 0.23310373723506927\n",
            "batch: 82 loss: 0.24553914368152618\n",
            "batch: 83 loss: 0.24717427790164948\n",
            "batch: 84 loss: 0.24006637930870056\n",
            "batch: 85 loss: 0.23909713327884674\n",
            "batch: 86 loss: 0.3224561810493469\n",
            "batch: 87 loss: 0.27015459537506104\n",
            "batch: 88 loss: 0.23887582123279572\n",
            "batch: 89 loss: 0.28994548320770264\n",
            "batch: 90 loss: 0.27724310755729675\n",
            "batch: 91 loss: 0.23446419835090637\n",
            "batch: 92 loss: 0.27043840289115906\n",
            "batch: 93 loss: 0.20307841897010803\n",
            "batch: 94 loss: 0.25186920166015625\n",
            "batch: 95 loss: 0.3263196051120758\n",
            "batch: 96 loss: 0.24589680135250092\n",
            "batch: 97 loss: 0.19888687133789062\n",
            "batch: 98 loss: 0.26006990671157837\n",
            "batch: 99 loss: 0.21912163496017456\n",
            "batch: 100 loss: 0.2421051412820816\n",
            "batch: 101 loss: 0.23535075783729553\n",
            "batch: 102 loss: 0.23252816498279572\n",
            "batch: 103 loss: 0.21163655817508698\n",
            "batch: 104 loss: 0.2639709711074829\n",
            "batch: 105 loss: 0.21358135342597961\n",
            "batch: 106 loss: 0.2534644603729248\n",
            "batch: 107 loss: 0.30941641330718994\n",
            "batch: 108 loss: 0.23368312418460846\n",
            "batch: 109 loss: 0.2143847495317459\n",
            "batch: 110 loss: 0.19461923837661743\n",
            "batch: 111 loss: 0.22140681743621826\n",
            "batch: 112 loss: 0.2169465571641922\n",
            "batch: 113 loss: 0.293671190738678\n",
            "batch: 114 loss: 0.3368414044380188\n",
            "batch: 115 loss: 0.2561655044555664\n",
            "batch: 116 loss: 0.24214838445186615\n",
            "batch: 117 loss: 0.25379419326782227\n",
            "batch: 118 loss: 0.2648608684539795\n",
            "batch: 119 loss: 0.2622702121734619\n",
            "batch: 120 loss: 0.19613799452781677\n",
            "batch: 121 loss: 0.19976720213890076\n",
            "batch: 122 loss: 0.28967833518981934\n",
            "batch: 123 loss: 0.2735116481781006\n",
            "batch: 124 loss: 0.29930150508880615\n",
            "batch: 125 loss: 0.23996645212173462\n",
            "batch: 126 loss: 0.23762136697769165\n",
            "batch: 127 loss: 0.24441954493522644\n",
            "batch: 128 loss: 0.23772837221622467\n",
            "batch: 129 loss: 0.31154417991638184\n",
            "batch: 130 loss: 0.2476751208305359\n",
            "batch: 131 loss: 0.2612864077091217\n",
            "batch: 132 loss: 0.23208361864089966\n",
            "batch: 133 loss: 0.28562021255493164\n",
            "batch: 134 loss: 0.1890927404165268\n",
            "batch: 135 loss: 0.19134412705898285\n",
            "batch: 136 loss: 0.267707884311676\n",
            "batch: 137 loss: 0.2628481984138489\n",
            "batch: 138 loss: 0.22775666415691376\n",
            "batch: 139 loss: 0.31250810623168945\n",
            "batch: 140 loss: 0.23994220793247223\n",
            "batch: 141 loss: 0.20692311227321625\n",
            "batch: 142 loss: 0.24603736400604248\n",
            "batch: 143 loss: 0.22840091586112976\n",
            "batch: 144 loss: 0.2446116805076599\n",
            "batch: 145 loss: 0.31140291690826416\n",
            "batch: 146 loss: 0.3508228659629822\n",
            "LOSS train 0.25690507888793945 valid 0.27029284834861755\n",
            "EPOCH 17:\n",
            "batch: 0 loss: 0.1679070144891739\n",
            "batch: 1 loss: 0.25651127099990845\n",
            "batch: 2 loss: 0.2670801877975464\n",
            "batch: 3 loss: 0.22963091731071472\n",
            "batch: 4 loss: 0.3091420531272888\n",
            "batch: 5 loss: 0.23105955123901367\n",
            "batch: 6 loss: 0.20885847508907318\n",
            "batch: 7 loss: 0.2793537974357605\n",
            "batch: 8 loss: 0.21641911566257477\n",
            "batch: 9 loss: 0.23807166516780853\n",
            "batch: 10 loss: 0.25960034132003784\n",
            "batch: 11 loss: 0.18877968192100525\n",
            "batch: 12 loss: 0.2607024312019348\n",
            "batch: 13 loss: 0.22377054393291473\n",
            "batch: 14 loss: 0.19919659197330475\n",
            "batch: 15 loss: 0.2306840568780899\n",
            "batch: 16 loss: 0.26021280884742737\n",
            "batch: 17 loss: 0.313076376914978\n",
            "batch: 18 loss: 0.23980407416820526\n",
            "batch: 19 loss: 0.23697957396507263\n",
            "batch: 20 loss: 0.19979943335056305\n",
            "batch: 21 loss: 0.23280884325504303\n",
            "batch: 22 loss: 0.20451489090919495\n",
            "batch: 23 loss: 0.24048352241516113\n",
            "batch: 24 loss: 0.22640474140644073\n",
            "batch: 25 loss: 0.22890599071979523\n",
            "batch: 26 loss: 0.17198732495307922\n",
            "batch: 27 loss: 0.21473798155784607\n",
            "batch: 28 loss: 0.24124133586883545\n",
            "batch: 29 loss: 0.2255372703075409\n",
            "batch: 30 loss: 0.2673655152320862\n",
            "batch: 31 loss: 0.2547352612018585\n",
            "batch: 32 loss: 0.2322922646999359\n",
            "batch: 33 loss: 0.23001185059547424\n",
            "batch: 34 loss: 0.22614344954490662\n",
            "batch: 35 loss: 0.25259098410606384\n",
            "batch: 36 loss: 0.2393425703048706\n",
            "batch: 37 loss: 0.24365606904029846\n",
            "batch: 38 loss: 0.24056757986545563\n",
            "batch: 39 loss: 0.18645821511745453\n",
            "batch: 40 loss: 0.2605316638946533\n",
            "batch: 41 loss: 0.2843776047229767\n",
            "batch: 42 loss: 0.19957664608955383\n",
            "batch: 43 loss: 0.2652764320373535\n",
            "batch: 44 loss: 0.24822479486465454\n",
            "batch: 45 loss: 0.1747751533985138\n",
            "batch: 46 loss: 0.2186581939458847\n",
            "batch: 47 loss: 0.2606596350669861\n",
            "batch: 48 loss: 0.27712395787239075\n",
            "batch: 49 loss: 0.2614927887916565\n",
            "batch: 50 loss: 0.2563961148262024\n",
            "batch: 51 loss: 0.28084874153137207\n",
            "batch: 52 loss: 0.2750164866447449\n",
            "batch: 53 loss: 0.24743595719337463\n",
            "batch: 54 loss: 0.22425150871276855\n",
            "batch: 55 loss: 0.3010680079460144\n",
            "batch: 56 loss: 0.20474326610565186\n",
            "batch: 57 loss: 0.23050974309444427\n",
            "batch: 58 loss: 0.2567341923713684\n",
            "batch: 59 loss: 0.23706087470054626\n",
            "batch: 60 loss: 0.16833741962909698\n",
            "batch: 61 loss: 0.2247450351715088\n",
            "batch: 62 loss: 0.25980931520462036\n",
            "batch: 63 loss: 0.20945559442043304\n",
            "batch: 64 loss: 0.2730695903301239\n",
            "batch: 65 loss: 0.24033814668655396\n",
            "batch: 66 loss: 0.21698398888111115\n",
            "batch: 67 loss: 0.2253095805644989\n",
            "batch: 68 loss: 0.25649428367614746\n",
            "batch: 69 loss: 0.27358579635620117\n",
            "batch: 70 loss: 0.2442588210105896\n",
            "batch: 71 loss: 0.20532020926475525\n",
            "batch: 72 loss: 0.2258002758026123\n",
            "batch: 73 loss: 0.2159024327993393\n",
            "batch: 74 loss: 0.3398328423500061\n",
            "batch: 75 loss: 0.19974155724048615\n",
            "batch: 76 loss: 0.23802480101585388\n",
            "batch: 77 loss: 0.2772051990032196\n",
            "batch: 78 loss: 0.25940263271331787\n",
            "batch: 79 loss: 0.19546575844287872\n",
            "batch: 80 loss: 0.20527996122837067\n",
            "batch: 81 loss: 0.17416256666183472\n",
            "batch: 82 loss: 0.2330819070339203\n",
            "batch: 83 loss: 0.28148603439331055\n",
            "batch: 84 loss: 0.30951160192489624\n",
            "batch: 85 loss: 0.2817160487174988\n",
            "batch: 86 loss: 0.22514812648296356\n",
            "batch: 87 loss: 0.1944238543510437\n",
            "batch: 88 loss: 0.24522867798805237\n",
            "batch: 89 loss: 0.24471160769462585\n",
            "batch: 90 loss: 0.22412225604057312\n",
            "batch: 91 loss: 0.2566547393798828\n",
            "batch: 92 loss: 0.1957499086856842\n",
            "batch: 93 loss: 0.2241561859846115\n",
            "batch: 94 loss: 0.2163940966129303\n",
            "batch: 95 loss: 0.24785497784614563\n",
            "batch: 96 loss: 0.20623351633548737\n",
            "batch: 97 loss: 0.2961234748363495\n",
            "batch: 98 loss: 0.33901724219322205\n",
            "batch: 99 loss: 0.21195189654827118\n",
            "batch: 100 loss: 0.2311609536409378\n",
            "batch: 101 loss: 0.23876331746578217\n",
            "batch: 102 loss: 0.2604532539844513\n",
            "batch: 103 loss: 0.2514650225639343\n",
            "batch: 104 loss: 0.23126843571662903\n",
            "batch: 105 loss: 0.24204595386981964\n",
            "batch: 106 loss: 0.22891345620155334\n",
            "batch: 107 loss: 0.2492651343345642\n",
            "batch: 108 loss: 0.24200275540351868\n",
            "batch: 109 loss: 0.28575676679611206\n",
            "batch: 110 loss: 0.2738555073738098\n",
            "batch: 111 loss: 0.2528350353240967\n",
            "batch: 112 loss: 0.2919085621833801\n",
            "batch: 113 loss: 0.2728767395019531\n",
            "batch: 114 loss: 0.26492494344711304\n",
            "batch: 115 loss: 0.2518191337585449\n",
            "batch: 116 loss: 0.24252374470233917\n",
            "batch: 117 loss: 0.23900656402111053\n",
            "batch: 118 loss: 0.20298880338668823\n",
            "batch: 119 loss: 0.2257622629404068\n",
            "batch: 120 loss: 0.23323436081409454\n",
            "batch: 121 loss: 0.18517319858074188\n",
            "batch: 122 loss: 0.22850054502487183\n",
            "batch: 123 loss: 0.3083600401878357\n",
            "batch: 124 loss: 0.3221716284751892\n",
            "batch: 125 loss: 0.2517099976539612\n",
            "batch: 126 loss: 0.20840582251548767\n",
            "batch: 127 loss: 0.2577667832374573\n",
            "batch: 128 loss: 0.22524307668209076\n",
            "batch: 129 loss: 0.22994978725910187\n",
            "batch: 130 loss: 0.2511255145072937\n",
            "batch: 131 loss: 0.32879918813705444\n",
            "batch: 132 loss: 0.2763095200061798\n",
            "batch: 133 loss: 0.23522590100765228\n",
            "batch: 134 loss: 0.2742048501968384\n",
            "batch: 135 loss: 0.3072371482849121\n",
            "batch: 136 loss: 0.36675596237182617\n",
            "batch: 137 loss: 0.1854267120361328\n",
            "batch: 138 loss: 0.24570997059345245\n",
            "batch: 139 loss: 0.25565582513809204\n",
            "batch: 140 loss: 0.19567003846168518\n",
            "batch: 141 loss: 0.21267595887184143\n",
            "batch: 142 loss: 0.22703014314174652\n",
            "batch: 143 loss: 0.29623085260391235\n",
            "batch: 144 loss: 0.3979443907737732\n",
            "batch: 145 loss: 0.20650307834148407\n",
            "batch: 146 loss: 0.34382328391075134\n",
            "LOSS train 0.2971749007701874 valid 0.3126298487186432\n",
            "EPOCH 18:\n",
            "batch: 0 loss: 0.228030726313591\n",
            "batch: 1 loss: 0.22672931849956512\n",
            "batch: 2 loss: 0.25670069456100464\n",
            "batch: 3 loss: 0.25733625888824463\n",
            "batch: 4 loss: 0.24447932839393616\n",
            "batch: 5 loss: 0.25628751516342163\n",
            "batch: 6 loss: 0.2644379734992981\n",
            "batch: 7 loss: 0.23784931004047394\n",
            "batch: 8 loss: 0.2247837781906128\n",
            "batch: 9 loss: 0.24212466180324554\n",
            "batch: 10 loss: 0.21067576110363007\n",
            "batch: 11 loss: 0.3002588152885437\n",
            "batch: 12 loss: 0.27401840686798096\n",
            "batch: 13 loss: 0.19461318850517273\n",
            "batch: 14 loss: 0.21150101721286774\n",
            "batch: 15 loss: 0.3444640636444092\n",
            "batch: 16 loss: 0.20532533526420593\n",
            "batch: 17 loss: 0.29187077283859253\n",
            "batch: 18 loss: 0.2770583927631378\n",
            "batch: 19 loss: 0.2710570693016052\n",
            "batch: 20 loss: 0.29124686121940613\n",
            "batch: 21 loss: 0.21104228496551514\n",
            "batch: 22 loss: 0.22066861391067505\n",
            "batch: 23 loss: 0.263278603553772\n",
            "batch: 24 loss: 0.30990809202194214\n",
            "batch: 25 loss: 0.22509720921516418\n",
            "batch: 26 loss: 0.24265651404857635\n",
            "batch: 27 loss: 0.2640104293823242\n",
            "batch: 28 loss: 0.26989030838012695\n",
            "batch: 29 loss: 0.20065782964229584\n",
            "batch: 30 loss: 0.25479060411453247\n",
            "batch: 31 loss: 0.2593231499195099\n",
            "batch: 32 loss: 0.19762395322322845\n",
            "batch: 33 loss: 0.2205861210823059\n",
            "batch: 34 loss: 0.2929394245147705\n",
            "batch: 35 loss: 0.28457093238830566\n",
            "batch: 36 loss: 0.21425405144691467\n",
            "batch: 37 loss: 0.25430595874786377\n",
            "batch: 38 loss: 0.24848736822605133\n",
            "batch: 39 loss: 0.3159196376800537\n",
            "batch: 40 loss: 0.30550694465637207\n",
            "batch: 41 loss: 0.29901278018951416\n",
            "batch: 42 loss: 0.22322307527065277\n",
            "batch: 43 loss: 0.20087061822414398\n",
            "batch: 44 loss: 0.26295679807662964\n",
            "batch: 45 loss: 0.3312535285949707\n",
            "batch: 46 loss: 0.2540816068649292\n",
            "batch: 47 loss: 0.28484874963760376\n",
            "batch: 48 loss: 0.2774946689605713\n",
            "batch: 49 loss: 0.1685413122177124\n",
            "batch: 50 loss: 0.20217974483966827\n",
            "batch: 51 loss: 0.2362462282180786\n",
            "batch: 52 loss: 0.23560462892055511\n",
            "batch: 53 loss: 0.20330673456192017\n",
            "batch: 54 loss: 0.2403634935617447\n",
            "batch: 55 loss: 0.2861882448196411\n",
            "batch: 56 loss: 0.31496870517730713\n",
            "batch: 57 loss: 0.29083654284477234\n",
            "batch: 58 loss: 0.22888338565826416\n",
            "batch: 59 loss: 0.25347650051116943\n",
            "batch: 60 loss: 0.20746850967407227\n",
            "batch: 61 loss: 0.31013160943984985\n",
            "batch: 62 loss: 0.20334291458129883\n",
            "batch: 63 loss: 0.2566577196121216\n",
            "batch: 64 loss: 0.2179393470287323\n",
            "batch: 65 loss: 0.21313545107841492\n",
            "batch: 66 loss: 0.22511573135852814\n",
            "batch: 67 loss: 0.2646903395652771\n",
            "batch: 68 loss: 0.2601426839828491\n",
            "batch: 69 loss: 0.34613093733787537\n",
            "batch: 70 loss: 0.2117365598678589\n",
            "batch: 71 loss: 0.2086828052997589\n",
            "batch: 72 loss: 0.2931098937988281\n",
            "batch: 73 loss: 0.2810137867927551\n",
            "batch: 74 loss: 0.19022098183631897\n",
            "batch: 75 loss: 0.28741616010665894\n",
            "batch: 76 loss: 0.24602575600147247\n",
            "batch: 77 loss: 0.17657749354839325\n",
            "batch: 78 loss: 0.20289137959480286\n",
            "batch: 79 loss: 0.2969985008239746\n",
            "batch: 80 loss: 0.2575933337211609\n",
            "batch: 81 loss: 0.16883981227874756\n",
            "batch: 82 loss: 0.23315198719501495\n",
            "batch: 83 loss: 0.23130840063095093\n",
            "batch: 84 loss: 0.2868086099624634\n",
            "batch: 85 loss: 0.274135947227478\n",
            "batch: 86 loss: 0.24055877327919006\n",
            "batch: 87 loss: 0.19825516641139984\n",
            "batch: 88 loss: 0.2762490212917328\n",
            "batch: 89 loss: 0.22752094268798828\n",
            "batch: 90 loss: 0.1724095344543457\n",
            "batch: 91 loss: 0.22278210520744324\n",
            "batch: 92 loss: 0.26554882526397705\n",
            "batch: 93 loss: 0.23313917219638824\n",
            "batch: 94 loss: 0.2646713852882385\n",
            "batch: 95 loss: 0.26833295822143555\n",
            "batch: 96 loss: 0.178840771317482\n",
            "batch: 97 loss: 0.24653787910938263\n",
            "batch: 98 loss: 0.46405845880508423\n",
            "batch: 99 loss: 0.22470754384994507\n",
            "batch: 100 loss: 0.3472106158733368\n",
            "batch: 101 loss: 0.2492433786392212\n",
            "batch: 102 loss: 0.2535300552845001\n",
            "batch: 103 loss: 0.2358044981956482\n",
            "batch: 104 loss: 0.2832164764404297\n",
            "batch: 105 loss: 0.2966194152832031\n",
            "batch: 106 loss: 0.23598933219909668\n",
            "batch: 107 loss: 0.24087150394916534\n",
            "batch: 108 loss: 0.22368597984313965\n",
            "batch: 109 loss: 0.21801218390464783\n",
            "batch: 110 loss: 0.23803459107875824\n",
            "batch: 111 loss: 0.2395448237657547\n",
            "batch: 112 loss: 0.21672223508358002\n",
            "batch: 113 loss: 0.3003726601600647\n",
            "batch: 114 loss: 0.27400487661361694\n",
            "batch: 115 loss: 0.2396485060453415\n",
            "batch: 116 loss: 0.20044173300266266\n",
            "batch: 117 loss: 0.22715000808238983\n",
            "batch: 118 loss: 0.20275117456912994\n",
            "batch: 119 loss: 0.23348888754844666\n",
            "batch: 120 loss: 0.23691421747207642\n",
            "batch: 121 loss: 0.3581700026988983\n",
            "batch: 122 loss: 0.3149127960205078\n",
            "batch: 123 loss: 0.20793290436267853\n",
            "batch: 124 loss: 0.363221675157547\n",
            "batch: 125 loss: 0.23232246935367584\n",
            "batch: 126 loss: 0.24062839150428772\n",
            "batch: 127 loss: 0.2311679720878601\n",
            "batch: 128 loss: 0.26829519867897034\n",
            "batch: 129 loss: 0.22203510999679565\n",
            "batch: 130 loss: 0.20738375186920166\n",
            "batch: 131 loss: 0.28902971744537354\n",
            "batch: 132 loss: 0.2427528351545334\n",
            "batch: 133 loss: 0.2760498821735382\n",
            "batch: 134 loss: 0.25521889328956604\n",
            "batch: 135 loss: 0.23029731214046478\n",
            "batch: 136 loss: 0.23513010144233704\n",
            "batch: 137 loss: 0.23222149908542633\n",
            "batch: 138 loss: 0.21533511579036713\n",
            "batch: 139 loss: 0.2083165943622589\n",
            "batch: 140 loss: 0.2770490050315857\n",
            "batch: 141 loss: 0.2717092037200928\n",
            "batch: 142 loss: 0.24004220962524414\n",
            "batch: 143 loss: 0.2726934552192688\n",
            "batch: 144 loss: 0.27681171894073486\n",
            "batch: 145 loss: 0.19318868219852448\n",
            "batch: 146 loss: 0.19244568049907684\n",
            "LOSS train 0.324650377035141 valid 0.3419353663921356\n",
            "EPOCH 19:\n",
            "batch: 0 loss: 0.22619128227233887\n",
            "batch: 1 loss: 0.24703241884708405\n",
            "batch: 2 loss: 0.19677482545375824\n",
            "batch: 3 loss: 0.2440676987171173\n",
            "batch: 4 loss: 0.2910967171192169\n",
            "batch: 5 loss: 0.2581824064254761\n",
            "batch: 6 loss: 0.19817155599594116\n",
            "batch: 7 loss: 0.19116562604904175\n",
            "batch: 8 loss: 0.22869165241718292\n",
            "batch: 9 loss: 0.2121824324131012\n",
            "batch: 10 loss: 0.26703375577926636\n",
            "batch: 11 loss: 0.2028765231370926\n",
            "batch: 12 loss: 0.22803838551044464\n",
            "batch: 13 loss: 0.2802189886569977\n",
            "batch: 14 loss: 0.33238500356674194\n",
            "batch: 15 loss: 0.19451873004436493\n",
            "batch: 16 loss: 0.15720587968826294\n",
            "batch: 17 loss: 0.23759673535823822\n",
            "batch: 18 loss: 0.2591104507446289\n",
            "batch: 19 loss: 0.24706019461154938\n",
            "batch: 20 loss: 0.20644709467887878\n",
            "batch: 21 loss: 0.23427924513816833\n",
            "batch: 22 loss: 0.239712193608284\n",
            "batch: 23 loss: 0.2942047119140625\n",
            "batch: 24 loss: 0.22563034296035767\n",
            "batch: 25 loss: 0.24206717312335968\n",
            "batch: 26 loss: 0.28015005588531494\n",
            "batch: 27 loss: 0.29393869638442993\n",
            "batch: 28 loss: 0.26797470450401306\n",
            "batch: 29 loss: 0.2162468433380127\n",
            "batch: 30 loss: 0.3659557104110718\n",
            "batch: 31 loss: 0.2755523920059204\n",
            "batch: 32 loss: 0.24896562099456787\n",
            "batch: 33 loss: 0.2443794310092926\n",
            "batch: 34 loss: 0.22244173288345337\n",
            "batch: 35 loss: 0.21578830480575562\n",
            "batch: 36 loss: 0.23200929164886475\n",
            "batch: 37 loss: 0.18818464875221252\n",
            "batch: 38 loss: 0.30626362562179565\n",
            "batch: 39 loss: 0.19678175449371338\n",
            "batch: 40 loss: 0.19572772085666656\n",
            "batch: 41 loss: 0.23823896050453186\n",
            "batch: 42 loss: 0.1711626499891281\n",
            "batch: 43 loss: 0.23761239647865295\n",
            "batch: 44 loss: 0.2528921961784363\n",
            "batch: 45 loss: 0.27745145559310913\n",
            "batch: 46 loss: 0.21262423694133759\n",
            "batch: 47 loss: 0.1811002790927887\n",
            "batch: 48 loss: 0.26929113268852234\n",
            "batch: 49 loss: 0.21867482364177704\n",
            "batch: 50 loss: 0.2518872618675232\n",
            "batch: 51 loss: 0.22262144088745117\n",
            "batch: 52 loss: 0.2175491750240326\n",
            "batch: 53 loss: 0.23088613152503967\n",
            "batch: 54 loss: 0.2131010740995407\n",
            "batch: 55 loss: 0.24492192268371582\n",
            "batch: 56 loss: 0.22524721920490265\n",
            "batch: 57 loss: 0.25923392176628113\n",
            "batch: 58 loss: 0.2569958567619324\n",
            "batch: 59 loss: 0.2450728416442871\n",
            "batch: 60 loss: 0.20835725963115692\n",
            "batch: 61 loss: 0.2824521064758301\n",
            "batch: 62 loss: 0.21965651214122772\n",
            "batch: 63 loss: 0.23911543190479279\n",
            "batch: 64 loss: 0.27806729078292847\n",
            "batch: 65 loss: 0.1806454360485077\n",
            "batch: 66 loss: 0.23412545025348663\n",
            "batch: 67 loss: 0.2153898924589157\n",
            "batch: 68 loss: 0.24297310411930084\n",
            "batch: 69 loss: 0.1917397379875183\n",
            "batch: 70 loss: 0.23627839982509613\n",
            "batch: 71 loss: 0.17267557978630066\n",
            "batch: 72 loss: 0.22755791246891022\n",
            "batch: 73 loss: 0.24170608818531036\n",
            "batch: 74 loss: 0.2986495792865753\n",
            "batch: 75 loss: 0.18535861372947693\n",
            "batch: 76 loss: 0.2625660300254822\n",
            "batch: 77 loss: 0.2687990665435791\n",
            "batch: 78 loss: 0.23600660264492035\n",
            "batch: 79 loss: 0.26172250509262085\n",
            "batch: 80 loss: 0.2053937017917633\n",
            "batch: 81 loss: 0.2442845106124878\n",
            "batch: 82 loss: 0.30861344933509827\n",
            "batch: 83 loss: 0.1956821233034134\n",
            "batch: 84 loss: 0.23992125689983368\n",
            "batch: 85 loss: 0.21080701053142548\n",
            "batch: 86 loss: 0.26152074337005615\n",
            "batch: 87 loss: 0.2558016777038574\n",
            "batch: 88 loss: 0.23626337945461273\n",
            "batch: 89 loss: 0.21159599721431732\n",
            "batch: 90 loss: 0.24116674065589905\n",
            "batch: 91 loss: 0.21861031651496887\n",
            "batch: 92 loss: 0.24778494238853455\n",
            "batch: 93 loss: 0.23664148151874542\n",
            "batch: 94 loss: 0.20245712995529175\n",
            "batch: 95 loss: 0.24588541686534882\n",
            "batch: 96 loss: 0.294040709733963\n",
            "batch: 97 loss: 0.15254566073417664\n",
            "batch: 98 loss: 0.2963860332965851\n",
            "batch: 99 loss: 0.31651198863983154\n",
            "batch: 100 loss: 0.2001131772994995\n",
            "batch: 101 loss: 0.19328759610652924\n",
            "batch: 102 loss: 0.28259778022766113\n",
            "batch: 103 loss: 0.24505382776260376\n",
            "batch: 104 loss: 0.2444652020931244\n",
            "batch: 105 loss: 0.2108953893184662\n",
            "batch: 106 loss: 0.20067760348320007\n",
            "batch: 107 loss: 0.17438942193984985\n",
            "batch: 108 loss: 0.24212276935577393\n",
            "batch: 109 loss: 0.18263757228851318\n",
            "batch: 110 loss: 0.2326032668352127\n",
            "batch: 111 loss: 0.1544228047132492\n",
            "batch: 112 loss: 0.2305145412683487\n",
            "batch: 113 loss: 0.24188165366649628\n",
            "batch: 114 loss: 0.25143927335739136\n",
            "batch: 115 loss: 0.24289336800575256\n",
            "batch: 116 loss: 0.23424328863620758\n",
            "batch: 117 loss: 0.22701393067836761\n",
            "batch: 118 loss: 0.21464505791664124\n",
            "batch: 119 loss: 0.29728788137435913\n",
            "batch: 120 loss: 0.1663036048412323\n",
            "batch: 121 loss: 0.2039397805929184\n",
            "batch: 122 loss: 0.2393050491809845\n",
            "batch: 123 loss: 0.35059165954589844\n",
            "batch: 124 loss: 0.27854520082473755\n",
            "batch: 125 loss: 0.26612192392349243\n",
            "batch: 126 loss: 0.23260606825351715\n",
            "batch: 127 loss: 0.24317124485969543\n",
            "batch: 128 loss: 0.24279555678367615\n",
            "batch: 129 loss: 0.20981702208518982\n",
            "batch: 130 loss: 0.23290619254112244\n",
            "batch: 131 loss: 0.23781804740428925\n",
            "batch: 132 loss: 0.2142820656299591\n",
            "batch: 133 loss: 0.22116371989250183\n",
            "batch: 134 loss: 0.1863408386707306\n",
            "batch: 135 loss: 0.2216380089521408\n",
            "batch: 136 loss: 0.2391483187675476\n",
            "batch: 137 loss: 0.20384928584098816\n",
            "batch: 138 loss: 0.31894156336784363\n",
            "batch: 139 loss: 0.247214213013649\n",
            "batch: 140 loss: 0.17761506140232086\n",
            "batch: 141 loss: 0.22739247977733612\n",
            "batch: 142 loss: 0.16524861752986908\n",
            "batch: 143 loss: 0.1946757286787033\n",
            "batch: 144 loss: 0.3606642186641693\n",
            "batch: 145 loss: 0.20939338207244873\n",
            "batch: 146 loss: 0.24559026956558228\n",
            "LOSS train 0.23645345866680145 valid 0.25678327679634094\n",
            "EPOCH 20:\n",
            "batch: 0 loss: 0.2660652995109558\n",
            "batch: 1 loss: 0.168317049741745\n",
            "batch: 2 loss: 0.2444455623626709\n",
            "batch: 3 loss: 0.1980689913034439\n",
            "batch: 4 loss: 0.18883122503757477\n",
            "batch: 5 loss: 0.24768880009651184\n",
            "batch: 6 loss: 0.2681782841682434\n",
            "batch: 7 loss: 0.18345892429351807\n",
            "batch: 8 loss: 0.19997729361057281\n",
            "batch: 9 loss: 0.19562187790870667\n",
            "batch: 10 loss: 0.2398107796907425\n",
            "batch: 11 loss: 0.2183958739042282\n",
            "batch: 12 loss: 0.22321444749832153\n",
            "batch: 13 loss: 0.24273104965686798\n",
            "batch: 14 loss: 0.2945581078529358\n",
            "batch: 15 loss: 0.2261255979537964\n",
            "batch: 16 loss: 0.23921315371990204\n",
            "batch: 17 loss: 0.22665327787399292\n",
            "batch: 18 loss: 0.21097567677497864\n",
            "batch: 19 loss: 0.2417723536491394\n",
            "batch: 20 loss: 0.21881169080734253\n",
            "batch: 21 loss: 0.18309319019317627\n",
            "batch: 22 loss: 0.3287915587425232\n",
            "batch: 23 loss: 0.2711062431335449\n",
            "batch: 24 loss: 0.1740514636039734\n",
            "batch: 25 loss: 0.2685989737510681\n",
            "batch: 26 loss: 0.1772344410419464\n",
            "batch: 27 loss: 0.2668761610984802\n",
            "batch: 28 loss: 0.1789664924144745\n",
            "batch: 29 loss: 0.25534260272979736\n",
            "batch: 30 loss: 0.20249362289905548\n",
            "batch: 31 loss: 0.19305387139320374\n",
            "batch: 32 loss: 0.2034185826778412\n",
            "batch: 33 loss: 0.25749778747558594\n",
            "batch: 34 loss: 0.21679581701755524\n",
            "batch: 35 loss: 0.21901382505893707\n",
            "batch: 36 loss: 0.2386033833026886\n",
            "batch: 37 loss: 0.22443662583827972\n",
            "batch: 38 loss: 0.2509765625\n",
            "batch: 39 loss: 0.24711009860038757\n",
            "batch: 40 loss: 0.2169969081878662\n",
            "batch: 41 loss: 0.2555805444717407\n",
            "batch: 42 loss: 0.24352215230464935\n",
            "batch: 43 loss: 0.2234163135290146\n",
            "batch: 44 loss: 0.21453696489334106\n",
            "batch: 45 loss: 0.2212192714214325\n",
            "batch: 46 loss: 0.1836632639169693\n",
            "batch: 47 loss: 0.1973562091588974\n",
            "batch: 48 loss: 0.24523073434829712\n",
            "batch: 49 loss: 0.24573194980621338\n",
            "batch: 50 loss: 0.19977018237113953\n",
            "batch: 51 loss: 0.22466324269771576\n",
            "batch: 52 loss: 0.26706817746162415\n",
            "batch: 53 loss: 0.315320760011673\n",
            "batch: 54 loss: 0.24132895469665527\n",
            "batch: 55 loss: 0.27108263969421387\n",
            "batch: 56 loss: 0.22576546669006348\n",
            "batch: 57 loss: 0.2881086766719818\n",
            "batch: 58 loss: 0.23077809810638428\n",
            "batch: 59 loss: 0.21813136339187622\n",
            "batch: 60 loss: 0.24209405481815338\n",
            "batch: 61 loss: 0.22883190214633942\n",
            "batch: 62 loss: 0.2800638675689697\n",
            "batch: 63 loss: 0.1819382756948471\n",
            "batch: 64 loss: 0.24243363738059998\n",
            "batch: 65 loss: 0.27406683564186096\n",
            "batch: 66 loss: 0.23410747945308685\n",
            "batch: 67 loss: 0.19416143000125885\n",
            "batch: 68 loss: 0.24190999567508698\n",
            "batch: 69 loss: 0.181559756398201\n",
            "batch: 70 loss: 0.20933520793914795\n",
            "batch: 71 loss: 0.2533915638923645\n",
            "batch: 72 loss: 0.2156948298215866\n",
            "batch: 73 loss: 0.18975947797298431\n",
            "batch: 74 loss: 0.32226938009262085\n",
            "batch: 75 loss: 0.2214781641960144\n",
            "batch: 76 loss: 0.23667921125888824\n",
            "batch: 77 loss: 0.2364882528781891\n",
            "batch: 78 loss: 0.2017347663640976\n",
            "batch: 79 loss: 0.18660840392112732\n",
            "batch: 80 loss: 0.2405099719762802\n",
            "batch: 81 loss: 0.20565927028656006\n",
            "batch: 82 loss: 0.17764683067798615\n",
            "batch: 83 loss: 0.2657170295715332\n",
            "batch: 84 loss: 0.2574642300605774\n",
            "batch: 85 loss: 0.2991456091403961\n",
            "batch: 86 loss: 0.15969803929328918\n",
            "batch: 87 loss: 0.19150978326797485\n",
            "batch: 88 loss: 0.31972917914390564\n",
            "batch: 89 loss: 0.29672878980636597\n",
            "batch: 90 loss: 0.28243422508239746\n",
            "batch: 91 loss: 0.2823655605316162\n",
            "batch: 92 loss: 0.16645130515098572\n",
            "batch: 93 loss: 0.2047591358423233\n",
            "batch: 94 loss: 0.22555264830589294\n",
            "batch: 95 loss: 0.20363210141658783\n",
            "batch: 96 loss: 0.20825985074043274\n",
            "batch: 97 loss: 0.21407291293144226\n",
            "batch: 98 loss: 0.22499290108680725\n",
            "batch: 99 loss: 0.21775607764720917\n",
            "batch: 100 loss: 0.26677149534225464\n",
            "batch: 101 loss: 0.21371598541736603\n",
            "batch: 102 loss: 0.2596951127052307\n",
            "batch: 103 loss: 0.22990328073501587\n",
            "batch: 104 loss: 0.2716362774372101\n",
            "batch: 105 loss: 0.2418917417526245\n",
            "batch: 106 loss: 0.23774473369121552\n",
            "batch: 107 loss: 0.1887131929397583\n",
            "batch: 108 loss: 0.1955397129058838\n",
            "batch: 109 loss: 0.1939370334148407\n",
            "batch: 110 loss: 0.29726433753967285\n",
            "batch: 111 loss: 0.22921131551265717\n",
            "batch: 112 loss: 0.2416783720254898\n",
            "batch: 113 loss: 0.1831396520137787\n",
            "batch: 114 loss: 0.2957150340080261\n",
            "batch: 115 loss: 0.22614899277687073\n",
            "batch: 116 loss: 0.21253469586372375\n",
            "batch: 117 loss: 0.31186172366142273\n",
            "batch: 118 loss: 0.215335875749588\n",
            "batch: 119 loss: 0.19833385944366455\n",
            "batch: 120 loss: 0.19569049775600433\n",
            "batch: 121 loss: 0.23023997247219086\n",
            "batch: 122 loss: 0.2155468612909317\n",
            "batch: 123 loss: 0.20959040522575378\n",
            "batch: 124 loss: 0.21245436370372772\n",
            "batch: 125 loss: 0.17575520277023315\n",
            "batch: 126 loss: 0.2592647075653076\n",
            "batch: 127 loss: 0.21491359174251556\n",
            "batch: 128 loss: 0.20027272403240204\n",
            "batch: 129 loss: 0.28983741998672485\n",
            "batch: 130 loss: 0.2192370444536209\n",
            "batch: 131 loss: 0.22409319877624512\n",
            "batch: 132 loss: 0.16792410612106323\n",
            "batch: 133 loss: 0.1774093210697174\n",
            "batch: 134 loss: 0.23405976593494415\n",
            "batch: 135 loss: 0.17914190888404846\n",
            "batch: 136 loss: 0.24030610918998718\n",
            "batch: 137 loss: 0.2034541517496109\n",
            "batch: 138 loss: 0.236875519156456\n",
            "batch: 139 loss: 0.22357770800590515\n",
            "batch: 140 loss: 0.267660528421402\n",
            "batch: 141 loss: 0.23096396028995514\n",
            "batch: 142 loss: 0.27871936559677124\n",
            "batch: 143 loss: 0.2448328733444214\n",
            "batch: 144 loss: 0.21375302970409393\n",
            "batch: 145 loss: 0.17343787848949432\n",
            "batch: 146 loss: 0.2390446960926056\n",
            "LOSS train 0.26183125376701355 valid 0.28416603803634644\n",
            "EPOCH 21:\n",
            "batch: 0 loss: 0.21733108162879944\n",
            "batch: 1 loss: 0.24074293673038483\n",
            "batch: 2 loss: 0.2142966389656067\n",
            "batch: 3 loss: 0.2257542610168457\n",
            "batch: 4 loss: 0.18737366795539856\n",
            "batch: 5 loss: 0.23037537932395935\n",
            "batch: 6 loss: 0.19318923354148865\n",
            "batch: 7 loss: 0.2087181657552719\n",
            "batch: 8 loss: 0.16929307579994202\n",
            "batch: 9 loss: 0.18700751662254333\n",
            "batch: 10 loss: 0.19311460852622986\n",
            "batch: 11 loss: 0.1740124523639679\n",
            "batch: 12 loss: 0.18355225026607513\n",
            "batch: 13 loss: 0.14854350686073303\n",
            "batch: 14 loss: 0.23124364018440247\n",
            "batch: 15 loss: 0.1952638477087021\n",
            "batch: 16 loss: 0.22188615798950195\n",
            "batch: 17 loss: 0.25369977951049805\n",
            "batch: 18 loss: 0.20212194323539734\n",
            "batch: 19 loss: 0.20933254063129425\n",
            "batch: 20 loss: 0.21708200871944427\n",
            "batch: 21 loss: 0.18738675117492676\n",
            "batch: 22 loss: 0.17979221045970917\n",
            "batch: 23 loss: 0.21687418222427368\n",
            "batch: 24 loss: 0.2347819209098816\n",
            "batch: 25 loss: 0.2106592208147049\n",
            "batch: 26 loss: 0.1767931878566742\n",
            "batch: 27 loss: 0.1559406816959381\n",
            "batch: 28 loss: 0.15763702988624573\n",
            "batch: 29 loss: 0.1885755956172943\n",
            "batch: 30 loss: 0.2834829092025757\n",
            "batch: 31 loss: 0.2889838218688965\n",
            "batch: 32 loss: 0.28158122301101685\n",
            "batch: 33 loss: 0.23746532201766968\n",
            "batch: 34 loss: 0.20260089635849\n",
            "batch: 35 loss: 0.22992877662181854\n",
            "batch: 36 loss: 0.24561063945293427\n",
            "batch: 37 loss: 0.20096075534820557\n",
            "batch: 38 loss: 0.2532733380794525\n",
            "batch: 39 loss: 0.2320367842912674\n",
            "batch: 40 loss: 0.16608917713165283\n",
            "batch: 41 loss: 0.24440957605838776\n",
            "batch: 42 loss: 0.23102198541164398\n",
            "batch: 43 loss: 0.2572833299636841\n",
            "batch: 44 loss: 0.2953920364379883\n",
            "batch: 45 loss: 0.2312374860048294\n",
            "batch: 46 loss: 0.30220866203308105\n",
            "batch: 47 loss: 0.21823950111865997\n",
            "batch: 48 loss: 0.201932892203331\n",
            "batch: 49 loss: 0.22352956235408783\n",
            "batch: 50 loss: 0.22566702961921692\n",
            "batch: 51 loss: 0.26931464672088623\n",
            "batch: 52 loss: 0.22660419344902039\n",
            "batch: 53 loss: 0.21657541394233704\n",
            "batch: 54 loss: 0.1984814703464508\n",
            "batch: 55 loss: 0.23663859069347382\n",
            "batch: 56 loss: 0.2155340015888214\n",
            "batch: 57 loss: 0.18565624952316284\n",
            "batch: 58 loss: 0.2668455243110657\n",
            "batch: 59 loss: 0.21593765914440155\n",
            "batch: 60 loss: 0.1716398298740387\n",
            "batch: 61 loss: 0.2316642850637436\n",
            "batch: 62 loss: 0.23885191977024078\n",
            "batch: 63 loss: 0.2019607126712799\n",
            "batch: 64 loss: 0.13781368732452393\n",
            "batch: 65 loss: 0.19616246223449707\n",
            "batch: 66 loss: 0.26025718450546265\n",
            "batch: 67 loss: 0.18114367127418518\n",
            "batch: 68 loss: 0.1692740023136139\n",
            "batch: 69 loss: 0.1794382631778717\n",
            "batch: 70 loss: 0.21250256896018982\n",
            "batch: 71 loss: 0.27559366822242737\n",
            "batch: 72 loss: 0.23596897721290588\n",
            "batch: 73 loss: 0.22670935094356537\n",
            "batch: 74 loss: 0.2616603374481201\n",
            "batch: 75 loss: 0.22265128791332245\n",
            "batch: 76 loss: 0.22982190549373627\n",
            "batch: 77 loss: 0.1808987855911255\n",
            "batch: 78 loss: 0.25167396664619446\n",
            "batch: 79 loss: 0.23823106288909912\n",
            "batch: 80 loss: 0.1828497350215912\n",
            "batch: 81 loss: 0.2169964462518692\n",
            "batch: 82 loss: 0.18931922316551208\n",
            "batch: 83 loss: 0.22248397767543793\n",
            "batch: 84 loss: 0.23728078603744507\n",
            "batch: 85 loss: 0.22294212877750397\n",
            "batch: 86 loss: 0.19758176803588867\n",
            "batch: 87 loss: 0.18248975276947021\n",
            "batch: 88 loss: 0.31280040740966797\n",
            "batch: 89 loss: 0.22695641219615936\n",
            "batch: 90 loss: 0.22114363312721252\n",
            "batch: 91 loss: 0.20113177597522736\n",
            "batch: 92 loss: 0.2107478827238083\n",
            "batch: 93 loss: 0.23502345383167267\n",
            "batch: 94 loss: 0.26630058884620667\n",
            "batch: 95 loss: 0.25500988960266113\n",
            "batch: 96 loss: 0.21038585901260376\n",
            "batch: 97 loss: 0.252414345741272\n",
            "batch: 98 loss: 0.20666834712028503\n",
            "batch: 99 loss: 0.20526434481143951\n",
            "batch: 100 loss: 0.1974431872367859\n",
            "batch: 101 loss: 0.2058316171169281\n",
            "batch: 102 loss: 0.32230043411254883\n",
            "batch: 103 loss: 0.2358507215976715\n",
            "batch: 104 loss: 0.17272049188613892\n",
            "batch: 105 loss: 0.23920051753520966\n",
            "batch: 106 loss: 0.18248996138572693\n",
            "batch: 107 loss: 0.25302913784980774\n",
            "batch: 108 loss: 0.2022126168012619\n",
            "batch: 109 loss: 0.21681387722492218\n",
            "batch: 110 loss: 0.2602207362651825\n",
            "batch: 111 loss: 0.21543815732002258\n",
            "batch: 112 loss: 0.24163837730884552\n",
            "batch: 113 loss: 0.2405727356672287\n",
            "batch: 114 loss: 0.1550547182559967\n",
            "batch: 115 loss: 0.1947733461856842\n",
            "batch: 116 loss: 0.24601607024669647\n",
            "batch: 117 loss: 0.19486434757709503\n",
            "batch: 118 loss: 0.2286228984594345\n",
            "batch: 119 loss: 0.20746085047721863\n",
            "batch: 120 loss: 0.19561134278774261\n",
            "batch: 121 loss: 0.23274485766887665\n",
            "batch: 122 loss: 0.26524457335472107\n",
            "batch: 123 loss: 0.22342036664485931\n",
            "batch: 124 loss: 0.1985073834657669\n",
            "batch: 125 loss: 0.18987582623958588\n",
            "batch: 126 loss: 0.21453498303890228\n",
            "batch: 127 loss: 0.22347305715084076\n",
            "batch: 128 loss: 0.3195604085922241\n",
            "batch: 129 loss: 0.24425099790096283\n",
            "batch: 130 loss: 0.18859907984733582\n",
            "batch: 131 loss: 0.26201412081718445\n",
            "batch: 132 loss: 0.2945410907268524\n",
            "batch: 133 loss: 0.22949720919132233\n",
            "batch: 134 loss: 0.22155076265335083\n",
            "batch: 135 loss: 0.2698655426502228\n",
            "batch: 136 loss: 0.17088305950164795\n",
            "batch: 137 loss: 0.19496798515319824\n",
            "batch: 138 loss: 0.2816065549850464\n",
            "batch: 139 loss: 0.24781566858291626\n",
            "batch: 140 loss: 0.24134066700935364\n",
            "batch: 141 loss: 0.29994308948516846\n",
            "batch: 142 loss: 0.22098857164382935\n",
            "batch: 143 loss: 0.22324280440807343\n",
            "batch: 144 loss: 0.20684698224067688\n",
            "batch: 145 loss: 0.2557278871536255\n",
            "batch: 146 loss: 0.24084778130054474\n",
            "LOSS train 0.22362399101257324 valid 0.24570722877979279\n",
            "EPOCH 22:\n",
            "batch: 0 loss: 0.19713832437992096\n",
            "batch: 1 loss: 0.19027626514434814\n",
            "batch: 2 loss: 0.24220022559165955\n",
            "batch: 3 loss: 0.15141025185585022\n",
            "batch: 4 loss: 0.2128608524799347\n",
            "batch: 5 loss: 0.20482000708580017\n",
            "batch: 6 loss: 0.20811325311660767\n",
            "batch: 7 loss: 0.2293992042541504\n",
            "batch: 8 loss: 0.2263544350862503\n",
            "batch: 9 loss: 0.16814130544662476\n",
            "batch: 10 loss: 0.25171950459480286\n",
            "batch: 11 loss: 0.25772276520729065\n",
            "batch: 12 loss: 0.2970684766769409\n",
            "batch: 13 loss: 0.25531455874443054\n",
            "batch: 14 loss: 0.25100576877593994\n",
            "batch: 15 loss: 0.26969027519226074\n",
            "batch: 16 loss: 0.22732295095920563\n",
            "batch: 17 loss: 0.14356490969657898\n",
            "batch: 18 loss: 0.19544239342212677\n",
            "batch: 19 loss: 0.19421601295471191\n",
            "batch: 20 loss: 0.28186988830566406\n",
            "batch: 21 loss: 0.16677159070968628\n",
            "batch: 22 loss: 0.25459128618240356\n",
            "batch: 23 loss: 0.1863740086555481\n",
            "batch: 24 loss: 0.2567700147628784\n",
            "batch: 25 loss: 0.18312492966651917\n",
            "batch: 26 loss: 0.18781711161136627\n",
            "batch: 27 loss: 0.1858321875333786\n",
            "batch: 28 loss: 0.18918155133724213\n",
            "batch: 29 loss: 0.20190507173538208\n",
            "batch: 30 loss: 0.21741974353790283\n",
            "batch: 31 loss: 0.23808404803276062\n",
            "batch: 32 loss: 0.26021862030029297\n",
            "batch: 33 loss: 0.20410990715026855\n",
            "batch: 34 loss: 0.2473549246788025\n",
            "batch: 35 loss: 0.2645930349826813\n",
            "batch: 36 loss: 0.18216654658317566\n",
            "batch: 37 loss: 0.29765385389328003\n",
            "batch: 38 loss: 0.1869816780090332\n",
            "batch: 39 loss: 0.25390198826789856\n",
            "batch: 40 loss: 0.24218149483203888\n",
            "batch: 41 loss: 0.2270534485578537\n",
            "batch: 42 loss: 0.25875234603881836\n",
            "batch: 43 loss: 0.23800888657569885\n",
            "batch: 44 loss: 0.17465755343437195\n",
            "batch: 45 loss: 0.18930839002132416\n",
            "batch: 46 loss: 0.22102664411067963\n",
            "batch: 47 loss: 0.24855373799800873\n",
            "batch: 48 loss: 0.23859237134456635\n",
            "batch: 49 loss: 0.24942873418331146\n",
            "batch: 50 loss: 0.21199272572994232\n",
            "batch: 51 loss: 0.20954002439975739\n",
            "batch: 52 loss: 0.21311907470226288\n",
            "batch: 53 loss: 0.22255264222621918\n",
            "batch: 54 loss: 0.26840677857398987\n",
            "batch: 55 loss: 0.281453013420105\n",
            "batch: 56 loss: 0.19652754068374634\n",
            "batch: 57 loss: 0.2324710637331009\n",
            "batch: 58 loss: 0.2048896998167038\n",
            "batch: 59 loss: 0.2386130690574646\n",
            "batch: 60 loss: 0.21820029616355896\n",
            "batch: 61 loss: 0.2276061475276947\n",
            "batch: 62 loss: 0.221685990691185\n",
            "batch: 63 loss: 0.25789159536361694\n",
            "batch: 64 loss: 0.15749551355838776\n",
            "batch: 65 loss: 0.22056491672992706\n",
            "batch: 66 loss: 0.17571401596069336\n",
            "batch: 67 loss: 0.22371883690357208\n",
            "batch: 68 loss: 0.21999654173851013\n",
            "batch: 69 loss: 0.22114138305187225\n",
            "batch: 70 loss: 0.18983128666877747\n",
            "batch: 71 loss: 0.22567830979824066\n",
            "batch: 72 loss: 0.2464354932308197\n",
            "batch: 73 loss: 0.17612090706825256\n",
            "batch: 74 loss: 0.24175526201725006\n",
            "batch: 75 loss: 0.1827314794063568\n",
            "batch: 76 loss: 0.2482428252696991\n",
            "batch: 77 loss: 0.22691184282302856\n",
            "batch: 78 loss: 0.22688639163970947\n",
            "batch: 79 loss: 0.17844602465629578\n",
            "batch: 80 loss: 0.37259310483932495\n",
            "batch: 81 loss: 0.20810973644256592\n",
            "batch: 82 loss: 0.205807626247406\n",
            "batch: 83 loss: 0.1914512813091278\n",
            "batch: 84 loss: 0.3403106927871704\n",
            "batch: 85 loss: 0.24825377762317657\n",
            "batch: 86 loss: 0.24536575376987457\n",
            "batch: 87 loss: 0.18733863532543182\n",
            "batch: 88 loss: 0.24410638213157654\n",
            "batch: 89 loss: 0.17743158340454102\n",
            "batch: 90 loss: 0.239539235830307\n",
            "batch: 91 loss: 0.2125348299741745\n",
            "batch: 92 loss: 0.2760341167449951\n",
            "batch: 93 loss: 0.19564996659755707\n",
            "batch: 94 loss: 0.24032790958881378\n",
            "batch: 95 loss: 0.22127506136894226\n",
            "batch: 96 loss: 0.19305969774723053\n",
            "batch: 97 loss: 0.18904033303260803\n",
            "batch: 98 loss: 0.21414950489997864\n",
            "batch: 99 loss: 0.18942885100841522\n",
            "batch: 100 loss: 0.19743779301643372\n",
            "batch: 101 loss: 0.2228655368089676\n",
            "batch: 102 loss: 0.17276069521903992\n",
            "batch: 103 loss: 0.17216676473617554\n",
            "batch: 104 loss: 0.1992526352405548\n",
            "batch: 105 loss: 0.20740008354187012\n",
            "batch: 106 loss: 0.1921824812889099\n",
            "batch: 107 loss: 0.18261247873306274\n",
            "batch: 108 loss: 0.23686310648918152\n",
            "batch: 109 loss: 0.20622551441192627\n",
            "batch: 110 loss: 0.27922385931015015\n",
            "batch: 111 loss: 0.2264329493045807\n",
            "batch: 112 loss: 0.21390879154205322\n",
            "batch: 113 loss: 0.2312963902950287\n",
            "batch: 114 loss: 0.21725183725357056\n",
            "batch: 115 loss: 0.22372683882713318\n",
            "batch: 116 loss: 0.22738157212734222\n",
            "batch: 117 loss: 0.2283143699169159\n",
            "batch: 118 loss: 0.2505056858062744\n",
            "batch: 119 loss: 0.16228698194026947\n",
            "batch: 120 loss: 0.1841062605381012\n",
            "batch: 121 loss: 0.22252696752548218\n",
            "batch: 122 loss: 0.19288194179534912\n",
            "batch: 123 loss: 0.20823435485363007\n",
            "batch: 124 loss: 0.23619478940963745\n",
            "batch: 125 loss: 0.2139056921005249\n",
            "batch: 126 loss: 0.2003328651189804\n",
            "batch: 127 loss: 0.2547997832298279\n",
            "batch: 128 loss: 0.21241867542266846\n",
            "batch: 129 loss: 0.1689392328262329\n",
            "batch: 130 loss: 0.25380173325538635\n",
            "batch: 131 loss: 0.19387534260749817\n",
            "batch: 132 loss: 0.1976223737001419\n",
            "batch: 133 loss: 0.23353634774684906\n",
            "batch: 134 loss: 0.244017094373703\n",
            "batch: 135 loss: 0.2169080674648285\n",
            "batch: 136 loss: 0.1728341281414032\n",
            "batch: 137 loss: 0.21627192199230194\n",
            "batch: 138 loss: 0.14255841076374054\n",
            "batch: 139 loss: 0.236842080950737\n",
            "batch: 140 loss: 0.30745261907577515\n",
            "batch: 141 loss: 0.2091859132051468\n",
            "batch: 142 loss: 0.22213146090507507\n",
            "batch: 143 loss: 0.18904715776443481\n",
            "batch: 144 loss: 0.1716795116662979\n",
            "batch: 145 loss: 0.16840429604053497\n",
            "batch: 146 loss: 0.274482786655426\n",
            "LOSS train 0.22478404641151428 valid 0.25890669226646423\n",
            "EPOCH 23:\n",
            "batch: 0 loss: 0.19996753334999084\n",
            "batch: 1 loss: 0.16837099194526672\n",
            "batch: 2 loss: 0.21592947840690613\n",
            "batch: 3 loss: 0.25781744718551636\n",
            "batch: 4 loss: 0.29224687814712524\n",
            "batch: 5 loss: 0.2875382900238037\n",
            "batch: 6 loss: 0.19489505887031555\n",
            "batch: 7 loss: 0.20052649080753326\n",
            "batch: 8 loss: 0.19767135381698608\n",
            "batch: 9 loss: 0.17076848447322845\n",
            "batch: 10 loss: 0.2817397117614746\n",
            "batch: 11 loss: 0.19226230680942535\n",
            "batch: 12 loss: 0.16663023829460144\n",
            "batch: 13 loss: 0.2384091466665268\n",
            "batch: 14 loss: 0.20365074276924133\n",
            "batch: 15 loss: 0.20530341565608978\n",
            "batch: 16 loss: 0.29892539978027344\n",
            "batch: 17 loss: 0.18849357962608337\n",
            "batch: 18 loss: 0.2087368369102478\n",
            "batch: 19 loss: 0.21461918950080872\n",
            "batch: 20 loss: 0.21246270835399628\n",
            "batch: 21 loss: 0.22097307443618774\n",
            "batch: 22 loss: 0.19588236510753632\n",
            "batch: 23 loss: 0.23988528549671173\n",
            "batch: 24 loss: 0.3052993416786194\n",
            "batch: 25 loss: 0.20249512791633606\n",
            "batch: 26 loss: 0.2276250422000885\n",
            "batch: 27 loss: 0.16917914152145386\n",
            "batch: 28 loss: 0.28355544805526733\n",
            "batch: 29 loss: 0.2621185779571533\n",
            "batch: 30 loss: 0.2490496188402176\n",
            "batch: 31 loss: 0.20995086431503296\n",
            "batch: 32 loss: 0.22057272493839264\n",
            "batch: 33 loss: 0.2290882021188736\n",
            "batch: 34 loss: 0.26703745126724243\n",
            "batch: 35 loss: 0.2011057734489441\n",
            "batch: 36 loss: 0.14472216367721558\n",
            "batch: 37 loss: 0.21707040071487427\n",
            "batch: 38 loss: 0.3145332932472229\n",
            "batch: 39 loss: 0.17685610055923462\n",
            "batch: 40 loss: 0.15706974267959595\n",
            "batch: 41 loss: 0.20682838559150696\n",
            "batch: 42 loss: 0.22495605051517487\n",
            "batch: 43 loss: 0.24189481139183044\n",
            "batch: 44 loss: 0.22018352150917053\n",
            "batch: 45 loss: 0.1959584802389145\n",
            "batch: 46 loss: 0.22165115177631378\n",
            "batch: 47 loss: 0.20631392300128937\n",
            "batch: 48 loss: 0.20244541764259338\n",
            "batch: 49 loss: 0.19881276786327362\n",
            "batch: 50 loss: 0.18346832692623138\n",
            "batch: 51 loss: 0.2609536647796631\n",
            "batch: 52 loss: 0.24185560643672943\n",
            "batch: 53 loss: 0.17940819263458252\n",
            "batch: 54 loss: 0.1972043663263321\n",
            "batch: 55 loss: 0.25736427307128906\n",
            "batch: 56 loss: 0.1603938639163971\n",
            "batch: 57 loss: 0.20738592743873596\n",
            "batch: 58 loss: 0.19581317901611328\n",
            "batch: 59 loss: 0.23399928212165833\n",
            "batch: 60 loss: 0.25856590270996094\n",
            "batch: 61 loss: 0.17602291703224182\n",
            "batch: 62 loss: 0.20149774849414825\n",
            "batch: 63 loss: 0.18846511840820312\n",
            "batch: 64 loss: 0.16760340332984924\n",
            "batch: 65 loss: 0.2133519947528839\n",
            "batch: 66 loss: 0.1808500587940216\n",
            "batch: 67 loss: 0.2029954046010971\n",
            "batch: 68 loss: 0.19091324508190155\n",
            "batch: 69 loss: 0.21389834582805634\n",
            "batch: 70 loss: 0.22715114057064056\n",
            "batch: 71 loss: 0.20864011347293854\n",
            "batch: 72 loss: 0.16105183959007263\n",
            "batch: 73 loss: 0.17355352640151978\n",
            "batch: 74 loss: 0.17826101183891296\n",
            "batch: 75 loss: 0.17610478401184082\n",
            "batch: 76 loss: 0.21931450068950653\n",
            "batch: 77 loss: 0.18476025760173798\n",
            "batch: 78 loss: 0.17432305216789246\n",
            "batch: 79 loss: 0.23588255047798157\n",
            "batch: 80 loss: 0.3722319006919861\n",
            "batch: 81 loss: 0.1931753158569336\n",
            "batch: 82 loss: 0.28592121601104736\n",
            "batch: 83 loss: 0.2609965205192566\n",
            "batch: 84 loss: 0.22740262746810913\n",
            "batch: 85 loss: 0.23594363033771515\n",
            "batch: 86 loss: 0.2465956211090088\n",
            "batch: 87 loss: 0.2374194860458374\n",
            "batch: 88 loss: 0.16435205936431885\n",
            "batch: 89 loss: 0.19540709257125854\n",
            "batch: 90 loss: 0.23048149049282074\n",
            "batch: 91 loss: 0.1847028285264969\n",
            "batch: 92 loss: 0.23074139654636383\n",
            "batch: 93 loss: 0.30550992488861084\n",
            "batch: 94 loss: 0.20255839824676514\n",
            "batch: 95 loss: 0.23945803940296173\n",
            "batch: 96 loss: 0.2351190149784088\n",
            "batch: 97 loss: 0.2282167375087738\n",
            "batch: 98 loss: 0.19459079205989838\n",
            "batch: 99 loss: 0.20739831030368805\n",
            "batch: 100 loss: 0.2508953809738159\n",
            "batch: 101 loss: 0.27745845913887024\n",
            "batch: 102 loss: 0.22782421112060547\n",
            "batch: 103 loss: 0.2508799433708191\n",
            "batch: 104 loss: 0.24510635435581207\n",
            "batch: 105 loss: 0.21753491461277008\n",
            "batch: 106 loss: 0.24392448365688324\n",
            "batch: 107 loss: 0.20218707621097565\n",
            "batch: 108 loss: 0.20859479904174805\n",
            "batch: 109 loss: 0.22486072778701782\n",
            "batch: 110 loss: 0.26382777094841003\n",
            "batch: 111 loss: 0.21221671998500824\n",
            "batch: 112 loss: 0.2742913067340851\n",
            "batch: 113 loss: 0.22840823233127594\n",
            "batch: 114 loss: 0.21855954825878143\n",
            "batch: 115 loss: 0.1978476345539093\n",
            "batch: 116 loss: 0.18677416443824768\n",
            "batch: 117 loss: 0.2027924805879593\n",
            "batch: 118 loss: 0.16847887635231018\n",
            "batch: 119 loss: 0.1729508638381958\n",
            "batch: 120 loss: 0.18597181141376495\n",
            "batch: 121 loss: 0.1721811592578888\n",
            "batch: 122 loss: 0.2444351315498352\n",
            "batch: 123 loss: 0.2602415680885315\n",
            "batch: 124 loss: 0.16412204504013062\n",
            "batch: 125 loss: 0.17580200731754303\n",
            "batch: 126 loss: 0.2572638988494873\n",
            "batch: 127 loss: 0.19647274911403656\n",
            "batch: 128 loss: 0.19921068847179413\n",
            "batch: 129 loss: 0.2086048424243927\n",
            "batch: 130 loss: 0.19221514463424683\n",
            "batch: 131 loss: 0.20250877737998962\n",
            "batch: 132 loss: 0.1592629998922348\n",
            "batch: 133 loss: 0.19644208252429962\n",
            "batch: 134 loss: 0.20527713000774384\n",
            "batch: 135 loss: 0.2209920734167099\n",
            "batch: 136 loss: 0.15229719877243042\n",
            "batch: 137 loss: 0.31697583198547363\n",
            "batch: 138 loss: 0.2286786288022995\n",
            "batch: 139 loss: 0.19947698712348938\n",
            "batch: 140 loss: 0.2388390749692917\n",
            "batch: 141 loss: 0.18734154105186462\n",
            "batch: 142 loss: 0.2890540361404419\n",
            "batch: 143 loss: 0.22214698791503906\n",
            "batch: 144 loss: 0.1523853838443756\n",
            "batch: 145 loss: 0.19181939959526062\n",
            "batch: 146 loss: 0.1839350163936615\n",
            "LOSS train 0.22410979866981506 valid 0.2496037632226944\n",
            "EPOCH 24:\n",
            "batch: 0 loss: 0.2084360420703888\n",
            "batch: 1 loss: 0.19779925048351288\n",
            "batch: 2 loss: 0.18481716513633728\n",
            "batch: 3 loss: 0.1864989995956421\n",
            "batch: 4 loss: 0.1763932704925537\n",
            "batch: 5 loss: 0.1786838173866272\n",
            "batch: 6 loss: 0.16321946680545807\n",
            "batch: 7 loss: 0.18285267055034637\n",
            "batch: 8 loss: 0.25063180923461914\n",
            "batch: 9 loss: 0.27034497261047363\n",
            "batch: 10 loss: 0.2467070370912552\n",
            "batch: 11 loss: 0.18389065563678741\n",
            "batch: 12 loss: 0.2234250009059906\n",
            "batch: 13 loss: 0.24886919558048248\n",
            "batch: 14 loss: 0.2572883665561676\n",
            "batch: 15 loss: 0.19111263751983643\n",
            "batch: 16 loss: 0.22030341625213623\n",
            "batch: 17 loss: 0.19719655811786652\n",
            "batch: 18 loss: 0.19914652407169342\n",
            "batch: 19 loss: 0.20543251931667328\n",
            "batch: 20 loss: 0.30236536264419556\n",
            "batch: 21 loss: 0.2109094262123108\n",
            "batch: 22 loss: 0.1965317577123642\n",
            "batch: 23 loss: 0.181499183177948\n",
            "batch: 24 loss: 0.18983694911003113\n",
            "batch: 25 loss: 0.23852604627609253\n",
            "batch: 26 loss: 0.1934804618358612\n",
            "batch: 27 loss: 0.21632765233516693\n",
            "batch: 28 loss: 0.16910749673843384\n",
            "batch: 29 loss: 0.22064974904060364\n",
            "batch: 30 loss: 0.2170041799545288\n",
            "batch: 31 loss: 0.21215835213661194\n",
            "batch: 32 loss: 0.17536786198616028\n",
            "batch: 33 loss: 0.15966618061065674\n",
            "batch: 34 loss: 0.2247350960969925\n",
            "batch: 35 loss: 0.24403789639472961\n",
            "batch: 36 loss: 0.2078799307346344\n",
            "batch: 37 loss: 0.17930734157562256\n",
            "batch: 38 loss: 0.2510080635547638\n",
            "batch: 39 loss: 0.1861875057220459\n",
            "batch: 40 loss: 0.2512196898460388\n",
            "batch: 41 loss: 0.2230960726737976\n",
            "batch: 42 loss: 0.18594348430633545\n",
            "batch: 43 loss: 0.30384212732315063\n",
            "batch: 44 loss: 0.17893622815608978\n",
            "batch: 45 loss: 0.26852911710739136\n",
            "batch: 46 loss: 0.2729162275791168\n",
            "batch: 47 loss: 0.21767070889472961\n",
            "batch: 48 loss: 0.20891839265823364\n",
            "batch: 49 loss: 0.21773239970207214\n",
            "batch: 50 loss: 0.26980918645858765\n",
            "batch: 51 loss: 0.20454555749893188\n",
            "batch: 52 loss: 0.2289799302816391\n",
            "batch: 53 loss: 0.23183505237102509\n",
            "batch: 54 loss: 0.2141345739364624\n",
            "batch: 55 loss: 0.2272554486989975\n",
            "batch: 56 loss: 0.20344962179660797\n",
            "batch: 57 loss: 0.2360287606716156\n",
            "batch: 58 loss: 0.2020239531993866\n",
            "batch: 59 loss: 0.22640275955200195\n",
            "batch: 60 loss: 0.1957763135433197\n",
            "batch: 61 loss: 0.20215992629528046\n",
            "batch: 62 loss: 0.23805232346057892\n",
            "batch: 63 loss: 0.19516870379447937\n",
            "batch: 64 loss: 0.1427677869796753\n",
            "batch: 65 loss: 0.16404195129871368\n",
            "batch: 66 loss: 0.16276079416275024\n",
            "batch: 67 loss: 0.21982738375663757\n",
            "batch: 68 loss: 0.20385895669460297\n",
            "batch: 69 loss: 0.23820771276950836\n",
            "batch: 70 loss: 0.1996927708387375\n",
            "batch: 71 loss: 0.26371195912361145\n",
            "batch: 72 loss: 0.2454642504453659\n",
            "batch: 73 loss: 0.23753225803375244\n",
            "batch: 74 loss: 0.20986922085285187\n",
            "batch: 75 loss: 0.2278144508600235\n",
            "batch: 76 loss: 0.18203842639923096\n",
            "batch: 77 loss: 0.1958840787410736\n",
            "batch: 78 loss: 0.1988735944032669\n",
            "batch: 79 loss: 0.20656578242778778\n",
            "batch: 80 loss: 0.20310670137405396\n",
            "batch: 81 loss: 0.25850456953048706\n",
            "batch: 82 loss: 0.15246474742889404\n",
            "batch: 83 loss: 0.14583125710487366\n",
            "batch: 84 loss: 0.21475695073604584\n",
            "batch: 85 loss: 0.1654127538204193\n",
            "batch: 86 loss: 0.21540959179401398\n",
            "batch: 87 loss: 0.2163746953010559\n",
            "batch: 88 loss: 0.18630073964595795\n",
            "batch: 89 loss: 0.1739746630191803\n",
            "batch: 90 loss: 0.24704928696155548\n",
            "batch: 91 loss: 0.17627108097076416\n",
            "batch: 92 loss: 0.2377343624830246\n",
            "batch: 93 loss: 0.22244970500469208\n",
            "batch: 94 loss: 0.21169330179691315\n",
            "batch: 95 loss: 0.2524465322494507\n",
            "batch: 96 loss: 0.21336965262889862\n",
            "batch: 97 loss: 0.1836356222629547\n",
            "batch: 98 loss: 0.16196250915527344\n",
            "batch: 99 loss: 0.15964195132255554\n",
            "batch: 100 loss: 0.22685876488685608\n",
            "batch: 101 loss: 0.19192145764827728\n",
            "batch: 102 loss: 0.26291972398757935\n",
            "batch: 103 loss: 0.19427573680877686\n",
            "batch: 104 loss: 0.18159344792366028\n",
            "batch: 105 loss: 0.30668580532073975\n",
            "batch: 106 loss: 0.20095372200012207\n",
            "batch: 107 loss: 0.216870978474617\n",
            "batch: 108 loss: 0.18737438321113586\n",
            "batch: 109 loss: 0.23335210978984833\n",
            "batch: 110 loss: 0.2032938152551651\n",
            "batch: 111 loss: 0.23402731120586395\n",
            "batch: 112 loss: 0.19704167544841766\n",
            "batch: 113 loss: 0.15753677487373352\n",
            "batch: 114 loss: 0.2245836853981018\n",
            "batch: 115 loss: 0.2041986733675003\n",
            "batch: 116 loss: 0.2170068472623825\n",
            "batch: 117 loss: 0.1770532727241516\n",
            "batch: 118 loss: 0.24384362995624542\n",
            "batch: 119 loss: 0.22041499614715576\n",
            "batch: 120 loss: 0.22415371239185333\n",
            "batch: 121 loss: 0.20224909484386444\n",
            "batch: 122 loss: 0.18826600909233093\n",
            "batch: 123 loss: 0.2557520270347595\n",
            "batch: 124 loss: 0.2045121192932129\n",
            "batch: 125 loss: 0.2112104892730713\n",
            "batch: 126 loss: 0.18077006936073303\n",
            "batch: 127 loss: 0.21564142405986786\n",
            "batch: 128 loss: 0.14972388744354248\n",
            "batch: 129 loss: 0.22549210488796234\n",
            "batch: 130 loss: 0.2052796632051468\n",
            "batch: 131 loss: 0.23074223101139069\n",
            "batch: 132 loss: 0.18517062067985535\n",
            "batch: 133 loss: 0.2114303708076477\n",
            "batch: 134 loss: 0.24944013357162476\n",
            "batch: 135 loss: 0.2521042227745056\n",
            "batch: 136 loss: 0.19385313987731934\n",
            "batch: 137 loss: 0.22862078249454498\n",
            "batch: 138 loss: 0.22707757353782654\n",
            "batch: 139 loss: 0.24381621181964874\n",
            "batch: 140 loss: 0.1790427565574646\n",
            "batch: 141 loss: 0.28151726722717285\n",
            "batch: 142 loss: 0.21204648911952972\n",
            "batch: 143 loss: 0.1898203194141388\n",
            "batch: 144 loss: 0.20619219541549683\n",
            "batch: 145 loss: 0.17963463068008423\n",
            "batch: 146 loss: 0.21238717436790466\n",
            "LOSS train 0.20492327213287354 valid 0.23776502907276154\n",
            "EPOCH 25:\n",
            "batch: 0 loss: 0.24803447723388672\n",
            "batch: 1 loss: 0.17529484629631042\n",
            "batch: 2 loss: 0.18911325931549072\n",
            "batch: 3 loss: 0.2021297961473465\n",
            "batch: 4 loss: 0.21602776646614075\n",
            "batch: 5 loss: 0.19187448918819427\n",
            "batch: 6 loss: 0.21878065168857574\n",
            "batch: 7 loss: 0.2709735631942749\n",
            "batch: 8 loss: 0.23904958367347717\n",
            "batch: 9 loss: 0.18270036578178406\n",
            "batch: 10 loss: 0.2642946243286133\n",
            "batch: 11 loss: 0.19168944656848907\n",
            "batch: 12 loss: 0.15693707764148712\n",
            "batch: 13 loss: 0.19870580732822418\n",
            "batch: 14 loss: 0.1646231710910797\n",
            "batch: 15 loss: 0.21958498656749725\n",
            "batch: 16 loss: 0.15763527154922485\n",
            "batch: 17 loss: 0.21559718251228333\n",
            "batch: 18 loss: 0.20893295109272003\n",
            "batch: 19 loss: 0.18760544061660767\n",
            "batch: 20 loss: 0.22165977954864502\n",
            "batch: 21 loss: 0.22198201715946198\n",
            "batch: 22 loss: 0.21228142082691193\n",
            "batch: 23 loss: 0.20283657312393188\n",
            "batch: 24 loss: 0.16653425991535187\n",
            "batch: 25 loss: 0.2550934851169586\n",
            "batch: 26 loss: 0.152872234582901\n",
            "batch: 27 loss: 0.17556172609329224\n",
            "batch: 28 loss: 0.1844887137413025\n",
            "batch: 29 loss: 0.2396673709154129\n",
            "batch: 30 loss: 0.17211875319480896\n",
            "batch: 31 loss: 0.2233434021472931\n",
            "batch: 32 loss: 0.19058921933174133\n",
            "batch: 33 loss: 0.24171064794063568\n",
            "batch: 34 loss: 0.25774306058883667\n",
            "batch: 35 loss: 0.1722049117088318\n",
            "batch: 36 loss: 0.17193573713302612\n",
            "batch: 37 loss: 0.1924949586391449\n",
            "batch: 38 loss: 0.15721046924591064\n",
            "batch: 39 loss: 0.24272257089614868\n",
            "batch: 40 loss: 0.2130058854818344\n",
            "batch: 41 loss: 0.17407000064849854\n",
            "batch: 42 loss: 0.16088488698005676\n",
            "batch: 43 loss: 0.2023705542087555\n",
            "batch: 44 loss: 0.15030579268932343\n",
            "batch: 45 loss: 0.2231368124485016\n",
            "batch: 46 loss: 0.19888916611671448\n",
            "batch: 47 loss: 0.19887666404247284\n",
            "batch: 48 loss: 0.2093072235584259\n",
            "batch: 49 loss: 0.23851656913757324\n",
            "batch: 50 loss: 0.1697501540184021\n",
            "batch: 51 loss: 0.1514635980129242\n",
            "batch: 52 loss: 0.23351944983005524\n",
            "batch: 53 loss: 0.19494807720184326\n",
            "batch: 54 loss: 0.22646352648735046\n",
            "batch: 55 loss: 0.25238993763923645\n",
            "batch: 56 loss: 0.17118582129478455\n",
            "batch: 57 loss: 0.1689477562904358\n",
            "batch: 58 loss: 0.17423246800899506\n",
            "batch: 59 loss: 0.18997953832149506\n",
            "batch: 60 loss: 0.18973205983638763\n",
            "batch: 61 loss: 0.2111905813217163\n",
            "batch: 62 loss: 0.2429516613483429\n",
            "batch: 63 loss: 0.18481671810150146\n",
            "batch: 64 loss: 0.20431028306484222\n",
            "batch: 65 loss: 0.2244197577238083\n",
            "batch: 66 loss: 0.28085702657699585\n",
            "batch: 67 loss: 0.13520002365112305\n",
            "batch: 68 loss: 0.1708773672580719\n",
            "batch: 69 loss: 0.2038877010345459\n",
            "batch: 70 loss: 0.1937026083469391\n",
            "batch: 71 loss: 0.16607755422592163\n",
            "batch: 72 loss: 0.23411448299884796\n",
            "batch: 73 loss: 0.25071024894714355\n",
            "batch: 74 loss: 0.16280129551887512\n",
            "batch: 75 loss: 0.27825379371643066\n",
            "batch: 76 loss: 0.1962643712759018\n",
            "batch: 77 loss: 0.18890011310577393\n",
            "batch: 78 loss: 0.17007452249526978\n",
            "batch: 79 loss: 0.1828058958053589\n",
            "batch: 80 loss: 0.19543853402137756\n",
            "batch: 81 loss: 0.15868499875068665\n",
            "batch: 82 loss: 0.18077848851680756\n",
            "batch: 83 loss: 0.14637377858161926\n",
            "batch: 84 loss: 0.23479771614074707\n",
            "batch: 85 loss: 0.2612207233905792\n",
            "batch: 86 loss: 0.15451925992965698\n",
            "batch: 87 loss: 0.16482451558113098\n",
            "batch: 88 loss: 0.2001722902059555\n",
            "batch: 89 loss: 0.24435105919837952\n",
            "batch: 90 loss: 0.19859099388122559\n",
            "batch: 91 loss: 0.1941010057926178\n",
            "batch: 92 loss: 0.21978998184204102\n",
            "batch: 93 loss: 0.27262642979621887\n",
            "batch: 94 loss: 0.18024200201034546\n",
            "batch: 95 loss: 0.19803102314472198\n",
            "batch: 96 loss: 0.21073627471923828\n",
            "batch: 97 loss: 0.23124246299266815\n",
            "batch: 98 loss: 0.1905854493379593\n",
            "batch: 99 loss: 0.20765577256679535\n",
            "batch: 100 loss: 0.15597239136695862\n",
            "batch: 101 loss: 0.16220054030418396\n",
            "batch: 102 loss: 0.20430320501327515\n",
            "batch: 103 loss: 0.23078103363513947\n",
            "batch: 104 loss: 0.1753077656030655\n",
            "batch: 105 loss: 0.1933273822069168\n",
            "batch: 106 loss: 0.23011994361877441\n",
            "batch: 107 loss: 0.1793237328529358\n",
            "batch: 108 loss: 0.2600196301937103\n",
            "batch: 109 loss: 0.16357287764549255\n",
            "batch: 110 loss: 0.21310850977897644\n",
            "batch: 111 loss: 0.3504011034965515\n",
            "batch: 112 loss: 0.16028553247451782\n",
            "batch: 113 loss: 0.20918747782707214\n",
            "batch: 114 loss: 0.19174130260944366\n",
            "batch: 115 loss: 0.2177845984697342\n",
            "batch: 116 loss: 0.17658458650112152\n",
            "batch: 117 loss: 0.20986372232437134\n",
            "batch: 118 loss: 0.24666717648506165\n",
            "batch: 119 loss: 0.21432891488075256\n",
            "batch: 120 loss: 0.17179137468338013\n",
            "batch: 121 loss: 0.2049565613269806\n",
            "batch: 122 loss: 0.17351604998111725\n",
            "batch: 123 loss: 0.21147184073925018\n",
            "batch: 124 loss: 0.2201230674982071\n",
            "batch: 125 loss: 0.20893768966197968\n",
            "batch: 126 loss: 0.27216941118240356\n",
            "batch: 127 loss: 0.24531114101409912\n",
            "batch: 128 loss: 0.23366381227970123\n",
            "batch: 129 loss: 0.2027038037776947\n",
            "batch: 130 loss: 0.2446921169757843\n",
            "batch: 131 loss: 0.17524997889995575\n",
            "batch: 132 loss: 0.180694580078125\n",
            "batch: 133 loss: 0.223377525806427\n",
            "batch: 134 loss: 0.2231360524892807\n",
            "batch: 135 loss: 0.207292377948761\n",
            "batch: 136 loss: 0.17138123512268066\n",
            "batch: 137 loss: 0.1784648448228836\n",
            "batch: 138 loss: 0.15998758375644684\n",
            "batch: 139 loss: 0.19531984627246857\n",
            "batch: 140 loss: 0.2378367930650711\n",
            "batch: 141 loss: 0.22291144728660583\n",
            "batch: 142 loss: 0.2526417374610901\n",
            "batch: 143 loss: 0.18919941782951355\n",
            "batch: 144 loss: 0.15835057199001312\n",
            "batch: 145 loss: 0.2154245674610138\n",
            "batch: 146 loss: 0.1574212908744812\n",
            "LOSS train 0.194416344165802 valid 0.2282276749610901\n",
            "EPOCH 26:\n",
            "batch: 0 loss: 0.22099721431732178\n",
            "batch: 1 loss: 0.1377466320991516\n",
            "batch: 2 loss: 0.2291346937417984\n",
            "batch: 3 loss: 0.1821756511926651\n",
            "batch: 4 loss: 0.19240078330039978\n",
            "batch: 5 loss: 0.1421240270137787\n",
            "batch: 6 loss: 0.17846642434597015\n",
            "batch: 7 loss: 0.18628397583961487\n",
            "batch: 8 loss: 0.1574658751487732\n",
            "batch: 9 loss: 0.19754700362682343\n",
            "batch: 10 loss: 0.16301661729812622\n",
            "batch: 11 loss: 0.2112656682729721\n",
            "batch: 12 loss: 0.191430926322937\n",
            "batch: 13 loss: 0.180644229054451\n",
            "batch: 14 loss: 0.18364249169826508\n",
            "batch: 15 loss: 0.24425075948238373\n",
            "batch: 16 loss: 0.18365167081356049\n",
            "batch: 17 loss: 0.20469224452972412\n",
            "batch: 18 loss: 0.1900767832994461\n",
            "batch: 19 loss: 0.17057961225509644\n",
            "batch: 20 loss: 0.2719643712043762\n",
            "batch: 21 loss: 0.1818595826625824\n",
            "batch: 22 loss: 0.1716463267803192\n",
            "batch: 23 loss: 0.20135876536369324\n",
            "batch: 24 loss: 0.21167148649692535\n",
            "batch: 25 loss: 0.1645948588848114\n",
            "batch: 26 loss: 0.1759059876203537\n",
            "batch: 27 loss: 0.1815515011548996\n",
            "batch: 28 loss: 0.19465865194797516\n",
            "batch: 29 loss: 0.1676616370677948\n",
            "batch: 30 loss: 0.20499178767204285\n",
            "batch: 31 loss: 0.15049031376838684\n",
            "batch: 32 loss: 0.16435635089874268\n",
            "batch: 33 loss: 0.2508712410926819\n",
            "batch: 34 loss: 0.22729860246181488\n",
            "batch: 35 loss: 0.19766798615455627\n",
            "batch: 36 loss: 0.1479707807302475\n",
            "batch: 37 loss: 0.19781552255153656\n",
            "batch: 38 loss: 0.1794104278087616\n",
            "batch: 39 loss: 0.21389131247997284\n",
            "batch: 40 loss: 0.21144498884677887\n",
            "batch: 41 loss: 0.1848636120557785\n",
            "batch: 42 loss: 0.21735888719558716\n",
            "batch: 43 loss: 0.18932797014713287\n",
            "batch: 44 loss: 0.22654718160629272\n",
            "batch: 45 loss: 0.20183971524238586\n",
            "batch: 46 loss: 0.20394375920295715\n",
            "batch: 47 loss: 0.1931246519088745\n",
            "batch: 48 loss: 0.14768901467323303\n",
            "batch: 49 loss: 0.22588889300823212\n",
            "batch: 50 loss: 0.32191896438598633\n",
            "batch: 51 loss: 0.20729824900627136\n",
            "batch: 52 loss: 0.18428505957126617\n",
            "batch: 53 loss: 0.17717748880386353\n",
            "batch: 54 loss: 0.22546277940273285\n",
            "batch: 55 loss: 0.15221253037452698\n",
            "batch: 56 loss: 0.2295074462890625\n",
            "batch: 57 loss: 0.2757447361946106\n",
            "batch: 58 loss: 0.20360691845417023\n",
            "batch: 59 loss: 0.15242256224155426\n",
            "batch: 60 loss: 0.24734358489513397\n",
            "batch: 61 loss: 0.21125571429729462\n",
            "batch: 62 loss: 0.20663830637931824\n",
            "batch: 63 loss: 0.16823488473892212\n",
            "batch: 64 loss: 0.22880978882312775\n",
            "batch: 65 loss: 0.23804965615272522\n",
            "batch: 66 loss: 0.20876005291938782\n",
            "batch: 67 loss: 0.1778784841299057\n",
            "batch: 68 loss: 0.21221034228801727\n",
            "batch: 69 loss: 0.17598488926887512\n",
            "batch: 70 loss: 0.21814607083797455\n",
            "batch: 71 loss: 0.22704096138477325\n",
            "batch: 72 loss: 0.17702314257621765\n",
            "batch: 73 loss: 0.1653505265712738\n",
            "batch: 74 loss: 0.2528303563594818\n",
            "batch: 75 loss: 0.17691101133823395\n",
            "batch: 76 loss: 0.180833101272583\n",
            "batch: 77 loss: 0.1792568564414978\n",
            "batch: 78 loss: 0.18983569741249084\n",
            "batch: 79 loss: 0.18843884766101837\n",
            "batch: 80 loss: 0.23524917662143707\n",
            "batch: 81 loss: 0.20122595131397247\n",
            "batch: 82 loss: 0.17241168022155762\n",
            "batch: 83 loss: 0.23011742532253265\n",
            "batch: 84 loss: 0.146070659160614\n",
            "batch: 85 loss: 0.17879244685173035\n",
            "batch: 86 loss: 0.2351788878440857\n",
            "batch: 87 loss: 0.16704529523849487\n",
            "batch: 88 loss: 0.24905957281589508\n",
            "batch: 89 loss: 0.24039506912231445\n",
            "batch: 90 loss: 0.1803429126739502\n",
            "batch: 91 loss: 0.1671859174966812\n",
            "batch: 92 loss: 0.14902827143669128\n",
            "batch: 93 loss: 0.20155401527881622\n",
            "batch: 94 loss: 0.1773064136505127\n",
            "batch: 95 loss: 0.20373228192329407\n",
            "batch: 96 loss: 0.18448114395141602\n",
            "batch: 97 loss: 0.17080822587013245\n",
            "batch: 98 loss: 0.19520844519138336\n",
            "batch: 99 loss: 0.16483749449253082\n",
            "batch: 100 loss: 0.18558380007743835\n",
            "batch: 101 loss: 0.21745799481868744\n",
            "batch: 102 loss: 0.18609683215618134\n",
            "batch: 103 loss: 0.20923568308353424\n",
            "batch: 104 loss: 0.21322815120220184\n",
            "batch: 105 loss: 0.16511289775371552\n",
            "batch: 106 loss: 0.1880228966474533\n",
            "batch: 107 loss: 0.231914684176445\n",
            "batch: 108 loss: 0.23710325360298157\n",
            "batch: 109 loss: 0.20889419317245483\n",
            "batch: 110 loss: 0.22539865970611572\n",
            "batch: 111 loss: 0.17624717950820923\n",
            "batch: 112 loss: 0.21639783680438995\n",
            "batch: 113 loss: 0.2353173941373825\n",
            "batch: 114 loss: 0.19213825464248657\n",
            "batch: 115 loss: 0.17297232151031494\n",
            "batch: 116 loss: 0.19235824048519135\n",
            "batch: 117 loss: 0.2044927179813385\n",
            "batch: 118 loss: 0.1948193907737732\n",
            "batch: 119 loss: 0.2856317162513733\n",
            "batch: 120 loss: 0.19485996663570404\n",
            "batch: 121 loss: 0.18207141757011414\n",
            "batch: 122 loss: 0.16697503626346588\n",
            "batch: 123 loss: 0.23042906820774078\n",
            "batch: 124 loss: 0.19728681445121765\n",
            "batch: 125 loss: 0.21495656669139862\n",
            "batch: 126 loss: 0.20892222225666046\n",
            "batch: 127 loss: 0.22305697202682495\n",
            "batch: 128 loss: 0.22099092602729797\n",
            "batch: 129 loss: 0.198638916015625\n",
            "batch: 130 loss: 0.21881280839443207\n",
            "batch: 131 loss: 0.20946839451789856\n",
            "batch: 132 loss: 0.19750621914863586\n",
            "batch: 133 loss: 0.2518236041069031\n",
            "batch: 134 loss: 0.17391356825828552\n",
            "batch: 135 loss: 0.19933082163333893\n",
            "batch: 136 loss: 0.21493801474571228\n",
            "batch: 137 loss: 0.23871272802352905\n",
            "batch: 138 loss: 0.14282570779323578\n",
            "batch: 139 loss: 0.34240293502807617\n",
            "batch: 140 loss: 0.21149882674217224\n",
            "batch: 141 loss: 0.1855955272912979\n",
            "batch: 142 loss: 0.2783523201942444\n",
            "batch: 143 loss: 0.2519490420818329\n",
            "batch: 144 loss: 0.2243185192346573\n",
            "batch: 145 loss: 0.20133709907531738\n",
            "batch: 146 loss: 0.1591099202632904\n",
            "LOSS train 0.2378338873386383 valid 0.27408576011657715\n",
            "EPOCH 27:\n",
            "batch: 0 loss: 0.2004668116569519\n",
            "batch: 1 loss: 0.17733585834503174\n",
            "batch: 2 loss: 0.2455691546201706\n",
            "batch: 3 loss: 0.2011273056268692\n",
            "batch: 4 loss: 0.20586270093917847\n",
            "batch: 5 loss: 0.22885629534721375\n",
            "batch: 6 loss: 0.21519960463047028\n",
            "batch: 7 loss: 0.22398902475833893\n",
            "batch: 8 loss: 0.18848472833633423\n",
            "batch: 9 loss: 0.20116804540157318\n",
            "batch: 10 loss: 0.22008028626441956\n",
            "batch: 11 loss: 0.2769428491592407\n",
            "batch: 12 loss: 0.19073733687400818\n",
            "batch: 13 loss: 0.25429409742355347\n",
            "batch: 14 loss: 0.17302286624908447\n",
            "batch: 15 loss: 0.19687946140766144\n",
            "batch: 16 loss: 0.2709925174713135\n",
            "batch: 17 loss: 0.1793309450149536\n",
            "batch: 18 loss: 0.19305776059627533\n",
            "batch: 19 loss: 0.2100297510623932\n",
            "batch: 20 loss: 0.24110712110996246\n",
            "batch: 21 loss: 0.2042807638645172\n",
            "batch: 22 loss: 0.16589072346687317\n",
            "batch: 23 loss: 0.20974645018577576\n",
            "batch: 24 loss: 0.17538514733314514\n",
            "batch: 25 loss: 0.2518788278102875\n",
            "batch: 26 loss: 0.1834763139486313\n",
            "batch: 27 loss: 0.23461304605007172\n",
            "batch: 28 loss: 0.1763034164905548\n",
            "batch: 29 loss: 0.18008357286453247\n",
            "batch: 30 loss: 0.2229166179895401\n",
            "batch: 31 loss: 0.16211512684822083\n",
            "batch: 32 loss: 0.18445554375648499\n",
            "batch: 33 loss: 0.24531593918800354\n",
            "batch: 34 loss: 0.19373719394207\n",
            "batch: 35 loss: 0.1688009351491928\n",
            "batch: 36 loss: 0.2092134803533554\n",
            "batch: 37 loss: 0.1872933804988861\n",
            "batch: 38 loss: 0.16643880307674408\n",
            "batch: 39 loss: 0.1778889000415802\n",
            "batch: 40 loss: 0.16208991408348083\n",
            "batch: 41 loss: 0.21393689513206482\n",
            "batch: 42 loss: 0.21450908482074738\n",
            "batch: 43 loss: 0.1910231113433838\n",
            "batch: 44 loss: 0.1631137728691101\n",
            "batch: 45 loss: 0.13795128464698792\n",
            "batch: 46 loss: 0.21636545658111572\n",
            "batch: 47 loss: 0.17518284916877747\n",
            "batch: 48 loss: 0.19390682876110077\n",
            "batch: 49 loss: 0.15633761882781982\n",
            "batch: 50 loss: 0.17584224045276642\n",
            "batch: 51 loss: 0.16388028860092163\n",
            "batch: 52 loss: 0.1810041069984436\n",
            "batch: 53 loss: 0.18693818151950836\n",
            "batch: 54 loss: 0.24864473938941956\n",
            "batch: 55 loss: 0.17303156852722168\n",
            "batch: 56 loss: 0.29264938831329346\n",
            "batch: 57 loss: 0.1651737093925476\n",
            "batch: 58 loss: 0.17995798587799072\n",
            "batch: 59 loss: 0.17867979407310486\n",
            "batch: 60 loss: 0.18497580289840698\n",
            "batch: 61 loss: 0.2368718683719635\n",
            "batch: 62 loss: 0.23748907446861267\n",
            "batch: 63 loss: 0.2260008305311203\n",
            "batch: 64 loss: 0.21366804838180542\n",
            "batch: 65 loss: 0.19823049008846283\n",
            "batch: 66 loss: 0.18915291130542755\n",
            "batch: 67 loss: 0.2490283101797104\n",
            "batch: 68 loss: 0.2598511576652527\n",
            "batch: 69 loss: 0.1672808825969696\n",
            "batch: 70 loss: 0.20951484143733978\n",
            "batch: 71 loss: 0.1875847429037094\n",
            "batch: 72 loss: 0.1916174441576004\n",
            "batch: 73 loss: 0.22804825007915497\n",
            "batch: 74 loss: 0.1916733682155609\n",
            "batch: 75 loss: 0.18988151848316193\n",
            "batch: 76 loss: 0.19174650311470032\n",
            "batch: 77 loss: 0.206632599234581\n",
            "batch: 78 loss: 0.1514941155910492\n",
            "batch: 79 loss: 0.17007192969322205\n",
            "batch: 80 loss: 0.18890558183193207\n",
            "batch: 81 loss: 0.23394891619682312\n",
            "batch: 82 loss: 0.2194787561893463\n",
            "batch: 83 loss: 0.2124176174402237\n",
            "batch: 84 loss: 0.18848074972629547\n",
            "batch: 85 loss: 0.13915619254112244\n",
            "batch: 86 loss: 0.19330283999443054\n",
            "batch: 87 loss: 0.2000967562198639\n",
            "batch: 88 loss: 0.1943678855895996\n",
            "batch: 89 loss: 0.21419908106327057\n",
            "batch: 90 loss: 0.19802872836589813\n",
            "batch: 91 loss: 0.18867632746696472\n",
            "batch: 92 loss: 0.1859658658504486\n",
            "batch: 93 loss: 0.2023976594209671\n",
            "batch: 94 loss: 0.22785437107086182\n",
            "batch: 95 loss: 0.16984230279922485\n",
            "batch: 96 loss: 0.23088383674621582\n",
            "batch: 97 loss: 0.1838691383600235\n",
            "batch: 98 loss: 0.15133678913116455\n",
            "batch: 99 loss: 0.1830051839351654\n",
            "batch: 100 loss: 0.1621519923210144\n",
            "batch: 101 loss: 0.15126709640026093\n",
            "batch: 102 loss: 0.17543257772922516\n",
            "batch: 103 loss: 0.19953936338424683\n",
            "batch: 104 loss: 0.1609928458929062\n",
            "batch: 105 loss: 0.20683959126472473\n",
            "batch: 106 loss: 0.14588463306427002\n",
            "batch: 107 loss: 0.22054272890090942\n",
            "batch: 108 loss: 0.26841312646865845\n",
            "batch: 109 loss: 0.19843928515911102\n",
            "batch: 110 loss: 0.23094624280929565\n",
            "batch: 111 loss: 0.2005741000175476\n",
            "batch: 112 loss: 0.19426195323467255\n",
            "batch: 113 loss: 0.23670026659965515\n",
            "batch: 114 loss: 0.16768160462379456\n",
            "batch: 115 loss: 0.21870926022529602\n",
            "batch: 116 loss: 0.18271642923355103\n",
            "batch: 117 loss: 0.16692078113555908\n",
            "batch: 118 loss: 0.19946427643299103\n",
            "batch: 119 loss: 0.1962110996246338\n",
            "batch: 120 loss: 0.21153223514556885\n",
            "batch: 121 loss: 0.20488914847373962\n",
            "batch: 122 loss: 0.1969706118106842\n",
            "batch: 123 loss: 0.23380081355571747\n",
            "batch: 124 loss: 0.23283448815345764\n",
            "batch: 125 loss: 0.15204983949661255\n",
            "batch: 126 loss: 0.2234456092119217\n",
            "batch: 127 loss: 0.22147227823734283\n",
            "batch: 128 loss: 0.2434069961309433\n",
            "batch: 129 loss: 0.2100350558757782\n",
            "batch: 130 loss: 0.24281315505504608\n",
            "batch: 131 loss: 0.20225563645362854\n",
            "batch: 132 loss: 0.19795970618724823\n",
            "batch: 133 loss: 0.20725132524967194\n",
            "batch: 134 loss: 0.22369329631328583\n",
            "batch: 135 loss: 0.14572486281394958\n",
            "batch: 136 loss: 0.19129037857055664\n",
            "batch: 137 loss: 0.1875794529914856\n",
            "batch: 138 loss: 0.23184962570667267\n",
            "batch: 139 loss: 0.16380733251571655\n",
            "batch: 140 loss: 0.2512432336807251\n",
            "batch: 141 loss: 0.1431889683008194\n",
            "batch: 142 loss: 0.15584808588027954\n",
            "batch: 143 loss: 0.20101681351661682\n",
            "batch: 144 loss: 0.2116982638835907\n",
            "batch: 145 loss: 0.18978776037693024\n",
            "batch: 146 loss: 0.2963779866695404\n",
            "LOSS train 0.18705038726329803 valid 0.2269987165927887\n",
            "EPOCH 28:\n",
            "batch: 0 loss: 0.22323279082775116\n",
            "batch: 1 loss: 0.17891350388526917\n",
            "batch: 2 loss: 0.19080165028572083\n",
            "batch: 3 loss: 0.2293696254491806\n",
            "batch: 4 loss: 0.20760290324687958\n",
            "batch: 5 loss: 0.1942252516746521\n",
            "batch: 6 loss: 0.19945713877677917\n",
            "batch: 7 loss: 0.22027291357517242\n",
            "batch: 8 loss: 0.1872403919696808\n",
            "batch: 9 loss: 0.21056073904037476\n",
            "batch: 10 loss: 0.19621360301971436\n",
            "batch: 11 loss: 0.2231256067752838\n",
            "batch: 12 loss: 0.1521930694580078\n",
            "batch: 13 loss: 0.20900656282901764\n",
            "batch: 14 loss: 0.15539169311523438\n",
            "batch: 15 loss: 0.16007837653160095\n",
            "batch: 16 loss: 0.17758052051067352\n",
            "batch: 17 loss: 0.17659154534339905\n",
            "batch: 18 loss: 0.1761646419763565\n",
            "batch: 19 loss: 0.15393035113811493\n",
            "batch: 20 loss: 0.3173639178276062\n",
            "batch: 21 loss: 0.17307746410369873\n",
            "batch: 22 loss: 0.17483851313591003\n",
            "batch: 23 loss: 0.18973344564437866\n",
            "batch: 24 loss: 0.16549614071846008\n",
            "batch: 25 loss: 0.1735074818134308\n",
            "batch: 26 loss: 0.18826794624328613\n",
            "batch: 27 loss: 0.19549700617790222\n",
            "batch: 28 loss: 0.16862305998802185\n",
            "batch: 29 loss: 0.22657465934753418\n",
            "batch: 30 loss: 0.23596428334712982\n",
            "batch: 31 loss: 0.14537659287452698\n",
            "batch: 32 loss: 0.17436958849430084\n",
            "batch: 33 loss: 0.21912428736686707\n",
            "batch: 34 loss: 0.18706217408180237\n",
            "batch: 35 loss: 0.1939692348241806\n",
            "batch: 36 loss: 0.20840021967887878\n",
            "batch: 37 loss: 0.2229975312948227\n",
            "batch: 38 loss: 0.20771388709545135\n",
            "batch: 39 loss: 0.1635798215866089\n",
            "batch: 40 loss: 0.1761055439710617\n",
            "batch: 41 loss: 0.17776793241500854\n",
            "batch: 42 loss: 0.20324236154556274\n",
            "batch: 43 loss: 0.18024493753910065\n",
            "batch: 44 loss: 0.17285025119781494\n",
            "batch: 45 loss: 0.19671593606472015\n",
            "batch: 46 loss: 0.15921366214752197\n",
            "batch: 47 loss: 0.1780715435743332\n",
            "batch: 48 loss: 0.2106853872537613\n",
            "batch: 49 loss: 0.1922905445098877\n",
            "batch: 50 loss: 0.18981216847896576\n",
            "batch: 51 loss: 0.21542784571647644\n",
            "batch: 52 loss: 0.21512429416179657\n",
            "batch: 53 loss: 0.16933080554008484\n",
            "batch: 54 loss: 0.178736612200737\n",
            "batch: 55 loss: 0.2253531515598297\n",
            "batch: 56 loss: 0.2267271876335144\n",
            "batch: 57 loss: 0.17336858808994293\n",
            "batch: 58 loss: 0.16608873009681702\n",
            "batch: 59 loss: 0.17290809750556946\n",
            "batch: 60 loss: 0.1981489658355713\n",
            "batch: 61 loss: 0.19282977283000946\n",
            "batch: 62 loss: 0.18252484500408173\n",
            "batch: 63 loss: 0.20937490463256836\n",
            "batch: 64 loss: 0.18492712080478668\n",
            "batch: 65 loss: 0.1701659858226776\n",
            "batch: 66 loss: 0.2086036056280136\n",
            "batch: 67 loss: 0.16590535640716553\n",
            "batch: 68 loss: 0.17500784993171692\n",
            "batch: 69 loss: 0.21776339411735535\n",
            "batch: 70 loss: 0.16622579097747803\n",
            "batch: 71 loss: 0.19261547923088074\n",
            "batch: 72 loss: 0.14939463138580322\n",
            "batch: 73 loss: 0.1767570823431015\n",
            "batch: 74 loss: 0.19468945264816284\n",
            "batch: 75 loss: 0.21018405258655548\n",
            "batch: 76 loss: 0.2248626947402954\n",
            "batch: 77 loss: 0.16095280647277832\n",
            "batch: 78 loss: 0.18752740323543549\n",
            "batch: 79 loss: 0.1933182179927826\n",
            "batch: 80 loss: 0.1946410983800888\n",
            "batch: 81 loss: 0.15690878033638\n",
            "batch: 82 loss: 0.1480311006307602\n",
            "batch: 83 loss: 0.17714723944664001\n",
            "batch: 84 loss: 0.21403487026691437\n",
            "batch: 85 loss: 0.1687248945236206\n",
            "batch: 86 loss: 0.20655038952827454\n",
            "batch: 87 loss: 0.23447784781455994\n",
            "batch: 88 loss: 0.15210965275764465\n",
            "batch: 89 loss: 0.2143358737230301\n",
            "batch: 90 loss: 0.16396209597587585\n",
            "batch: 91 loss: 0.18581491708755493\n",
            "batch: 92 loss: 0.14913836121559143\n",
            "batch: 93 loss: 0.19619858264923096\n",
            "batch: 94 loss: 0.1971985548734665\n",
            "batch: 95 loss: 0.19977378845214844\n",
            "batch: 96 loss: 0.15295997262001038\n",
            "batch: 97 loss: 0.23215144872665405\n",
            "batch: 98 loss: 0.21422885358333588\n",
            "batch: 99 loss: 0.14224909245967865\n",
            "batch: 100 loss: 0.16523781418800354\n",
            "batch: 101 loss: 0.16063933074474335\n",
            "batch: 102 loss: 0.1642916202545166\n",
            "batch: 103 loss: 0.2107483595609665\n",
            "batch: 104 loss: 0.1983088254928589\n",
            "batch: 105 loss: 0.18891635537147522\n",
            "batch: 106 loss: 0.1935538947582245\n",
            "batch: 107 loss: 0.20756983757019043\n",
            "batch: 108 loss: 0.2067750096321106\n",
            "batch: 109 loss: 0.22455720603466034\n",
            "batch: 110 loss: 0.19612756371498108\n",
            "batch: 111 loss: 0.18159866333007812\n",
            "batch: 112 loss: 0.1873060017824173\n",
            "batch: 113 loss: 0.1864551305770874\n",
            "batch: 114 loss: 0.20406462252140045\n",
            "batch: 115 loss: 0.16020843386650085\n",
            "batch: 116 loss: 0.18087226152420044\n",
            "batch: 117 loss: 0.20575813949108124\n",
            "batch: 118 loss: 0.20070041716098785\n",
            "batch: 119 loss: 0.1559760868549347\n",
            "batch: 120 loss: 0.1950124055147171\n",
            "batch: 121 loss: 0.17265328764915466\n",
            "batch: 122 loss: 0.22361527383327484\n",
            "batch: 123 loss: 0.1581869125366211\n",
            "batch: 124 loss: 0.1510862410068512\n",
            "batch: 125 loss: 0.17563053965568542\n",
            "batch: 126 loss: 0.23190581798553467\n",
            "batch: 127 loss: 0.1739213764667511\n",
            "batch: 128 loss: 0.1562088131904602\n",
            "batch: 129 loss: 0.20118454098701477\n",
            "batch: 130 loss: 0.1687629520893097\n",
            "batch: 131 loss: 0.17753630876541138\n",
            "batch: 132 loss: 0.19388964772224426\n",
            "batch: 133 loss: 0.16232283413410187\n",
            "batch: 134 loss: 0.22443921864032745\n",
            "batch: 135 loss: 0.20202983915805817\n",
            "batch: 136 loss: 0.1945704221725464\n",
            "batch: 137 loss: 0.1827455759048462\n",
            "batch: 138 loss: 0.21442028880119324\n",
            "batch: 139 loss: 0.1484164297580719\n",
            "batch: 140 loss: 0.19349819421768188\n",
            "batch: 141 loss: 0.14634430408477783\n",
            "batch: 142 loss: 0.18884000182151794\n",
            "batch: 143 loss: 0.20770137012004852\n",
            "batch: 144 loss: 0.23951052129268646\n",
            "batch: 145 loss: 0.13901370763778687\n",
            "batch: 146 loss: 0.14810413122177124\n",
            "LOSS train 0.19589613378047943 valid 0.2425394058227539\n",
            "EPOCH 29:\n",
            "batch: 0 loss: 0.16984394192695618\n",
            "batch: 1 loss: 0.2088424116373062\n",
            "batch: 2 loss: 0.14432042837142944\n",
            "batch: 3 loss: 0.23022900521755219\n",
            "batch: 4 loss: 0.24450351297855377\n",
            "batch: 5 loss: 0.16623327136039734\n",
            "batch: 6 loss: 0.20289072394371033\n",
            "batch: 7 loss: 0.18956822156906128\n",
            "batch: 8 loss: 0.15798020362854004\n",
            "batch: 9 loss: 0.23884165287017822\n",
            "batch: 10 loss: 0.1513359248638153\n",
            "batch: 11 loss: 0.14232632517814636\n",
            "batch: 12 loss: 0.17416569590568542\n",
            "batch: 13 loss: 0.16969506442546844\n",
            "batch: 14 loss: 0.15805792808532715\n",
            "batch: 15 loss: 0.15331679582595825\n",
            "batch: 16 loss: 0.21290820837020874\n",
            "batch: 17 loss: 0.18365168571472168\n",
            "batch: 18 loss: 0.2538979649543762\n",
            "batch: 19 loss: 0.20416918396949768\n",
            "batch: 20 loss: 0.19646532833576202\n",
            "batch: 21 loss: 0.17785003781318665\n",
            "batch: 22 loss: 0.24138614535331726\n",
            "batch: 23 loss: 0.17595088481903076\n",
            "batch: 24 loss: 0.21826410293579102\n",
            "batch: 25 loss: 0.1871240735054016\n",
            "batch: 26 loss: 0.17097949981689453\n",
            "batch: 27 loss: 0.17865373194217682\n",
            "batch: 28 loss: 0.1799941062927246\n",
            "batch: 29 loss: 0.18480215966701508\n",
            "batch: 30 loss: 0.1584765762090683\n",
            "batch: 31 loss: 0.2232537865638733\n",
            "batch: 32 loss: 0.1841738224029541\n",
            "batch: 33 loss: 0.23003919422626495\n",
            "batch: 34 loss: 0.21770578622817993\n",
            "batch: 35 loss: 0.1594531536102295\n",
            "batch: 36 loss: 0.17882049083709717\n",
            "batch: 37 loss: 0.24082720279693604\n",
            "batch: 38 loss: 0.16987088322639465\n",
            "batch: 39 loss: 0.20171771943569183\n",
            "batch: 40 loss: 0.23208282887935638\n",
            "batch: 41 loss: 0.20974984765052795\n",
            "batch: 42 loss: 0.21750737726688385\n",
            "batch: 43 loss: 0.1783856898546219\n",
            "batch: 44 loss: 0.18660162389278412\n",
            "batch: 45 loss: 0.19253414869308472\n",
            "batch: 46 loss: 0.15788117051124573\n",
            "batch: 47 loss: 0.15261897444725037\n",
            "batch: 48 loss: 0.15288816392421722\n",
            "batch: 49 loss: 0.16892275214195251\n",
            "batch: 50 loss: 0.15296772122383118\n",
            "batch: 51 loss: 0.18560169637203217\n",
            "batch: 52 loss: 0.15021803975105286\n",
            "batch: 53 loss: 0.15293580293655396\n",
            "batch: 54 loss: 0.22864671051502228\n",
            "batch: 55 loss: 0.19988954067230225\n",
            "batch: 56 loss: 0.23142372071743011\n",
            "batch: 57 loss: 0.16745451092720032\n",
            "batch: 58 loss: 0.18252621591091156\n",
            "batch: 59 loss: 0.2412480264902115\n",
            "batch: 60 loss: 0.16215740144252777\n",
            "batch: 61 loss: 0.17894184589385986\n",
            "batch: 62 loss: 0.13328245282173157\n",
            "batch: 63 loss: 0.14008808135986328\n",
            "batch: 64 loss: 0.1934124082326889\n",
            "batch: 65 loss: 0.18805067241191864\n",
            "batch: 66 loss: 0.2535412907600403\n",
            "batch: 67 loss: 0.15328490734100342\n",
            "batch: 68 loss: 0.1921887844800949\n",
            "batch: 69 loss: 0.15565700829029083\n",
            "batch: 70 loss: 0.19491015374660492\n",
            "batch: 71 loss: 0.1877666711807251\n",
            "batch: 72 loss: 0.2120111584663391\n",
            "batch: 73 loss: 0.16211289167404175\n",
            "batch: 74 loss: 0.18548280000686646\n",
            "batch: 75 loss: 0.18106210231781006\n",
            "batch: 76 loss: 0.1761648803949356\n",
            "batch: 77 loss: 0.18227988481521606\n",
            "batch: 78 loss: 0.2308291792869568\n",
            "batch: 79 loss: 0.2281467616558075\n",
            "batch: 80 loss: 0.16549499332904816\n",
            "batch: 81 loss: 0.16893887519836426\n",
            "batch: 82 loss: 0.21017757058143616\n",
            "batch: 83 loss: 0.17129644751548767\n",
            "batch: 84 loss: 0.17945560812950134\n",
            "batch: 85 loss: 0.1744750738143921\n",
            "batch: 86 loss: 0.16324907541275024\n",
            "batch: 87 loss: 0.2142709195613861\n",
            "batch: 88 loss: 0.20485225319862366\n",
            "batch: 89 loss: 0.16014228761196136\n",
            "batch: 90 loss: 0.1716267168521881\n",
            "batch: 91 loss: 0.16618388891220093\n",
            "batch: 92 loss: 0.17776191234588623\n",
            "batch: 93 loss: 0.18870314955711365\n",
            "batch: 94 loss: 0.16397394239902496\n",
            "batch: 95 loss: 0.16020819544792175\n",
            "batch: 96 loss: 0.25174689292907715\n",
            "batch: 97 loss: 0.1558731198310852\n",
            "batch: 98 loss: 0.1725253164768219\n",
            "batch: 99 loss: 0.18652187287807465\n",
            "batch: 100 loss: 0.18532155454158783\n",
            "batch: 101 loss: 0.19792070984840393\n",
            "batch: 102 loss: 0.21047133207321167\n",
            "batch: 103 loss: 0.20518025755882263\n",
            "batch: 104 loss: 0.16588187217712402\n",
            "batch: 105 loss: 0.20035474002361298\n",
            "batch: 106 loss: 0.1714065670967102\n",
            "batch: 107 loss: 0.22816455364227295\n",
            "batch: 108 loss: 0.20012669265270233\n",
            "batch: 109 loss: 0.15129977464675903\n",
            "batch: 110 loss: 0.25404179096221924\n",
            "batch: 111 loss: 0.18946392834186554\n",
            "batch: 112 loss: 0.25731170177459717\n",
            "batch: 113 loss: 0.14157003164291382\n",
            "batch: 114 loss: 0.16836708784103394\n",
            "batch: 115 loss: 0.22013229131698608\n",
            "batch: 116 loss: 0.22333475947380066\n",
            "batch: 117 loss: 0.16752158105373383\n",
            "batch: 118 loss: 0.16608703136444092\n",
            "batch: 119 loss: 0.2396155297756195\n",
            "batch: 120 loss: 0.19068266451358795\n",
            "batch: 121 loss: 0.19268956780433655\n",
            "batch: 122 loss: 0.17966647446155548\n",
            "batch: 123 loss: 0.16214419901371002\n",
            "batch: 124 loss: 0.2331453561782837\n",
            "batch: 125 loss: 0.15878918766975403\n",
            "batch: 126 loss: 0.17385417222976685\n",
            "batch: 127 loss: 0.16860772669315338\n",
            "batch: 128 loss: 0.18960267305374146\n",
            "batch: 129 loss: 0.21229062974452972\n",
            "batch: 130 loss: 0.16279751062393188\n",
            "batch: 131 loss: 0.2440546452999115\n",
            "batch: 132 loss: 0.2458895593881607\n",
            "batch: 133 loss: 0.1734776645898819\n",
            "batch: 134 loss: 0.17733077704906464\n",
            "batch: 135 loss: 0.17810846865177155\n",
            "batch: 136 loss: 0.253706157207489\n",
            "batch: 137 loss: 0.15782782435417175\n",
            "batch: 138 loss: 0.21071377396583557\n",
            "batch: 139 loss: 0.18944363296031952\n",
            "batch: 140 loss: 0.18072494864463806\n",
            "batch: 141 loss: 0.1648315042257309\n",
            "batch: 142 loss: 0.23249365389347076\n",
            "batch: 143 loss: 0.16260629892349243\n",
            "batch: 144 loss: 0.15295560657978058\n",
            "batch: 145 loss: 0.19070737063884735\n",
            "batch: 146 loss: 0.2345195859670639\n",
            "LOSS train 0.17983393371105194 valid 0.22604341804981232\n",
            "EPOCH 30:\n",
            "batch: 0 loss: 0.17892490327358246\n",
            "batch: 1 loss: 0.19529247283935547\n",
            "batch: 2 loss: 0.17094162106513977\n",
            "batch: 3 loss: 0.20930388569831848\n",
            "batch: 4 loss: 0.22392284870147705\n",
            "batch: 5 loss: 0.19616778194904327\n",
            "batch: 6 loss: 0.20515884459018707\n",
            "batch: 7 loss: 0.19127081334590912\n",
            "batch: 8 loss: 0.18380150198936462\n",
            "batch: 9 loss: 0.18209348618984222\n",
            "batch: 10 loss: 0.23172248899936676\n",
            "batch: 11 loss: 0.1396852731704712\n",
            "batch: 12 loss: 0.13259288668632507\n",
            "batch: 13 loss: 0.19308409094810486\n",
            "batch: 14 loss: 0.17764630913734436\n",
            "batch: 15 loss: 0.195741206407547\n",
            "batch: 16 loss: 0.17378965020179749\n",
            "batch: 17 loss: 0.1952412873506546\n",
            "batch: 18 loss: 0.19308578968048096\n",
            "batch: 19 loss: 0.21613937616348267\n",
            "batch: 20 loss: 0.16257476806640625\n",
            "batch: 21 loss: 0.18964460492134094\n",
            "batch: 22 loss: 0.16928985714912415\n",
            "batch: 23 loss: 0.1955263763666153\n",
            "batch: 24 loss: 0.18613439798355103\n",
            "batch: 25 loss: 0.15999917685985565\n",
            "batch: 26 loss: 0.17861086130142212\n",
            "batch: 27 loss: 0.21235822141170502\n",
            "batch: 28 loss: 0.18786807358264923\n",
            "batch: 29 loss: 0.20888152718544006\n",
            "batch: 30 loss: 0.1880963295698166\n",
            "batch: 31 loss: 0.16151857376098633\n",
            "batch: 32 loss: 0.23152154684066772\n",
            "batch: 33 loss: 0.168002188205719\n",
            "batch: 34 loss: 0.254361093044281\n",
            "batch: 35 loss: 0.17275384068489075\n",
            "batch: 36 loss: 0.16493964195251465\n",
            "batch: 37 loss: 0.1738015115261078\n",
            "batch: 38 loss: 0.20921221375465393\n",
            "batch: 39 loss: 0.20730189979076385\n",
            "batch: 40 loss: 0.22926798462867737\n",
            "batch: 41 loss: 0.18347856402397156\n",
            "batch: 42 loss: 0.20318736135959625\n",
            "batch: 43 loss: 0.1904364973306656\n",
            "batch: 44 loss: 0.20335398614406586\n",
            "batch: 45 loss: 0.16673535108566284\n",
            "batch: 46 loss: 0.20443148910999298\n",
            "batch: 47 loss: 0.22373820841312408\n",
            "batch: 48 loss: 0.1843138337135315\n",
            "batch: 49 loss: 0.18501082062721252\n",
            "batch: 50 loss: 0.17631064355373383\n",
            "batch: 51 loss: 0.1688041388988495\n",
            "batch: 52 loss: 0.18832401931285858\n",
            "batch: 53 loss: 0.2203492969274521\n",
            "batch: 54 loss: 0.18196141719818115\n",
            "batch: 55 loss: 0.20542213320732117\n",
            "batch: 56 loss: 0.15842533111572266\n",
            "batch: 57 loss: 0.19131816923618317\n",
            "batch: 58 loss: 0.1660543978214264\n",
            "batch: 59 loss: 0.15363195538520813\n",
            "batch: 60 loss: 0.13918501138687134\n",
            "batch: 61 loss: 0.2859776020050049\n",
            "batch: 62 loss: 0.17729730904102325\n",
            "batch: 63 loss: 0.1330280303955078\n",
            "batch: 64 loss: 0.15788252651691437\n",
            "batch: 65 loss: 0.18087276816368103\n",
            "batch: 66 loss: 0.1967357099056244\n",
            "batch: 67 loss: 0.17613109946250916\n",
            "batch: 68 loss: 0.20216384530067444\n",
            "batch: 69 loss: 0.2046673595905304\n",
            "batch: 70 loss: 0.19592157006263733\n",
            "batch: 71 loss: 0.18078646063804626\n",
            "batch: 72 loss: 0.17710912227630615\n",
            "batch: 73 loss: 0.19744420051574707\n",
            "batch: 74 loss: 0.21447724103927612\n",
            "batch: 75 loss: 0.15985703468322754\n",
            "batch: 76 loss: 0.1795683056116104\n",
            "batch: 77 loss: 0.18418778479099274\n",
            "batch: 78 loss: 0.21509438753128052\n",
            "batch: 79 loss: 0.18421527743339539\n",
            "batch: 80 loss: 0.15766479074954987\n",
            "batch: 81 loss: 0.1406981348991394\n",
            "batch: 82 loss: 0.19645951688289642\n",
            "batch: 83 loss: 0.24149088561534882\n",
            "batch: 84 loss: 0.14692071080207825\n",
            "batch: 85 loss: 0.19606998562812805\n",
            "batch: 86 loss: 0.16429322957992554\n",
            "batch: 87 loss: 0.1957145482301712\n",
            "batch: 88 loss: 0.2629731595516205\n",
            "batch: 89 loss: 0.20176377892494202\n",
            "batch: 90 loss: 0.20927663147449493\n",
            "batch: 91 loss: 0.14835482835769653\n",
            "batch: 92 loss: 0.14534445106983185\n",
            "batch: 93 loss: 0.16154338419437408\n",
            "batch: 94 loss: 0.16096419095993042\n",
            "batch: 95 loss: 0.1708412766456604\n",
            "batch: 96 loss: 0.13009807467460632\n",
            "batch: 97 loss: 0.21833233535289764\n",
            "batch: 98 loss: 0.19986239075660706\n",
            "batch: 99 loss: 0.15582597255706787\n",
            "batch: 100 loss: 0.22730059921741486\n",
            "batch: 101 loss: 0.19513481855392456\n",
            "batch: 102 loss: 0.17490509152412415\n",
            "batch: 103 loss: 0.1628168225288391\n",
            "batch: 104 loss: 0.16517934203147888\n",
            "batch: 105 loss: 0.1559295356273651\n",
            "batch: 106 loss: 0.17468439042568207\n",
            "batch: 107 loss: 0.14637428522109985\n",
            "batch: 108 loss: 0.16591599583625793\n",
            "batch: 109 loss: 0.17521518468856812\n",
            "batch: 110 loss: 0.20653662085533142\n",
            "batch: 111 loss: 0.17762553691864014\n",
            "batch: 112 loss: 0.15364795923233032\n",
            "batch: 113 loss: 0.165012389421463\n",
            "batch: 114 loss: 0.1857321411371231\n",
            "batch: 115 loss: 0.28353971242904663\n",
            "batch: 116 loss: 0.2256455272436142\n",
            "batch: 117 loss: 0.1754624843597412\n",
            "batch: 118 loss: 0.19808201491832733\n",
            "batch: 119 loss: 0.1993987113237381\n",
            "batch: 120 loss: 0.12034808844327927\n",
            "batch: 121 loss: 0.18951678276062012\n",
            "batch: 122 loss: 0.15497305989265442\n",
            "batch: 123 loss: 0.20622214674949646\n",
            "batch: 124 loss: 0.1757168173789978\n",
            "batch: 125 loss: 0.17542533576488495\n",
            "batch: 126 loss: 0.2098391205072403\n",
            "batch: 127 loss: 0.18858855962753296\n",
            "batch: 128 loss: 0.16149425506591797\n",
            "batch: 129 loss: 0.18136468529701233\n",
            "batch: 130 loss: 0.15636496245861053\n",
            "batch: 131 loss: 0.15392309427261353\n",
            "batch: 132 loss: 0.16815486550331116\n",
            "batch: 133 loss: 0.1581690013408661\n",
            "batch: 134 loss: 0.16311220824718475\n",
            "batch: 135 loss: 0.15763717889785767\n",
            "batch: 136 loss: 0.19738200306892395\n",
            "batch: 137 loss: 0.18075783550739288\n",
            "batch: 138 loss: 0.22446030378341675\n",
            "batch: 139 loss: 0.17935946583747864\n",
            "batch: 140 loss: 0.17360946536064148\n",
            "batch: 141 loss: 0.16868814826011658\n",
            "batch: 142 loss: 0.20401906967163086\n",
            "batch: 143 loss: 0.19195421040058136\n",
            "batch: 144 loss: 0.1952763944864273\n",
            "batch: 145 loss: 0.14427226781845093\n",
            "batch: 146 loss: 0.10936915874481201\n",
            "LOSS train 0.18306376039981842 valid 0.22655552625656128\n",
            "EPOCH 31:\n",
            "batch: 0 loss: 0.15263286232948303\n",
            "batch: 1 loss: 0.2057538777589798\n",
            "batch: 2 loss: 0.19115784764289856\n",
            "batch: 3 loss: 0.1705443263053894\n",
            "batch: 4 loss: 0.16066104173660278\n",
            "batch: 5 loss: 0.15885409712791443\n",
            "batch: 6 loss: 0.1727389097213745\n",
            "batch: 7 loss: 0.14956724643707275\n",
            "batch: 8 loss: 0.1883387267589569\n",
            "batch: 9 loss: 0.20084035396575928\n",
            "batch: 10 loss: 0.19257014989852905\n",
            "batch: 11 loss: 0.18439969420433044\n",
            "batch: 12 loss: 0.18389543890953064\n",
            "batch: 13 loss: 0.16883128881454468\n",
            "batch: 14 loss: 0.2068178504705429\n",
            "batch: 15 loss: 0.15937939286231995\n",
            "batch: 16 loss: 0.17821110785007477\n",
            "batch: 17 loss: 0.14776349067687988\n",
            "batch: 18 loss: 0.16355274617671967\n",
            "batch: 19 loss: 0.15114283561706543\n",
            "batch: 20 loss: 0.19991280138492584\n",
            "batch: 21 loss: 0.19578075408935547\n",
            "batch: 22 loss: 0.1430668830871582\n",
            "batch: 23 loss: 0.13993549346923828\n",
            "batch: 24 loss: 0.20353281497955322\n",
            "batch: 25 loss: 0.19358575344085693\n",
            "batch: 26 loss: 0.1611391305923462\n",
            "batch: 27 loss: 0.16221648454666138\n",
            "batch: 28 loss: 0.1840934157371521\n",
            "batch: 29 loss: 0.19478750228881836\n",
            "batch: 30 loss: 0.16150227189064026\n",
            "batch: 31 loss: 0.18494679033756256\n",
            "batch: 32 loss: 0.17426468431949615\n",
            "batch: 33 loss: 0.16617751121520996\n",
            "batch: 34 loss: 0.1468338817358017\n",
            "batch: 35 loss: 0.19112370908260345\n",
            "batch: 36 loss: 0.14556047320365906\n",
            "batch: 37 loss: 0.15579304099082947\n",
            "batch: 38 loss: 0.2200271338224411\n",
            "batch: 39 loss: 0.16284580528736115\n",
            "batch: 40 loss: 0.14570170640945435\n",
            "batch: 41 loss: 0.1822400689125061\n",
            "batch: 42 loss: 0.20292744040489197\n",
            "batch: 43 loss: 0.14407853782176971\n",
            "batch: 44 loss: 0.17297658324241638\n",
            "batch: 45 loss: 0.20176544785499573\n",
            "batch: 46 loss: 0.1767129898071289\n",
            "batch: 47 loss: 0.17572751641273499\n",
            "batch: 48 loss: 0.19149711728096008\n",
            "batch: 49 loss: 0.1772567331790924\n",
            "batch: 50 loss: 0.23741693794727325\n",
            "batch: 51 loss: 0.18268762528896332\n",
            "batch: 52 loss: 0.16752731800079346\n",
            "batch: 53 loss: 0.17940977215766907\n",
            "batch: 54 loss: 0.1869024634361267\n",
            "batch: 55 loss: 0.12897290289402008\n",
            "batch: 56 loss: 0.1940917819738388\n",
            "batch: 57 loss: 0.2208413928747177\n",
            "batch: 58 loss: 0.19168539345264435\n",
            "batch: 59 loss: 0.17178842425346375\n",
            "batch: 60 loss: 0.1948913335800171\n",
            "batch: 61 loss: 0.1761418581008911\n",
            "batch: 62 loss: 0.21357907354831696\n",
            "batch: 63 loss: 0.14224451780319214\n",
            "batch: 64 loss: 0.1763472557067871\n",
            "batch: 65 loss: 0.16329964995384216\n",
            "batch: 66 loss: 0.16371020674705505\n",
            "batch: 67 loss: 0.21606382727622986\n",
            "batch: 68 loss: 0.1924046277999878\n",
            "batch: 69 loss: 0.18288962543010712\n",
            "batch: 70 loss: 0.1481221616268158\n",
            "batch: 71 loss: 0.16848164796829224\n",
            "batch: 72 loss: 0.1795625239610672\n",
            "batch: 73 loss: 0.15358786284923553\n",
            "batch: 74 loss: 0.14415384829044342\n",
            "batch: 75 loss: 0.22668257355690002\n",
            "batch: 76 loss: 0.19751672446727753\n",
            "batch: 77 loss: 0.20245276391506195\n",
            "batch: 78 loss: 0.17916390299797058\n",
            "batch: 79 loss: 0.18485493957996368\n",
            "batch: 80 loss: 0.1620226949453354\n",
            "batch: 81 loss: 0.13545536994934082\n",
            "batch: 82 loss: 0.23260828852653503\n",
            "batch: 83 loss: 0.1782928705215454\n",
            "batch: 84 loss: 0.1856154054403305\n",
            "batch: 85 loss: 0.14569440484046936\n",
            "batch: 86 loss: 0.18100605905056\n",
            "batch: 87 loss: 0.18987517058849335\n",
            "batch: 88 loss: 0.21129335463047028\n",
            "batch: 89 loss: 0.15033981204032898\n",
            "batch: 90 loss: 0.17643779516220093\n",
            "batch: 91 loss: 0.15276934206485748\n",
            "batch: 92 loss: 0.21593263745307922\n",
            "batch: 93 loss: 0.1644982397556305\n",
            "batch: 94 loss: 0.17572790384292603\n",
            "batch: 95 loss: 0.15995243191719055\n",
            "batch: 96 loss: 0.20406141877174377\n",
            "batch: 97 loss: 0.1807081252336502\n",
            "batch: 98 loss: 0.1813911497592926\n",
            "batch: 99 loss: 0.16418959200382233\n",
            "batch: 100 loss: 0.1987440139055252\n",
            "batch: 101 loss: 0.17081476747989655\n",
            "batch: 102 loss: 0.14666078984737396\n",
            "batch: 103 loss: 0.18069133162498474\n",
            "batch: 104 loss: 0.1958167552947998\n",
            "batch: 105 loss: 0.16415196657180786\n",
            "batch: 106 loss: 0.1376107633113861\n",
            "batch: 107 loss: 0.211792454123497\n",
            "batch: 108 loss: 0.16733884811401367\n",
            "batch: 109 loss: 0.19697271287441254\n",
            "batch: 110 loss: 0.2506535053253174\n",
            "batch: 111 loss: 0.19824621081352234\n",
            "batch: 112 loss: 0.16971507668495178\n",
            "batch: 113 loss: 0.20755364000797272\n",
            "batch: 114 loss: 0.12744110822677612\n",
            "batch: 115 loss: 0.17036202549934387\n",
            "batch: 116 loss: 0.1984718292951584\n",
            "batch: 117 loss: 0.19022433459758759\n",
            "batch: 118 loss: 0.1339912712574005\n",
            "batch: 119 loss: 0.1939767748117447\n",
            "batch: 120 loss: 0.19171273708343506\n",
            "batch: 121 loss: 0.1588342785835266\n",
            "batch: 122 loss: 0.17127785086631775\n",
            "batch: 123 loss: 0.22274282574653625\n",
            "batch: 124 loss: 0.16549117863178253\n",
            "batch: 125 loss: 0.14560717344284058\n",
            "batch: 126 loss: 0.17884516716003418\n",
            "batch: 127 loss: 0.2113901972770691\n",
            "batch: 128 loss: 0.20728731155395508\n",
            "batch: 129 loss: 0.166030615568161\n",
            "batch: 130 loss: 0.15087899565696716\n",
            "batch: 131 loss: 0.1755388081073761\n",
            "batch: 132 loss: 0.17500555515289307\n",
            "batch: 133 loss: 0.16565093398094177\n",
            "batch: 134 loss: 0.16345971822738647\n",
            "batch: 135 loss: 0.2530534267425537\n",
            "batch: 136 loss: 0.16770631074905396\n",
            "batch: 137 loss: 0.20284485816955566\n",
            "batch: 138 loss: 0.17663447558879852\n",
            "batch: 139 loss: 0.18173018097877502\n",
            "batch: 140 loss: 0.19923193752765656\n",
            "batch: 141 loss: 0.14916333556175232\n",
            "batch: 142 loss: 0.16761629283428192\n",
            "batch: 143 loss: 0.18858224153518677\n",
            "batch: 144 loss: 0.1831180453300476\n",
            "batch: 145 loss: 0.15548565983772278\n",
            "batch: 146 loss: 0.1577683538198471\n",
            "LOSS train 0.17781813442707062 valid 0.2349899709224701\n",
            "EPOCH 32:\n",
            "batch: 0 loss: 0.16045358777046204\n",
            "batch: 1 loss: 0.17126256227493286\n",
            "batch: 2 loss: 0.2353225201368332\n",
            "batch: 3 loss: 0.13944852352142334\n",
            "batch: 4 loss: 0.1986900418996811\n",
            "batch: 5 loss: 0.18533365428447723\n",
            "batch: 6 loss: 0.1321025788784027\n",
            "batch: 7 loss: 0.15907123684883118\n",
            "batch: 8 loss: 0.15308241546154022\n",
            "batch: 9 loss: 0.1781260073184967\n",
            "batch: 10 loss: 0.14402443170547485\n",
            "batch: 11 loss: 0.149085134267807\n",
            "batch: 12 loss: 0.1751059889793396\n",
            "batch: 13 loss: 0.14985637366771698\n",
            "batch: 14 loss: 0.16031721234321594\n",
            "batch: 15 loss: 0.18546324968338013\n",
            "batch: 16 loss: 0.17757084965705872\n",
            "batch: 17 loss: 0.1787889003753662\n",
            "batch: 18 loss: 0.11955331265926361\n",
            "batch: 19 loss: 0.18147625029087067\n",
            "batch: 20 loss: 0.17299555242061615\n",
            "batch: 21 loss: 0.17352069914340973\n",
            "batch: 22 loss: 0.1357150673866272\n",
            "batch: 23 loss: 0.1560017168521881\n",
            "batch: 24 loss: 0.15770144760608673\n",
            "batch: 25 loss: 0.1445768177509308\n",
            "batch: 26 loss: 0.18807901442050934\n",
            "batch: 27 loss: 0.13525453209877014\n",
            "batch: 28 loss: 0.18464556336402893\n",
            "batch: 29 loss: 0.15567660331726074\n",
            "batch: 30 loss: 0.1556205451488495\n",
            "batch: 31 loss: 0.14829272031784058\n",
            "batch: 32 loss: 0.1519031524658203\n",
            "batch: 33 loss: 0.21642780303955078\n",
            "batch: 34 loss: 0.18747524917125702\n",
            "batch: 35 loss: 0.1675017774105072\n",
            "batch: 36 loss: 0.13775089383125305\n",
            "batch: 37 loss: 0.13930925726890564\n",
            "batch: 38 loss: 0.1560688614845276\n",
            "batch: 39 loss: 0.19175389409065247\n",
            "batch: 40 loss: 0.17783501744270325\n",
            "batch: 41 loss: 0.16176185011863708\n",
            "batch: 42 loss: 0.15820695459842682\n",
            "batch: 43 loss: 0.17273956537246704\n",
            "batch: 44 loss: 0.18174222111701965\n",
            "batch: 45 loss: 0.1622086614370346\n",
            "batch: 46 loss: 0.19081085920333862\n",
            "batch: 47 loss: 0.16873079538345337\n",
            "batch: 48 loss: 0.16646453738212585\n",
            "batch: 49 loss: 0.15526524186134338\n",
            "batch: 50 loss: 0.16963377594947815\n",
            "batch: 51 loss: 0.18094855546951294\n",
            "batch: 52 loss: 0.1841300129890442\n",
            "batch: 53 loss: 0.1537407785654068\n",
            "batch: 54 loss: 0.191870778799057\n",
            "batch: 55 loss: 0.15356087684631348\n",
            "batch: 56 loss: 0.1487046480178833\n",
            "batch: 57 loss: 0.16788248717784882\n",
            "batch: 58 loss: 0.22193461656570435\n",
            "batch: 59 loss: 0.17825230956077576\n",
            "batch: 60 loss: 0.1741504967212677\n",
            "batch: 61 loss: 0.1849488765001297\n",
            "batch: 62 loss: 0.19664113223552704\n",
            "batch: 63 loss: 0.155714213848114\n",
            "batch: 64 loss: 0.19692352414131165\n",
            "batch: 65 loss: 0.1574026644229889\n",
            "batch: 66 loss: 0.16643521189689636\n",
            "batch: 67 loss: 0.14909139275550842\n",
            "batch: 68 loss: 0.17336846888065338\n",
            "batch: 69 loss: 0.16689756512641907\n",
            "batch: 70 loss: 0.17300398647785187\n",
            "batch: 71 loss: 0.17251890897750854\n",
            "batch: 72 loss: 0.18447165191173553\n",
            "batch: 73 loss: 0.21257402002811432\n",
            "batch: 74 loss: 0.16363510489463806\n",
            "batch: 75 loss: 0.1677270233631134\n",
            "batch: 76 loss: 0.14586766064167023\n",
            "batch: 77 loss: 0.1755189597606659\n",
            "batch: 78 loss: 0.16796928644180298\n",
            "batch: 79 loss: 0.18496772646903992\n",
            "batch: 80 loss: 0.1670072078704834\n",
            "batch: 81 loss: 0.23190252482891083\n",
            "batch: 82 loss: 0.17063404619693756\n",
            "batch: 83 loss: 0.15238451957702637\n",
            "batch: 84 loss: 0.17374327778816223\n",
            "batch: 85 loss: 0.17719978094100952\n",
            "batch: 86 loss: 0.16361162066459656\n",
            "batch: 87 loss: 0.25665682554244995\n",
            "batch: 88 loss: 0.16157874464988708\n",
            "batch: 89 loss: 0.15896035730838776\n",
            "batch: 90 loss: 0.13936665654182434\n",
            "batch: 91 loss: 0.23046913743019104\n",
            "batch: 92 loss: 0.17365840077400208\n",
            "batch: 93 loss: 0.19501104950904846\n",
            "batch: 94 loss: 0.21620763838291168\n",
            "batch: 95 loss: 0.17383511364459991\n",
            "batch: 96 loss: 0.16978991031646729\n",
            "batch: 97 loss: 0.1488998830318451\n",
            "batch: 98 loss: 0.21110928058624268\n",
            "batch: 99 loss: 0.18172800540924072\n",
            "batch: 100 loss: 0.19229556620121002\n",
            "batch: 101 loss: 0.16764476895332336\n",
            "batch: 102 loss: 0.1449347585439682\n",
            "batch: 103 loss: 0.17557832598686218\n",
            "batch: 104 loss: 0.16531383991241455\n",
            "batch: 105 loss: 0.1629171520471573\n",
            "batch: 106 loss: 0.20925551652908325\n",
            "batch: 107 loss: 0.20398564636707306\n",
            "batch: 108 loss: 0.23460733890533447\n",
            "batch: 109 loss: 0.19537903368473053\n",
            "batch: 110 loss: 0.18481644988059998\n",
            "batch: 111 loss: 0.16946600377559662\n",
            "batch: 112 loss: 0.17079861462116241\n",
            "batch: 113 loss: 0.22289444506168365\n",
            "batch: 114 loss: 0.15866975486278534\n",
            "batch: 115 loss: 0.17147615551948547\n",
            "batch: 116 loss: 0.2028595507144928\n",
            "batch: 117 loss: 0.18891943991184235\n",
            "batch: 118 loss: 0.14762507379055023\n",
            "batch: 119 loss: 0.20362716913223267\n",
            "batch: 120 loss: 0.21412555873394012\n",
            "batch: 121 loss: 0.17193731665611267\n",
            "batch: 122 loss: 0.18492238223552704\n",
            "batch: 123 loss: 0.16082017123699188\n",
            "batch: 124 loss: 0.16775168478488922\n",
            "batch: 125 loss: 0.23401103913784027\n",
            "batch: 126 loss: 0.1661214381456375\n",
            "batch: 127 loss: 0.19538624584674835\n",
            "batch: 128 loss: 0.14748281240463257\n",
            "batch: 129 loss: 0.23107761144638062\n",
            "batch: 130 loss: 0.1830875277519226\n",
            "batch: 131 loss: 0.16938519477844238\n",
            "batch: 132 loss: 0.14694418013095856\n",
            "batch: 133 loss: 0.17199048399925232\n",
            "batch: 134 loss: 0.2036338746547699\n",
            "batch: 135 loss: 0.1922687292098999\n",
            "batch: 136 loss: 0.15211521089076996\n",
            "batch: 137 loss: 0.1701306700706482\n",
            "batch: 138 loss: 0.17940270900726318\n",
            "batch: 139 loss: 0.1514907032251358\n",
            "batch: 140 loss: 0.16328148543834686\n",
            "batch: 141 loss: 0.16979728639125824\n",
            "batch: 142 loss: 0.20551368594169617\n",
            "batch: 143 loss: 0.1636359989643097\n",
            "batch: 144 loss: 0.14970675110816956\n",
            "batch: 145 loss: 0.22683800756931305\n",
            "batch: 146 loss: 0.22461287677288055\n",
            "LOSS train 0.17013205587863922 valid 0.2271105796098709\n",
            "EPOCH 33:\n",
            "batch: 0 loss: 0.1646532565355301\n",
            "batch: 1 loss: 0.14857247471809387\n",
            "batch: 2 loss: 0.16025421023368835\n",
            "batch: 3 loss: 0.154366597533226\n",
            "batch: 4 loss: 0.1782098412513733\n",
            "batch: 5 loss: 0.1723763644695282\n",
            "batch: 6 loss: 0.151021346449852\n",
            "batch: 7 loss: 0.14491182565689087\n",
            "batch: 8 loss: 0.162975013256073\n",
            "batch: 9 loss: 0.16815342009067535\n",
            "batch: 10 loss: 0.14786066114902496\n",
            "batch: 11 loss: 0.16740840673446655\n",
            "batch: 12 loss: 0.17050638794898987\n",
            "batch: 13 loss: 0.1716408133506775\n",
            "batch: 14 loss: 0.22088676691055298\n",
            "batch: 15 loss: 0.15269577503204346\n",
            "batch: 16 loss: 0.22402100265026093\n",
            "batch: 17 loss: 0.18441259860992432\n",
            "batch: 18 loss: 0.1677488386631012\n",
            "batch: 19 loss: 0.17270123958587646\n",
            "batch: 20 loss: 0.17253945767879486\n",
            "batch: 21 loss: 0.14872616529464722\n",
            "batch: 22 loss: 0.15519817173480988\n",
            "batch: 23 loss: 0.15299931168556213\n",
            "batch: 24 loss: 0.19312265515327454\n",
            "batch: 25 loss: 0.16842851042747498\n",
            "batch: 26 loss: 0.2409556657075882\n",
            "batch: 27 loss: 0.17331671714782715\n",
            "batch: 28 loss: 0.187675341963768\n",
            "batch: 29 loss: 0.15081378817558289\n",
            "batch: 30 loss: 0.1545226275920868\n",
            "batch: 31 loss: 0.19105416536331177\n",
            "batch: 32 loss: 0.15379884839057922\n",
            "batch: 33 loss: 0.15185534954071045\n",
            "batch: 34 loss: 0.14442074298858643\n",
            "batch: 35 loss: 0.19620338082313538\n",
            "batch: 36 loss: 0.1457499861717224\n",
            "batch: 37 loss: 0.17644037306308746\n",
            "batch: 38 loss: 0.15326355397701263\n",
            "batch: 39 loss: 0.14556466042995453\n",
            "batch: 40 loss: 0.1601831465959549\n",
            "batch: 41 loss: 0.20001013576984406\n",
            "batch: 42 loss: 0.1693340241909027\n",
            "batch: 43 loss: 0.17716261744499207\n",
            "batch: 44 loss: 0.2019207924604416\n",
            "batch: 45 loss: 0.19320271909236908\n",
            "batch: 46 loss: 0.22102586925029755\n",
            "batch: 47 loss: 0.17975571751594543\n",
            "batch: 48 loss: 0.2123173475265503\n",
            "batch: 49 loss: 0.1787835657596588\n",
            "batch: 50 loss: 0.16643935441970825\n",
            "batch: 51 loss: 0.15298257768154144\n",
            "batch: 52 loss: 0.1610046625137329\n",
            "batch: 53 loss: 0.14716783165931702\n",
            "batch: 54 loss: 0.1729280948638916\n",
            "batch: 55 loss: 0.16360124945640564\n",
            "batch: 56 loss: 0.2050783485174179\n",
            "batch: 57 loss: 0.18913359940052032\n",
            "batch: 58 loss: 0.16552840173244476\n",
            "batch: 59 loss: 0.14064902067184448\n",
            "batch: 60 loss: 0.159523606300354\n",
            "batch: 61 loss: 0.1563619077205658\n",
            "batch: 62 loss: 0.19133353233337402\n",
            "batch: 63 loss: 0.23962324857711792\n",
            "batch: 64 loss: 0.161418616771698\n",
            "batch: 65 loss: 0.2316087782382965\n",
            "batch: 66 loss: 0.139534592628479\n",
            "batch: 67 loss: 0.1588962972164154\n",
            "batch: 68 loss: 0.17963218688964844\n",
            "batch: 69 loss: 0.16756223142147064\n",
            "batch: 70 loss: 0.16714149713516235\n",
            "batch: 71 loss: 0.21979275345802307\n",
            "batch: 72 loss: 0.19553250074386597\n",
            "batch: 73 loss: 0.1606074869632721\n",
            "batch: 74 loss: 0.15981018543243408\n",
            "batch: 75 loss: 0.15569013357162476\n",
            "batch: 76 loss: 0.17392516136169434\n",
            "batch: 77 loss: 0.16986316442489624\n",
            "batch: 78 loss: 0.16024929285049438\n",
            "batch: 79 loss: 0.1363036036491394\n",
            "batch: 80 loss: 0.16164296865463257\n",
            "batch: 81 loss: 0.15404270589351654\n",
            "batch: 82 loss: 0.15430554747581482\n",
            "batch: 83 loss: 0.15050262212753296\n",
            "batch: 84 loss: 0.18263068795204163\n",
            "batch: 85 loss: 0.15179362893104553\n",
            "batch: 86 loss: 0.17725759744644165\n",
            "batch: 87 loss: 0.16809040307998657\n",
            "batch: 88 loss: 0.15809474885463715\n",
            "batch: 89 loss: 0.17924393713474274\n",
            "batch: 90 loss: 0.14775437116622925\n",
            "batch: 91 loss: 0.1573580503463745\n",
            "batch: 92 loss: 0.19477258622646332\n",
            "batch: 93 loss: 0.1712050437927246\n",
            "batch: 94 loss: 0.11789468675851822\n",
            "batch: 95 loss: 0.22925731539726257\n",
            "batch: 96 loss: 0.155609592795372\n",
            "batch: 97 loss: 0.17133821547031403\n",
            "batch: 98 loss: 0.17103537917137146\n",
            "batch: 99 loss: 0.2023911029100418\n",
            "batch: 100 loss: 0.15424717962741852\n",
            "batch: 101 loss: 0.14409559965133667\n",
            "batch: 102 loss: 0.11930447816848755\n",
            "batch: 103 loss: 0.21590720117092133\n",
            "batch: 104 loss: 0.15342068672180176\n",
            "batch: 105 loss: 0.16336452960968018\n",
            "batch: 106 loss: 0.1887531280517578\n",
            "batch: 107 loss: 0.19048374891281128\n",
            "batch: 108 loss: 0.2107982486486435\n",
            "batch: 109 loss: 0.14576582610607147\n",
            "batch: 110 loss: 0.1325618028640747\n",
            "batch: 111 loss: 0.15737490355968475\n",
            "batch: 112 loss: 0.15010133385658264\n",
            "batch: 113 loss: 0.14521750807762146\n",
            "batch: 114 loss: 0.15518978238105774\n",
            "batch: 115 loss: 0.15092909336090088\n",
            "batch: 116 loss: 0.17714142799377441\n",
            "batch: 117 loss: 0.14800287783145905\n",
            "batch: 118 loss: 0.15572881698608398\n",
            "batch: 119 loss: 0.1434306800365448\n",
            "batch: 120 loss: 0.19163529574871063\n",
            "batch: 121 loss: 0.1979183405637741\n",
            "batch: 122 loss: 0.17356792092323303\n",
            "batch: 123 loss: 0.16090604662895203\n",
            "batch: 124 loss: 0.20220893621444702\n",
            "batch: 125 loss: 0.20911265909671783\n",
            "batch: 126 loss: 0.1816566288471222\n",
            "batch: 127 loss: 0.19495369493961334\n",
            "batch: 128 loss: 0.173572838306427\n",
            "batch: 129 loss: 0.17791983485221863\n",
            "batch: 130 loss: 0.18081051111221313\n",
            "batch: 131 loss: 0.16009555757045746\n",
            "batch: 132 loss: 0.14598576724529266\n",
            "batch: 133 loss: 0.16435614228248596\n",
            "batch: 134 loss: 0.1268036812543869\n",
            "batch: 135 loss: 0.20677340030670166\n",
            "batch: 136 loss: 0.20905059576034546\n",
            "batch: 137 loss: 0.18779557943344116\n",
            "batch: 138 loss: 0.19426600635051727\n",
            "batch: 139 loss: 0.224326953291893\n",
            "batch: 140 loss: 0.18492406606674194\n",
            "batch: 141 loss: 0.20652136206626892\n",
            "batch: 142 loss: 0.19958311319351196\n",
            "batch: 143 loss: 0.17988121509552002\n",
            "batch: 144 loss: 0.17700611054897308\n",
            "batch: 145 loss: 0.17786863446235657\n",
            "batch: 146 loss: 0.2063230723142624\n",
            "LOSS train 0.18415479362010956 valid 0.24374797940254211\n",
            "EPOCH 34:\n",
            "batch: 0 loss: 0.17287790775299072\n",
            "batch: 1 loss: 0.15192115306854248\n",
            "batch: 2 loss: 0.27488452196121216\n",
            "batch: 3 loss: 0.18474525213241577\n",
            "batch: 4 loss: 0.2606332004070282\n",
            "batch: 5 loss: 0.17924094200134277\n",
            "batch: 6 loss: 0.24487297236919403\n",
            "batch: 7 loss: 0.23575472831726074\n",
            "batch: 8 loss: 0.20798738300800323\n",
            "batch: 9 loss: 0.19177472591400146\n",
            "batch: 10 loss: 0.14480477571487427\n",
            "batch: 11 loss: 0.19711509346961975\n",
            "batch: 12 loss: 0.17103451490402222\n",
            "batch: 13 loss: 0.2089272290468216\n",
            "batch: 14 loss: 0.18700721859931946\n",
            "batch: 15 loss: 0.1747354418039322\n",
            "batch: 16 loss: 0.16150206327438354\n",
            "batch: 17 loss: 0.15300101041793823\n",
            "batch: 18 loss: 0.19021227955818176\n",
            "batch: 19 loss: 0.21865762770175934\n",
            "batch: 20 loss: 0.18535801768302917\n",
            "batch: 21 loss: 0.1692863404750824\n",
            "batch: 22 loss: 0.2255256175994873\n",
            "batch: 23 loss: 0.20491760969161987\n",
            "batch: 24 loss: 0.15774866938591003\n",
            "batch: 25 loss: 0.147271528840065\n",
            "batch: 26 loss: 0.13022857904434204\n",
            "batch: 27 loss: 0.1927005648612976\n",
            "batch: 28 loss: 0.13440527021884918\n",
            "batch: 29 loss: 0.19324634969234467\n",
            "batch: 30 loss: 0.14094805717468262\n",
            "batch: 31 loss: 0.191832035779953\n",
            "batch: 32 loss: 0.19775374233722687\n",
            "batch: 33 loss: 0.19680501520633698\n",
            "batch: 34 loss: 0.19365251064300537\n",
            "batch: 35 loss: 0.16396208107471466\n",
            "batch: 36 loss: 0.20953217148780823\n",
            "batch: 37 loss: 0.16761565208435059\n",
            "batch: 38 loss: 0.20420515537261963\n",
            "batch: 39 loss: 0.18220284581184387\n",
            "batch: 40 loss: 0.1643276810646057\n",
            "batch: 41 loss: 0.18024230003356934\n",
            "batch: 42 loss: 0.19820290803909302\n",
            "batch: 43 loss: 0.19956347346305847\n",
            "batch: 44 loss: 0.15940788388252258\n",
            "batch: 45 loss: 0.1914156973361969\n",
            "batch: 46 loss: 0.17882293462753296\n",
            "batch: 47 loss: 0.21256636083126068\n",
            "batch: 48 loss: 0.22320862114429474\n",
            "batch: 49 loss: 0.14002567529678345\n",
            "batch: 50 loss: 0.16612565517425537\n",
            "batch: 51 loss: 0.19548721611499786\n",
            "batch: 52 loss: 0.157359778881073\n",
            "batch: 53 loss: 0.17687490582466125\n",
            "batch: 54 loss: 0.14512833952903748\n",
            "batch: 55 loss: 0.17994296550750732\n",
            "batch: 56 loss: 0.18658499419689178\n",
            "batch: 57 loss: 0.19729925692081451\n",
            "batch: 58 loss: 0.20300884544849396\n",
            "batch: 59 loss: 0.14490032196044922\n",
            "batch: 60 loss: 0.16406473517417908\n",
            "batch: 61 loss: 0.14514224231243134\n",
            "batch: 62 loss: 0.17346441745758057\n",
            "batch: 63 loss: 0.16740566492080688\n",
            "batch: 64 loss: 0.17418047785758972\n",
            "batch: 65 loss: 0.18048357963562012\n",
            "batch: 66 loss: 0.19344741106033325\n",
            "batch: 67 loss: 0.22217808663845062\n",
            "batch: 68 loss: 0.11965472251176834\n",
            "batch: 69 loss: 0.1641838550567627\n",
            "batch: 70 loss: 0.153333380818367\n",
            "batch: 71 loss: 0.14296963810920715\n",
            "batch: 72 loss: 0.18948936462402344\n",
            "batch: 73 loss: 0.1487773060798645\n",
            "batch: 74 loss: 0.18799760937690735\n",
            "batch: 75 loss: 0.1362297683954239\n",
            "batch: 76 loss: 0.19298560917377472\n",
            "batch: 77 loss: 0.14324352145195007\n",
            "batch: 78 loss: 0.1500527262687683\n",
            "batch: 79 loss: 0.17432260513305664\n",
            "batch: 80 loss: 0.1605537235736847\n",
            "batch: 81 loss: 0.16749626398086548\n",
            "batch: 82 loss: 0.14865821599960327\n",
            "batch: 83 loss: 0.15636564791202545\n",
            "batch: 84 loss: 0.1594807207584381\n",
            "batch: 85 loss: 0.13025276362895966\n",
            "batch: 86 loss: 0.15655694901943207\n",
            "batch: 87 loss: 0.18383467197418213\n",
            "batch: 88 loss: 0.19694261252880096\n",
            "batch: 89 loss: 0.1674002707004547\n",
            "batch: 90 loss: 0.15999716520309448\n",
            "batch: 91 loss: 0.21802093088626862\n",
            "batch: 92 loss: 0.11571528762578964\n",
            "batch: 93 loss: 0.19491460919380188\n",
            "batch: 94 loss: 0.16324523091316223\n",
            "batch: 95 loss: 0.1613527238368988\n",
            "batch: 96 loss: 0.18222692608833313\n",
            "batch: 97 loss: 0.1376633644104004\n",
            "batch: 98 loss: 0.16041699051856995\n",
            "batch: 99 loss: 0.16605520248413086\n",
            "batch: 100 loss: 0.13942842185497284\n",
            "batch: 101 loss: 0.15810754895210266\n",
            "batch: 102 loss: 0.17403754591941833\n",
            "batch: 103 loss: 0.16964328289031982\n",
            "batch: 104 loss: 0.1565566062927246\n",
            "batch: 105 loss: 0.2191537767648697\n",
            "batch: 106 loss: 0.1626141369342804\n",
            "batch: 107 loss: 0.16158926486968994\n",
            "batch: 108 loss: 0.16494464874267578\n",
            "batch: 109 loss: 0.15627479553222656\n",
            "batch: 110 loss: 0.11718649417161942\n",
            "batch: 111 loss: 0.1983797252178192\n",
            "batch: 112 loss: 0.2072000801563263\n",
            "batch: 113 loss: 0.1835346519947052\n",
            "batch: 114 loss: 0.1787334382534027\n",
            "batch: 115 loss: 0.15838466584682465\n",
            "batch: 116 loss: 0.16020116209983826\n",
            "batch: 117 loss: 0.1277567595243454\n",
            "batch: 118 loss: 0.16257551312446594\n",
            "batch: 119 loss: 0.16349084675312042\n",
            "batch: 120 loss: 0.16000330448150635\n",
            "batch: 121 loss: 0.16164934635162354\n",
            "batch: 122 loss: 0.14752238988876343\n",
            "batch: 123 loss: 0.1767984926700592\n",
            "batch: 124 loss: 0.17565235495567322\n",
            "batch: 125 loss: 0.17827185988426208\n",
            "batch: 126 loss: 0.1922861784696579\n",
            "batch: 127 loss: 0.15216001868247986\n",
            "batch: 128 loss: 0.1815171241760254\n",
            "batch: 129 loss: 0.18809324502944946\n",
            "batch: 130 loss: 0.17027385532855988\n",
            "batch: 131 loss: 0.13834306597709656\n",
            "batch: 132 loss: 0.1968504786491394\n",
            "batch: 133 loss: 0.16904211044311523\n",
            "batch: 134 loss: 0.17320892214775085\n",
            "batch: 135 loss: 0.1409134864807129\n",
            "batch: 136 loss: 0.17873810231685638\n",
            "batch: 137 loss: 0.16662856936454773\n",
            "batch: 138 loss: 0.14338132739067078\n",
            "batch: 139 loss: 0.1633809208869934\n",
            "batch: 140 loss: 0.1596055030822754\n",
            "batch: 141 loss: 0.169805109500885\n",
            "batch: 142 loss: 0.14618802070617676\n",
            "batch: 143 loss: 0.30207130312919617\n",
            "batch: 144 loss: 0.16900208592414856\n",
            "batch: 145 loss: 0.14742644131183624\n",
            "batch: 146 loss: 0.2132340669631958\n",
            "LOSS train 0.1923683136701584 valid 0.2555038332939148\n",
            "EPOCH 35:\n",
            "batch: 0 loss: 0.18190845847129822\n",
            "batch: 1 loss: 0.17476752400398254\n",
            "batch: 2 loss: 0.15162181854248047\n",
            "batch: 3 loss: 0.16882136464118958\n",
            "batch: 4 loss: 0.16805242002010345\n",
            "batch: 5 loss: 0.1420186460018158\n",
            "batch: 6 loss: 0.15823674201965332\n",
            "batch: 7 loss: 0.16805773973464966\n",
            "batch: 8 loss: 0.2058052122592926\n",
            "batch: 9 loss: 0.16843056678771973\n",
            "batch: 10 loss: 0.24016481637954712\n",
            "batch: 11 loss: 0.1645585298538208\n",
            "batch: 12 loss: 0.1667846441268921\n",
            "batch: 13 loss: 0.20459207892417908\n",
            "batch: 14 loss: 0.1763063669204712\n",
            "batch: 15 loss: 0.1650182604789734\n",
            "batch: 16 loss: 0.17592987418174744\n",
            "batch: 17 loss: 0.14310583472251892\n",
            "batch: 18 loss: 0.18110916018486023\n",
            "batch: 19 loss: 0.15374910831451416\n",
            "batch: 20 loss: 0.1306266337633133\n",
            "batch: 21 loss: 0.17122820019721985\n",
            "batch: 22 loss: 0.18312041461467743\n",
            "batch: 23 loss: 0.12845474481582642\n",
            "batch: 24 loss: 0.16050098836421967\n",
            "batch: 25 loss: 0.16185210645198822\n",
            "batch: 26 loss: 0.20786866545677185\n",
            "batch: 27 loss: 0.17721936106681824\n",
            "batch: 28 loss: 0.18292096257209778\n",
            "batch: 29 loss: 0.15496650338172913\n",
            "batch: 30 loss: 0.18884490430355072\n",
            "batch: 31 loss: 0.16339340806007385\n",
            "batch: 32 loss: 0.1822829246520996\n",
            "batch: 33 loss: 0.15088008344173431\n",
            "batch: 34 loss: 0.138217031955719\n",
            "batch: 35 loss: 0.17744088172912598\n",
            "batch: 36 loss: 0.16700293123722076\n",
            "batch: 37 loss: 0.18200945854187012\n",
            "batch: 38 loss: 0.16905272006988525\n",
            "batch: 39 loss: 0.18870310485363007\n",
            "batch: 40 loss: 0.17616862058639526\n",
            "batch: 41 loss: 0.17274683713912964\n",
            "batch: 42 loss: 0.17841306328773499\n",
            "batch: 43 loss: 0.13777662813663483\n",
            "batch: 44 loss: 0.16232889890670776\n",
            "batch: 45 loss: 0.2121032476425171\n",
            "batch: 46 loss: 0.17659808695316315\n",
            "batch: 47 loss: 0.1825552135705948\n",
            "batch: 48 loss: 0.2154444456100464\n",
            "batch: 49 loss: 0.15765058994293213\n",
            "batch: 50 loss: 0.17450860142707825\n",
            "batch: 51 loss: 0.14654256403446198\n",
            "batch: 52 loss: 0.16335725784301758\n",
            "batch: 53 loss: 0.13922086358070374\n",
            "batch: 54 loss: 0.15844492614269257\n",
            "batch: 55 loss: 0.13190233707427979\n",
            "batch: 56 loss: 0.15205451846122742\n",
            "batch: 57 loss: 0.14700326323509216\n",
            "batch: 58 loss: 0.15339131653308868\n",
            "batch: 59 loss: 0.17571958899497986\n",
            "batch: 60 loss: 0.11942687630653381\n",
            "batch: 61 loss: 0.1300729513168335\n",
            "batch: 62 loss: 0.21023207902908325\n",
            "batch: 63 loss: 0.12498407065868378\n",
            "batch: 64 loss: 0.18665428459644318\n",
            "batch: 65 loss: 0.19584280252456665\n",
            "batch: 66 loss: 0.16065029799938202\n",
            "batch: 67 loss: 0.147902712225914\n",
            "batch: 68 loss: 0.1946565806865692\n",
            "batch: 69 loss: 0.17016056180000305\n",
            "batch: 70 loss: 0.1503419280052185\n",
            "batch: 71 loss: 0.14457625150680542\n",
            "batch: 72 loss: 0.14698734879493713\n",
            "batch: 73 loss: 0.14650273323059082\n",
            "batch: 74 loss: 0.19369438290596008\n",
            "batch: 75 loss: 0.14786037802696228\n",
            "batch: 76 loss: 0.15645048022270203\n",
            "batch: 77 loss: 0.15284186601638794\n",
            "batch: 78 loss: 0.12706878781318665\n",
            "batch: 79 loss: 0.19621343910694122\n",
            "batch: 80 loss: 0.13841427862644196\n",
            "batch: 81 loss: 0.1524914801120758\n",
            "batch: 82 loss: 0.20389296114444733\n",
            "batch: 83 loss: 0.17508479952812195\n",
            "batch: 84 loss: 0.17798706889152527\n",
            "batch: 85 loss: 0.17816677689552307\n",
            "batch: 86 loss: 0.15937839448451996\n",
            "batch: 87 loss: 0.1606128066778183\n",
            "batch: 88 loss: 0.14751796424388885\n",
            "batch: 89 loss: 0.2040989249944687\n",
            "batch: 90 loss: 0.1243843361735344\n",
            "batch: 91 loss: 0.16902586817741394\n",
            "batch: 92 loss: 0.16342148184776306\n",
            "batch: 93 loss: 0.15564972162246704\n",
            "batch: 94 loss: 0.1328146755695343\n",
            "batch: 95 loss: 0.1927272230386734\n",
            "batch: 96 loss: 0.17022138833999634\n",
            "batch: 97 loss: 0.13740742206573486\n",
            "batch: 98 loss: 0.16778504848480225\n",
            "batch: 99 loss: 0.17664523422718048\n",
            "batch: 100 loss: 0.12975724041461945\n",
            "batch: 101 loss: 0.19793584942817688\n",
            "batch: 102 loss: 0.1477993130683899\n",
            "batch: 103 loss: 0.14632250368595123\n",
            "batch: 104 loss: 0.2296295315027237\n",
            "batch: 105 loss: 0.1639833152294159\n",
            "batch: 106 loss: 0.16071219742298126\n",
            "batch: 107 loss: 0.18421989679336548\n",
            "batch: 108 loss: 0.17075850069522858\n",
            "batch: 109 loss: 0.1491086333990097\n",
            "batch: 110 loss: 0.16910502314567566\n",
            "batch: 111 loss: 0.19116155803203583\n",
            "batch: 112 loss: 0.13836482167243958\n",
            "batch: 113 loss: 0.12586134672164917\n",
            "batch: 114 loss: 0.143976628780365\n",
            "batch: 115 loss: 0.16403615474700928\n",
            "batch: 116 loss: 0.21184273064136505\n",
            "batch: 117 loss: 0.18207675218582153\n",
            "batch: 118 loss: 0.15276089310646057\n",
            "batch: 119 loss: 0.14431527256965637\n",
            "batch: 120 loss: 0.15851925313472748\n",
            "batch: 121 loss: 0.17187407612800598\n",
            "batch: 122 loss: 0.16102753579616547\n",
            "batch: 123 loss: 0.15587764978408813\n",
            "batch: 124 loss: 0.1547800749540329\n",
            "batch: 125 loss: 0.16428714990615845\n",
            "batch: 126 loss: 0.20611150562763214\n",
            "batch: 127 loss: 0.13339324295520782\n",
            "batch: 128 loss: 0.12758184969425201\n",
            "batch: 129 loss: 0.1646064817905426\n",
            "batch: 130 loss: 0.18863195180892944\n",
            "batch: 131 loss: 0.18608255684375763\n",
            "batch: 132 loss: 0.15242640674114227\n",
            "batch: 133 loss: 0.1651851087808609\n",
            "batch: 134 loss: 0.17163780331611633\n",
            "batch: 135 loss: 0.15803995728492737\n",
            "batch: 136 loss: 0.19502143561840057\n",
            "batch: 137 loss: 0.15501686930656433\n",
            "batch: 138 loss: 0.12692037224769592\n",
            "batch: 139 loss: 0.14590707421302795\n",
            "batch: 140 loss: 0.14770440757274628\n",
            "batch: 141 loss: 0.23007820546627045\n",
            "batch: 142 loss: 0.1706567108631134\n",
            "batch: 143 loss: 0.15195295214653015\n",
            "batch: 144 loss: 0.14519938826560974\n",
            "batch: 145 loss: 0.1876014918088913\n",
            "batch: 146 loss: 0.17650732398033142\n",
            "LOSS train 0.16650813817977905 valid 0.22932980954647064\n",
            "EPOCH 36:\n",
            "batch: 0 loss: 0.15681470930576324\n",
            "batch: 1 loss: 0.1812904179096222\n",
            "batch: 2 loss: 0.1393679529428482\n",
            "batch: 3 loss: 0.15967179834842682\n",
            "batch: 4 loss: 0.15580272674560547\n",
            "batch: 5 loss: 0.14792846143245697\n",
            "batch: 6 loss: 0.18600037693977356\n",
            "batch: 7 loss: 0.18467435240745544\n",
            "batch: 8 loss: 0.14968541264533997\n",
            "batch: 9 loss: 0.15172620117664337\n",
            "batch: 10 loss: 0.1400943547487259\n",
            "batch: 11 loss: 0.16680875420570374\n",
            "batch: 12 loss: 0.16367705166339874\n",
            "batch: 13 loss: 0.16725105047225952\n",
            "batch: 14 loss: 0.20166300237178802\n",
            "batch: 15 loss: 0.13517968356609344\n",
            "batch: 16 loss: 0.1184161901473999\n",
            "batch: 17 loss: 0.186454638838768\n",
            "batch: 18 loss: 0.14869587123394012\n",
            "batch: 19 loss: 0.18315143883228302\n",
            "batch: 20 loss: 0.20984187722206116\n",
            "batch: 21 loss: 0.1888766586780548\n",
            "batch: 22 loss: 0.1531921625137329\n",
            "batch: 23 loss: 0.1440453827381134\n",
            "batch: 24 loss: 0.17619052529335022\n",
            "batch: 25 loss: 0.16608265042304993\n",
            "batch: 26 loss: 0.12015099078416824\n",
            "batch: 27 loss: 0.14992092549800873\n",
            "batch: 28 loss: 0.16106295585632324\n",
            "batch: 29 loss: 0.1594451367855072\n",
            "batch: 30 loss: 0.14468343555927277\n",
            "batch: 31 loss: 0.16265632212162018\n",
            "batch: 32 loss: 0.14211228489875793\n",
            "batch: 33 loss: 0.13220733404159546\n",
            "batch: 34 loss: 0.1605410873889923\n",
            "batch: 35 loss: 0.19041620194911957\n",
            "batch: 36 loss: 0.1459927260875702\n",
            "batch: 37 loss: 0.15420208871364594\n",
            "batch: 38 loss: 0.14900973439216614\n",
            "batch: 39 loss: 0.1760983169078827\n",
            "batch: 40 loss: 0.16210466623306274\n",
            "batch: 41 loss: 0.14648769795894623\n",
            "batch: 42 loss: 0.1722639501094818\n",
            "batch: 43 loss: 0.17552635073661804\n",
            "batch: 44 loss: 0.20026296377182007\n",
            "batch: 45 loss: 0.16515064239501953\n",
            "batch: 46 loss: 0.149723619222641\n",
            "batch: 47 loss: 0.13231562077999115\n",
            "batch: 48 loss: 0.13994255661964417\n",
            "batch: 49 loss: 0.14625221490859985\n",
            "batch: 50 loss: 0.1563146710395813\n",
            "batch: 51 loss: 0.14377422630786896\n",
            "batch: 52 loss: 0.19417470693588257\n",
            "batch: 53 loss: 0.12161003053188324\n",
            "batch: 54 loss: 0.1347532719373703\n",
            "batch: 55 loss: 0.12283594161272049\n",
            "batch: 56 loss: 0.16798138618469238\n",
            "batch: 57 loss: 0.14993450045585632\n",
            "batch: 58 loss: 0.15573257207870483\n",
            "batch: 59 loss: 0.13113084435462952\n",
            "batch: 60 loss: 0.16047590970993042\n",
            "batch: 61 loss: 0.13856452703475952\n",
            "batch: 62 loss: 0.1682163029909134\n",
            "batch: 63 loss: 0.1371890902519226\n",
            "batch: 64 loss: 0.15935049951076508\n",
            "batch: 65 loss: 0.15542367100715637\n",
            "batch: 66 loss: 0.14852550625801086\n",
            "batch: 67 loss: 0.16571785509586334\n",
            "batch: 68 loss: 0.18044471740722656\n",
            "batch: 69 loss: 0.17029549181461334\n",
            "batch: 70 loss: 0.1452634036540985\n",
            "batch: 71 loss: 0.16485550999641418\n",
            "batch: 72 loss: 0.16966502368450165\n",
            "batch: 73 loss: 0.13060536980628967\n",
            "batch: 74 loss: 0.13798591494560242\n",
            "batch: 75 loss: 0.1998700648546219\n",
            "batch: 76 loss: 0.17473077774047852\n",
            "batch: 77 loss: 0.14220938086509705\n",
            "batch: 78 loss: 0.12928174436092377\n",
            "batch: 79 loss: 0.1591571867465973\n",
            "batch: 80 loss: 0.12889060378074646\n",
            "batch: 81 loss: 0.15865173935890198\n",
            "batch: 82 loss: 0.1441059708595276\n",
            "batch: 83 loss: 0.1617710143327713\n",
            "batch: 84 loss: 0.13061675429344177\n",
            "batch: 85 loss: 0.15164253115653992\n",
            "batch: 86 loss: 0.1599428653717041\n",
            "batch: 87 loss: 0.15914258360862732\n",
            "batch: 88 loss: 0.19304785132408142\n",
            "batch: 89 loss: 0.15431730449199677\n",
            "batch: 90 loss: 0.13058297336101532\n",
            "batch: 91 loss: 0.16673757135868073\n",
            "batch: 92 loss: 0.16811062395572662\n",
            "batch: 93 loss: 0.20365163683891296\n",
            "batch: 94 loss: 0.15672320127487183\n",
            "batch: 95 loss: 0.16306981444358826\n",
            "batch: 96 loss: 0.13460829854011536\n",
            "batch: 97 loss: 0.21594250202178955\n",
            "batch: 98 loss: 0.19919835031032562\n",
            "batch: 99 loss: 0.15149788558483124\n",
            "batch: 100 loss: 0.12551628053188324\n",
            "batch: 101 loss: 0.17033253610134125\n",
            "batch: 102 loss: 0.18986450135707855\n",
            "batch: 103 loss: 0.1997561901807785\n",
            "batch: 104 loss: 0.16687297821044922\n",
            "batch: 105 loss: 0.14104798436164856\n",
            "batch: 106 loss: 0.17053303122520447\n",
            "batch: 107 loss: 0.1746376007795334\n",
            "batch: 108 loss: 0.15700331330299377\n",
            "batch: 109 loss: 0.14399126172065735\n",
            "batch: 110 loss: 0.15137964487075806\n",
            "batch: 111 loss: 0.17442037165164948\n",
            "batch: 112 loss: 0.1719639003276825\n",
            "batch: 113 loss: 0.17449939250946045\n",
            "batch: 114 loss: 0.16040286421775818\n",
            "batch: 115 loss: 0.17658206820487976\n",
            "batch: 116 loss: 0.13640940189361572\n",
            "batch: 117 loss: 0.17808745801448822\n",
            "batch: 118 loss: 0.15353694558143616\n",
            "batch: 119 loss: 0.1819538176059723\n",
            "batch: 120 loss: 0.13366281986236572\n",
            "batch: 121 loss: 0.14445585012435913\n",
            "batch: 122 loss: 0.1820971965789795\n",
            "batch: 123 loss: 0.13614237308502197\n",
            "batch: 124 loss: 0.23769259452819824\n",
            "batch: 125 loss: 0.16314300894737244\n",
            "batch: 126 loss: 0.1672157347202301\n",
            "batch: 127 loss: 0.1638319194316864\n",
            "batch: 128 loss: 0.2224103808403015\n",
            "batch: 129 loss: 0.181632861495018\n",
            "batch: 130 loss: 0.20334292948246002\n",
            "batch: 131 loss: 0.15153303742408752\n",
            "batch: 132 loss: 0.16158822178840637\n",
            "batch: 133 loss: 0.14245924353599548\n",
            "batch: 134 loss: 0.1575513482093811\n",
            "batch: 135 loss: 0.17160582542419434\n",
            "batch: 136 loss: 0.1800539195537567\n",
            "batch: 137 loss: 0.1728881299495697\n",
            "batch: 138 loss: 0.16671815514564514\n",
            "batch: 139 loss: 0.20069535076618195\n",
            "batch: 140 loss: 0.15323787927627563\n",
            "batch: 141 loss: 0.14732369780540466\n",
            "batch: 142 loss: 0.17589741945266724\n",
            "batch: 143 loss: 0.16736483573913574\n",
            "batch: 144 loss: 0.16988873481750488\n",
            "batch: 145 loss: 0.18207216262817383\n",
            "batch: 146 loss: 0.16091175377368927\n",
            "LOSS train 0.1519405096769333 valid 0.2176228165626526\n",
            "EPOCH 37:\n",
            "batch: 0 loss: 0.15981853008270264\n",
            "batch: 1 loss: 0.14102381467819214\n",
            "batch: 2 loss: 0.15724879503250122\n",
            "batch: 3 loss: 0.1593647003173828\n",
            "batch: 4 loss: 0.18896594643592834\n",
            "batch: 5 loss: 0.1699540615081787\n",
            "batch: 6 loss: 0.15867207944393158\n",
            "batch: 7 loss: 0.15032634139060974\n",
            "batch: 8 loss: 0.15858885645866394\n",
            "batch: 9 loss: 0.23246698081493378\n",
            "batch: 10 loss: 0.15060698986053467\n",
            "batch: 11 loss: 0.15482616424560547\n",
            "batch: 12 loss: 0.1804296374320984\n",
            "batch: 13 loss: 0.16617032885551453\n",
            "batch: 14 loss: 0.13596266508102417\n",
            "batch: 15 loss: 0.15546000003814697\n",
            "batch: 16 loss: 0.19131681323051453\n",
            "batch: 17 loss: 0.14224106073379517\n",
            "batch: 18 loss: 0.1171649768948555\n",
            "batch: 19 loss: 0.148857980966568\n",
            "batch: 20 loss: 0.1456032395362854\n",
            "batch: 21 loss: 0.13096359372138977\n",
            "batch: 22 loss: 0.14222019910812378\n",
            "batch: 23 loss: 0.16144144535064697\n",
            "batch: 24 loss: 0.1251404881477356\n",
            "batch: 25 loss: 0.1714925765991211\n",
            "batch: 26 loss: 0.183460533618927\n",
            "batch: 27 loss: 0.19042319059371948\n",
            "batch: 28 loss: 0.15116597712039948\n",
            "batch: 29 loss: 0.15020275115966797\n",
            "batch: 30 loss: 0.14007911086082458\n",
            "batch: 31 loss: 0.1580727994441986\n",
            "batch: 32 loss: 0.16683630645275116\n",
            "batch: 33 loss: 0.16794171929359436\n",
            "batch: 34 loss: 0.1457608938217163\n",
            "batch: 35 loss: 0.16602924466133118\n",
            "batch: 36 loss: 0.13343089818954468\n",
            "batch: 37 loss: 0.15880461037158966\n",
            "batch: 38 loss: 0.1576463133096695\n",
            "batch: 39 loss: 0.1727481186389923\n",
            "batch: 40 loss: 0.19227105379104614\n",
            "batch: 41 loss: 0.1520606130361557\n",
            "batch: 42 loss: 0.14742186665534973\n",
            "batch: 43 loss: 0.15212580561637878\n",
            "batch: 44 loss: 0.15750138461589813\n",
            "batch: 45 loss: 0.15715031325817108\n",
            "batch: 46 loss: 0.14010220766067505\n",
            "batch: 47 loss: 0.14268234372138977\n",
            "batch: 48 loss: 0.13681986927986145\n",
            "batch: 49 loss: 0.16657355427742004\n",
            "batch: 50 loss: 0.13774648308753967\n",
            "batch: 51 loss: 0.16508403420448303\n",
            "batch: 52 loss: 0.21594253182411194\n",
            "batch: 53 loss: 0.17334777116775513\n",
            "batch: 54 loss: 0.12096596509218216\n",
            "batch: 55 loss: 0.14842955768108368\n",
            "batch: 56 loss: 0.1766662895679474\n",
            "batch: 57 loss: 0.1468600630760193\n",
            "batch: 58 loss: 0.1598082333803177\n",
            "batch: 59 loss: 0.19092774391174316\n",
            "batch: 60 loss: 0.15238794684410095\n",
            "batch: 61 loss: 0.16982382535934448\n",
            "batch: 62 loss: 0.1976284384727478\n",
            "batch: 63 loss: 0.1477499008178711\n",
            "batch: 64 loss: 0.15288779139518738\n",
            "batch: 65 loss: 0.1466740071773529\n",
            "batch: 66 loss: 0.1731766164302826\n",
            "batch: 67 loss: 0.16222001612186432\n",
            "batch: 68 loss: 0.18457992374897003\n",
            "batch: 69 loss: 0.13727521896362305\n",
            "batch: 70 loss: 0.17834432423114777\n",
            "batch: 71 loss: 0.14466901123523712\n",
            "batch: 72 loss: 0.1266811490058899\n",
            "batch: 73 loss: 0.1783234179019928\n",
            "batch: 74 loss: 0.1401323676109314\n",
            "batch: 75 loss: 0.15777720510959625\n",
            "batch: 76 loss: 0.18402591347694397\n",
            "batch: 77 loss: 0.1580011546611786\n",
            "batch: 78 loss: 0.21468903124332428\n",
            "batch: 79 loss: 0.14107568562030792\n",
            "batch: 80 loss: 0.18265986442565918\n",
            "batch: 81 loss: 0.1437235027551651\n",
            "batch: 82 loss: 0.15004289150238037\n",
            "batch: 83 loss: 0.1526210606098175\n",
            "batch: 84 loss: 0.1512106955051422\n",
            "batch: 85 loss: 0.17816762626171112\n",
            "batch: 86 loss: 0.12006743997335434\n",
            "batch: 87 loss: 0.16654843091964722\n",
            "batch: 88 loss: 0.16949310898780823\n",
            "batch: 89 loss: 0.17233067750930786\n",
            "batch: 90 loss: 0.17648880183696747\n",
            "batch: 91 loss: 0.2199731469154358\n",
            "batch: 92 loss: 0.1305949091911316\n",
            "batch: 93 loss: 0.1516084522008896\n",
            "batch: 94 loss: 0.1332470178604126\n",
            "batch: 95 loss: 0.17752617597579956\n",
            "batch: 96 loss: 0.12702640891075134\n",
            "batch: 97 loss: 0.1933746039867401\n",
            "batch: 98 loss: 0.14467954635620117\n",
            "batch: 99 loss: 0.17556647956371307\n",
            "batch: 100 loss: 0.12823732197284698\n",
            "batch: 101 loss: 0.13592848181724548\n",
            "batch: 102 loss: 0.20948496460914612\n",
            "batch: 103 loss: 0.17498676478862762\n",
            "batch: 104 loss: 0.13989578187465668\n",
            "batch: 105 loss: 0.17061801254749298\n",
            "batch: 106 loss: 0.17293378710746765\n",
            "batch: 107 loss: 0.16008758544921875\n",
            "batch: 108 loss: 0.1730753481388092\n",
            "batch: 109 loss: 0.14562483131885529\n",
            "batch: 110 loss: 0.1593928039073944\n",
            "batch: 111 loss: 0.18068698048591614\n",
            "batch: 112 loss: 0.13089808821678162\n",
            "batch: 113 loss: 0.13574448227882385\n",
            "batch: 114 loss: 0.20863887667655945\n",
            "batch: 115 loss: 0.18289166688919067\n",
            "batch: 116 loss: 0.15907177329063416\n",
            "batch: 117 loss: 0.1330714076757431\n",
            "batch: 118 loss: 0.15039744973182678\n",
            "batch: 119 loss: 0.16459321975708008\n",
            "batch: 120 loss: 0.13595888018608093\n",
            "batch: 121 loss: 0.17932428419589996\n",
            "batch: 122 loss: 0.13394051790237427\n",
            "batch: 123 loss: 0.11776170879602432\n",
            "batch: 124 loss: 0.14016403257846832\n",
            "batch: 125 loss: 0.1380101591348648\n",
            "batch: 126 loss: 0.17413058876991272\n",
            "batch: 127 loss: 0.14893975853919983\n",
            "batch: 128 loss: 0.1251191943883896\n",
            "batch: 129 loss: 0.14921632409095764\n",
            "batch: 130 loss: 0.15041741728782654\n",
            "batch: 131 loss: 0.16209715604782104\n",
            "batch: 132 loss: 0.1674606204032898\n",
            "batch: 133 loss: 0.1579914689064026\n",
            "batch: 134 loss: 0.1432788074016571\n",
            "batch: 135 loss: 0.19102956354618073\n",
            "batch: 136 loss: 0.14269208908081055\n",
            "batch: 137 loss: 0.16892170906066895\n",
            "batch: 138 loss: 0.1470029503107071\n",
            "batch: 139 loss: 0.17083807289600372\n",
            "batch: 140 loss: 0.1761716604232788\n",
            "batch: 141 loss: 0.13714556396007538\n",
            "batch: 142 loss: 0.16011929512023926\n",
            "batch: 143 loss: 0.16125595569610596\n",
            "batch: 144 loss: 0.12234706431627274\n",
            "batch: 145 loss: 0.16174796223640442\n",
            "batch: 146 loss: 0.12306446582078934\n",
            "LOSS train 0.15938694775104523 valid 0.22701895236968994\n",
            "EPOCH 38:\n",
            "batch: 0 loss: 0.1459190547466278\n",
            "batch: 1 loss: 0.15248022973537445\n",
            "batch: 2 loss: 0.15414485335350037\n",
            "batch: 3 loss: 0.15354865789413452\n",
            "batch: 4 loss: 0.17600765824317932\n",
            "batch: 5 loss: 0.14491203427314758\n",
            "batch: 6 loss: 0.18553832173347473\n",
            "batch: 7 loss: 0.14032870531082153\n",
            "batch: 8 loss: 0.15175597369670868\n",
            "batch: 9 loss: 0.13662661612033844\n",
            "batch: 10 loss: 0.17026697099208832\n",
            "batch: 11 loss: 0.16417783498764038\n",
            "batch: 12 loss: 0.15979591012001038\n",
            "batch: 13 loss: 0.14485934376716614\n",
            "batch: 14 loss: 0.1710955798625946\n",
            "batch: 15 loss: 0.17739251255989075\n",
            "batch: 16 loss: 0.147121399641037\n",
            "batch: 17 loss: 0.14844787120819092\n",
            "batch: 18 loss: 0.14930880069732666\n",
            "batch: 19 loss: 0.1268370896577835\n",
            "batch: 20 loss: 0.13029436767101288\n",
            "batch: 21 loss: 0.12676692008972168\n",
            "batch: 22 loss: 0.14687569439411163\n",
            "batch: 23 loss: 0.169765442609787\n",
            "batch: 24 loss: 0.2083316445350647\n",
            "batch: 25 loss: 0.12190426141023636\n",
            "batch: 26 loss: 0.152462899684906\n",
            "batch: 27 loss: 0.16395673155784607\n",
            "batch: 28 loss: 0.1638861894607544\n",
            "batch: 29 loss: 0.14771369099617004\n",
            "batch: 30 loss: 0.14067311584949493\n",
            "batch: 31 loss: 0.17114081978797913\n",
            "batch: 32 loss: 0.12681543827056885\n",
            "batch: 33 loss: 0.15682248771190643\n",
            "batch: 34 loss: 0.1641739308834076\n",
            "batch: 35 loss: 0.1406400352716446\n",
            "batch: 36 loss: 0.13552062213420868\n",
            "batch: 37 loss: 0.15160956978797913\n",
            "batch: 38 loss: 0.18865622580051422\n",
            "batch: 39 loss: 0.1517508327960968\n",
            "batch: 40 loss: 0.15001311898231506\n",
            "batch: 41 loss: 0.1449904441833496\n",
            "batch: 42 loss: 0.154335618019104\n",
            "batch: 43 loss: 0.18350903689861298\n",
            "batch: 44 loss: 0.1518300175666809\n",
            "batch: 45 loss: 0.13983240723609924\n",
            "batch: 46 loss: 0.21349051594734192\n",
            "batch: 47 loss: 0.16656845808029175\n",
            "batch: 48 loss: 0.1539316475391388\n",
            "batch: 49 loss: 0.13510939478874207\n",
            "batch: 50 loss: 0.14957797527313232\n",
            "batch: 51 loss: 0.17011156678199768\n",
            "batch: 52 loss: 0.14138361811637878\n",
            "batch: 53 loss: 0.19604314863681793\n",
            "batch: 54 loss: 0.14320239424705505\n",
            "batch: 55 loss: 0.1535935401916504\n",
            "batch: 56 loss: 0.15400384366512299\n",
            "batch: 57 loss: 0.15640632808208466\n",
            "batch: 58 loss: 0.1305145025253296\n",
            "batch: 59 loss: 0.132009357213974\n",
            "batch: 60 loss: 0.16600994765758514\n",
            "batch: 61 loss: 0.15638436377048492\n",
            "batch: 62 loss: 0.13199898600578308\n",
            "batch: 63 loss: 0.15744033455848694\n",
            "batch: 64 loss: 0.1442919373512268\n",
            "batch: 65 loss: 0.14868135750293732\n",
            "batch: 66 loss: 0.13031910359859467\n",
            "batch: 67 loss: 0.19797541201114655\n",
            "batch: 68 loss: 0.15965887904167175\n",
            "batch: 69 loss: 0.1348486691713333\n",
            "batch: 70 loss: 0.19240863621234894\n",
            "batch: 71 loss: 0.1909949630498886\n",
            "batch: 72 loss: 0.16344484686851501\n",
            "batch: 73 loss: 0.15852606296539307\n",
            "batch: 74 loss: 0.1432010978460312\n",
            "batch: 75 loss: 0.16517305374145508\n",
            "batch: 76 loss: 0.11257024854421616\n",
            "batch: 77 loss: 0.14876079559326172\n",
            "batch: 78 loss: 0.17137515544891357\n",
            "batch: 79 loss: 0.16921773552894592\n",
            "batch: 80 loss: 0.15670308470726013\n",
            "batch: 81 loss: 0.18401633203029633\n",
            "batch: 82 loss: 0.13264968991279602\n",
            "batch: 83 loss: 0.195626363158226\n",
            "batch: 84 loss: 0.13327090442180634\n",
            "batch: 85 loss: 0.1725241094827652\n",
            "batch: 86 loss: 0.15883678197860718\n",
            "batch: 87 loss: 0.1483265608549118\n",
            "batch: 88 loss: 0.14865434169769287\n",
            "batch: 89 loss: 0.1538950502872467\n",
            "batch: 90 loss: 0.1327553242444992\n",
            "batch: 91 loss: 0.13145515322685242\n",
            "batch: 92 loss: 0.13222742080688477\n",
            "batch: 93 loss: 0.1458999365568161\n",
            "batch: 94 loss: 0.15760841965675354\n",
            "batch: 95 loss: 0.16526317596435547\n",
            "batch: 96 loss: 0.15070633590221405\n",
            "batch: 97 loss: 0.12501291930675507\n",
            "batch: 98 loss: 0.167874276638031\n",
            "batch: 99 loss: 0.1377389132976532\n",
            "batch: 100 loss: 0.15089045464992523\n",
            "batch: 101 loss: 0.1420060396194458\n",
            "batch: 102 loss: 0.14130085706710815\n",
            "batch: 103 loss: 0.13975927233695984\n",
            "batch: 104 loss: 0.1354922205209732\n",
            "batch: 105 loss: 0.15449199080467224\n",
            "batch: 106 loss: 0.1337636113166809\n",
            "batch: 107 loss: 0.13921532034873962\n",
            "batch: 108 loss: 0.12630105018615723\n",
            "batch: 109 loss: 0.13046444952487946\n",
            "batch: 110 loss: 0.14378415048122406\n",
            "batch: 111 loss: 0.16143354773521423\n",
            "batch: 112 loss: 0.17002427577972412\n",
            "batch: 113 loss: 0.16917283833026886\n",
            "batch: 114 loss: 0.13992491364479065\n",
            "batch: 115 loss: 0.17731143534183502\n",
            "batch: 116 loss: 0.1738683581352234\n",
            "batch: 117 loss: 0.14029741287231445\n",
            "batch: 118 loss: 0.14482735097408295\n",
            "batch: 119 loss: 0.1346064805984497\n",
            "batch: 120 loss: 0.14175331592559814\n",
            "batch: 121 loss: 0.1308722347021103\n",
            "batch: 122 loss: 0.14685142040252686\n",
            "batch: 123 loss: 0.1340397298336029\n",
            "batch: 124 loss: 0.1589013636112213\n",
            "batch: 125 loss: 0.14958682656288147\n",
            "batch: 126 loss: 0.1482643038034439\n",
            "batch: 127 loss: 0.11987607181072235\n",
            "batch: 128 loss: 0.1636810004711151\n",
            "batch: 129 loss: 0.11684291809797287\n",
            "batch: 130 loss: 0.14383503794670105\n",
            "batch: 131 loss: 0.16135573387145996\n",
            "batch: 132 loss: 0.1283569484949112\n",
            "batch: 133 loss: 0.12962311506271362\n",
            "batch: 134 loss: 0.165498286485672\n",
            "batch: 135 loss: 0.1592937409877777\n",
            "batch: 136 loss: 0.16138657927513123\n",
            "batch: 137 loss: 0.1506524682044983\n",
            "batch: 138 loss: 0.12605097889900208\n",
            "batch: 139 loss: 0.19010937213897705\n",
            "batch: 140 loss: 0.17272210121154785\n",
            "batch: 141 loss: 0.16550520062446594\n",
            "batch: 142 loss: 0.16906411945819855\n",
            "batch: 143 loss: 0.14545586705207825\n",
            "batch: 144 loss: 0.1354837417602539\n",
            "batch: 145 loss: 0.1377832293510437\n",
            "batch: 146 loss: 0.20043499767780304\n",
            "LOSS train 0.14346453547477722 valid 0.21840377151966095\n",
            "EPOCH 39:\n",
            "batch: 0 loss: 0.1545099914073944\n",
            "batch: 1 loss: 0.1698407381772995\n",
            "batch: 2 loss: 0.16886088252067566\n",
            "batch: 3 loss: 0.1457000970840454\n",
            "batch: 4 loss: 0.18778133392333984\n",
            "batch: 5 loss: 0.1714893877506256\n",
            "batch: 6 loss: 0.1456529051065445\n",
            "batch: 7 loss: 0.11145652830600739\n",
            "batch: 8 loss: 0.15395323932170868\n",
            "batch: 9 loss: 0.16875886917114258\n",
            "batch: 10 loss: 0.15683460235595703\n",
            "batch: 11 loss: 0.17667441070079803\n",
            "batch: 12 loss: 0.1691429316997528\n",
            "batch: 13 loss: 0.14259836077690125\n",
            "batch: 14 loss: 0.15200868248939514\n",
            "batch: 15 loss: 0.1816839873790741\n",
            "batch: 16 loss: 0.17370544373989105\n",
            "batch: 17 loss: 0.1519857496023178\n",
            "batch: 18 loss: 0.1631157398223877\n",
            "batch: 19 loss: 0.21279260516166687\n",
            "batch: 20 loss: 0.13257823884487152\n",
            "batch: 21 loss: 0.16225048899650574\n",
            "batch: 22 loss: 0.16310574114322662\n",
            "batch: 23 loss: 0.1690799593925476\n",
            "batch: 24 loss: 0.19816254079341888\n",
            "batch: 25 loss: 0.15569894015789032\n",
            "batch: 26 loss: 0.17745834589004517\n",
            "batch: 27 loss: 0.13461101055145264\n",
            "batch: 28 loss: 0.14661788940429688\n",
            "batch: 29 loss: 0.1617313027381897\n",
            "batch: 30 loss: 0.17796406149864197\n",
            "batch: 31 loss: 0.1365540325641632\n",
            "batch: 32 loss: 0.15771964192390442\n",
            "batch: 33 loss: 0.16881519556045532\n",
            "batch: 34 loss: 0.15493595600128174\n",
            "batch: 35 loss: 0.13421206176280975\n",
            "batch: 36 loss: 0.1441335827112198\n",
            "batch: 37 loss: 0.13550734519958496\n",
            "batch: 38 loss: 0.16101759672164917\n",
            "batch: 39 loss: 0.12006460875272751\n",
            "batch: 40 loss: 0.16680783033370972\n",
            "batch: 41 loss: 0.12372482568025589\n",
            "batch: 42 loss: 0.1582070291042328\n",
            "batch: 43 loss: 0.13248799741268158\n",
            "batch: 44 loss: 0.17236162722110748\n",
            "batch: 45 loss: 0.15578553080558777\n",
            "batch: 46 loss: 0.14939352869987488\n",
            "batch: 47 loss: 0.1325823962688446\n",
            "batch: 48 loss: 0.12797968089580536\n",
            "batch: 49 loss: 0.15134261548519135\n",
            "batch: 50 loss: 0.12622490525245667\n",
            "batch: 51 loss: 0.13744968175888062\n",
            "batch: 52 loss: 0.15697012841701508\n",
            "batch: 53 loss: 0.14831224083900452\n",
            "batch: 54 loss: 0.15988153219223022\n",
            "batch: 55 loss: 0.13274134695529938\n",
            "batch: 56 loss: 0.12672868371009827\n",
            "batch: 57 loss: 0.13590560853481293\n",
            "batch: 58 loss: 0.1352270245552063\n",
            "batch: 59 loss: 0.14852742850780487\n",
            "batch: 60 loss: 0.13583916425704956\n",
            "batch: 61 loss: 0.1578858196735382\n",
            "batch: 62 loss: 0.13417336344718933\n",
            "batch: 63 loss: 0.13210734724998474\n",
            "batch: 64 loss: 0.14389097690582275\n",
            "batch: 65 loss: 0.2093656212091446\n",
            "batch: 66 loss: 0.13610152900218964\n",
            "batch: 67 loss: 0.1637793630361557\n",
            "batch: 68 loss: 0.1256001591682434\n",
            "batch: 69 loss: 0.15594248473644257\n",
            "batch: 70 loss: 0.11178956180810928\n",
            "batch: 71 loss: 0.1660555750131607\n",
            "batch: 72 loss: 0.16549080610275269\n",
            "batch: 73 loss: 0.1828186810016632\n",
            "batch: 74 loss: 0.13720276951789856\n",
            "batch: 75 loss: 0.14464178681373596\n",
            "batch: 76 loss: 0.19805054366588593\n",
            "batch: 77 loss: 0.12285222113132477\n",
            "batch: 78 loss: 0.15587742626667023\n",
            "batch: 79 loss: 0.13311418890953064\n",
            "batch: 80 loss: 0.14841043949127197\n",
            "batch: 81 loss: 0.1496155560016632\n",
            "batch: 82 loss: 0.11203012615442276\n",
            "batch: 83 loss: 0.12892299890518188\n",
            "batch: 84 loss: 0.12189839035272598\n",
            "batch: 85 loss: 0.17701773345470428\n",
            "batch: 86 loss: 0.22324147820472717\n",
            "batch: 87 loss: 0.13675764203071594\n",
            "batch: 88 loss: 0.14701196551322937\n",
            "batch: 89 loss: 0.1509419083595276\n",
            "batch: 90 loss: 0.14215224981307983\n",
            "batch: 91 loss: 0.16956627368927002\n",
            "batch: 92 loss: 0.1714896857738495\n",
            "batch: 93 loss: 0.16397199034690857\n",
            "batch: 94 loss: 0.14296872913837433\n",
            "batch: 95 loss: 0.16190144419670105\n",
            "batch: 96 loss: 0.13889075815677643\n",
            "batch: 97 loss: 0.14041131734848022\n",
            "batch: 98 loss: 0.147212952375412\n",
            "batch: 99 loss: 0.17322459816932678\n",
            "batch: 100 loss: 0.13725310564041138\n",
            "batch: 101 loss: 0.1363222450017929\n",
            "batch: 102 loss: 0.10574799031019211\n",
            "batch: 103 loss: 0.17047110199928284\n",
            "batch: 104 loss: 0.1762102246284485\n",
            "batch: 105 loss: 0.14661544561386108\n",
            "batch: 106 loss: 0.15239059925079346\n",
            "batch: 107 loss: 0.1253657042980194\n",
            "batch: 108 loss: 0.1493317037820816\n",
            "batch: 109 loss: 0.18998457491397858\n",
            "batch: 110 loss: 0.14931955933570862\n",
            "batch: 111 loss: 0.14570873975753784\n",
            "batch: 112 loss: 0.14778262376785278\n",
            "batch: 113 loss: 0.13905884325504303\n",
            "batch: 114 loss: 0.14729323983192444\n",
            "batch: 115 loss: 0.15801513195037842\n",
            "batch: 116 loss: 0.1791885495185852\n",
            "batch: 117 loss: 0.11378733813762665\n",
            "batch: 118 loss: 0.1455087661743164\n",
            "batch: 119 loss: 0.17555153369903564\n",
            "batch: 120 loss: 0.12345010787248611\n",
            "batch: 121 loss: 0.13630275428295135\n",
            "batch: 122 loss: 0.15592172741889954\n",
            "batch: 123 loss: 0.13321790099143982\n",
            "batch: 124 loss: 0.16469110548496246\n",
            "batch: 125 loss: 0.15648005902767181\n",
            "batch: 126 loss: 0.14325293898582458\n",
            "batch: 127 loss: 0.13637857139110565\n",
            "batch: 128 loss: 0.12656602263450623\n",
            "batch: 129 loss: 0.1576032042503357\n",
            "batch: 130 loss: 0.17220231890678406\n",
            "batch: 131 loss: 0.14514321088790894\n",
            "batch: 132 loss: 0.16332398355007172\n",
            "batch: 133 loss: 0.14960341155529022\n",
            "batch: 134 loss: 0.18198102712631226\n",
            "batch: 135 loss: 0.14490199089050293\n",
            "batch: 136 loss: 0.16499590873718262\n",
            "batch: 137 loss: 0.16077552735805511\n",
            "batch: 138 loss: 0.17465469241142273\n",
            "batch: 139 loss: 0.14453086256980896\n",
            "batch: 140 loss: 0.17121192812919617\n",
            "batch: 141 loss: 0.15639181435108185\n",
            "batch: 142 loss: 0.16512936353683472\n",
            "batch: 143 loss: 0.2037847936153412\n",
            "batch: 144 loss: 0.15526732802391052\n",
            "batch: 145 loss: 0.1448906660079956\n",
            "batch: 146 loss: 0.1403418928384781\n",
            "LOSS train 0.14538253843784332 valid 0.21808089315891266\n"
          ]
        }
      ],
      "source": [
        "from TrainUnet import train, loss_fn_unet\n",
        "\n",
        "# training\n",
        "lr = 0.001 \n",
        "optimizer = torch.optim.Adam(unet_model.parameters(), lr=lr)\n",
        "\n",
        "loss_fn = loss_fn_unet\n",
        "num_of_epochs = 40\n",
        "\n",
        "wandb.init(\n",
        "    project=\"cv-dl\", \n",
        "    config={\n",
        "        \"epochs\": num_of_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr\n",
        "    }\n",
        ")\n",
        "train_losses, test_losses = train(unet_model, num_of_epochs, train_dataloader, test_dataloader, optimizer, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# show loss curve\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(range(0, num_of_epochs), train_losses, label='Train loss')\n",
        "plt.plot(range(0, num_of_epochs), test_losses, label='Test loss')\n",
        "plt.title(\"Loss curve\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import random\n",
        "imageId = random.randint(0, len(test_data)-1)\n",
        "image, truth = test_data.__getitem__(imageId)\n",
        "\n",
        "pred = unet_model(image.unsqueeze(0)).cpu().detach()[0]\n",
        "image = image.cpu()\n",
        "truth = truth.cpu()\n",
        "\n",
        "figure = plt.figure(figsize=(10, 10))\n",
        "\n",
        "figure.add_subplot(1, 3, 1)\n",
        "plt.title('image')\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image.numpy().transpose(1, 2, 0))\n",
        "\n",
        "figure.add_subplot(1, 3, 2)\n",
        "plt.title('seg_pred')\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(pred.numpy().transpose(1, 2, 0), cmap=\"gray\")\n",
        "\n",
        "figure.add_subplot(1, 3, 3)\n",
        "plt.title('seg_true')\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(truth.numpy().transpose(1, 2, 0), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23OxRSYSmznF"
      },
      "source": [
        "## Evaluation metrics\n",
        "\n",
        "Finally, you will implement several segmentation metrics to evaluate the model you've just trained. As usual, try to implement these metrics without using any for loops.\n",
        "\n",
        "In the remainder of this section we'll use the following notation:\n",
        "- $n_{ij}$ - the total number of pixels classified to class\n",
        "j but actually belonging to class i; $i, j \\in 1, .., C$;\n",
        "- $t_i = \\sum_{j = 1}^{C} n_{ij}$ - the total number of pixels belonging to class $i$ (in the ground truth segmentation mask);\n",
        "- $C$ - the total number of classes in the segmentation problem.\n",
        "\n",
        "### Mean pixel accuracy\n",
        "\n",
        "Pixel accuracy is the simplest image segmentation metric; it is defined as the percentage of pixels that were correctly classified by the model.\n",
        "\n",
        "\\begin{equation}\n",
        "p_a = \\frac{1}{C} \\frac{\\sum_{i}^{C} n_{ii}}{\\sum_{i}^{C} t_i}\n",
        "\\end{equation}\n",
        "\n",
        "This metric is not that relevant for class imbalanced problems (which occurs for most segmentation problems).\n",
        "\n",
        "### Intersection over Union (IoU)\n",
        "\n",
        "the intersection over union metric is defined as the ratio between the area of intersection and the area of union (between the predicted segmentation mask and the ground truth segmentation mask of a single class).\n",
        "In case of a multi-class segmentation problem, we simple average the IoUs over all the classes. This metric is called mean Intersection over Union (mIou).\n",
        "\n",
        "\\begin{equation}\n",
        "mIoU = \\frac{1}{C} \\sum_{i = 1}^{C} \\frac{n_{ii}}{t_i - n_{ii} + \\sum_{j = 1}^{C} n_{ji}}\n",
        "\\end{equation}\n",
        "\n",
        "The ideal value for this metric is 1; usually values lower than 0.6 indicate a very bad performance.\n",
        "\n",
        "### Frequency Weighted Intersection over Union\n",
        "\n",
        "The frequency weighted over union metric is similar to mean IoU, but the values are weighted with the adequate frequencies of the pixels.\n",
        "\n",
        "\\begin{equation}\n",
        "fIou = (\\sum_{i = 1}^{k} t_i)^{-1}   \\sum_{i = 1}^{C} t_i \\cdot \\frac{n_{ii}}{t_i - n_{ii} + \\sum_{j = 1}^{C} n_{ji}}\n",
        "\\end{equation}\n",
        "\n",
        "The values of this metric lie in the interval [0, 1], and the ideal value for this metric is 1.\n",
        "\n",
        "Compute and report these metrics for your trained model(s).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mpa = 0.3155818283557892, iou = 0.33333333333333276, fwiou = 0.8988761769137166\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>test_loss</td><td>â–‡â–…â–…â–„â–…â–ƒâ–ƒâ–ƒâ–ˆâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_loss</td><td>â–‡â–…â–…â–…â–…â–„â–„â–„â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>fwiou</td><td>0.89888</td></tr><tr><td>iou</td><td>0.33333</td></tr><tr><td>mpa</td><td>0.31558</td></tr><tr><td>test_loss</td><td>0.21808</td></tr><tr><td>train_loss</td><td>0.14538</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rich-hill-22</strong> at: <a href='https://wandb.ai/deiubejan/cv-dl/runs/l5gaf73s' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl/runs/l5gaf73s</a><br/>Synced 5 W&B file(s), 1 media file(s), 43 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240110_234451-l5gaf73s\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from UnetEval import model_eval\n",
        "\n",
        "model_eval(unet_model, test_dataloader)\n",
        "torch.save(unet_model, 'saved_model.pth')\n",
        "\n",
        "# artifact = wandb.Artifact('saved_model', type='model')\n",
        "# artifact.add_file('saved_model.pth')\n",
        "# wandb.log_artifact(artifact)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUXod31hAyFI"
      },
      "source": [
        "# Wandb\n",
        "\n",
        "\n",
        "This time you will use [wandb](https://wandb.ai/) to track your experiments and perform hyperparameter search.\n",
        "\n",
        "\n",
        "1. Log the loss and several metrics after each epoch: use ``wandb.log()`` method within your training loop after each epoch to log the loss and at least two metrics on the train and the validation dataset.\n",
        "\n",
        "\n",
        "2. Use a ``wandb.Table`` to visualize the predictions on the validation dataset across the training process. You can find more details [here](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/datasets-predictions/W%26B_Tables_Quickstart.ipynb#scrollTo=tbOiat0mrWA2).\n",
        "\n",
        "\n",
        "3. [Artifacts](https://docs.wandb.ai/guides/artifacts/construct-an-artifact) are used to track and version any serialized data as the inputs and outputs.\n",
        "\n",
        "\n",
        "\"W&B Artifacts was designed to make it effortless to version your datasets and models, regardless of whether you want to store your files with W&B or whether you already have a bucket you want W&B to track. Once you've tracked your datasets or model files, W&B will automatically log each and every modification, giving you a complete and auditable history of changes to your files.\"\n",
        "\n",
        "\n",
        "Create a class ModelCheckpoint which will be responsible for tracking the best N checkpoints across the training process. This class will monitor a metric, and if the value for that metric is higher/lower (depending on whether the ) than the current max/min, it will save a checkpoint of the model.\n",
        "Also, you should ensure that at a given time only N checkpoints are being saved. So, if the number of saved checkpoints is greater than N, you should perform a cleanup.\n",
        "\n",
        "\n",
        "```\n",
        "    def __call__(self, model, epoch, metric_val):\n",
        "        must_save = metric_val < self.best_metric_val if self.decreasing_metric else metric_val > self.best_metric_val\n",
        "        if must_save:\n",
        "            self.best_metric_val = metric_val\n",
        "            # TODO use torch.save to save the model\n",
        "            # TODO use the function below to log the model artifact\n",
        "       \n",
        "        # TODO if needed, perform cleanup\n",
        "   \n",
        "    def write_artifact(self, path, model_path, metric_val):\n",
        "        artifact = wandb.Artifact(filename, type='model', metadata={'metric': metric_val})\n",
        "        artifact.add_file(model_path)\n",
        "        wandb.run.log_artifact(artifact)    \n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "4. You will also use [wandb sweeps](https://docs.wandb.ai/guides/sweeps) to automate hyperparameter tuning. This module explores different combinations of hyperparameters to help users find the best configuration for their models.\n",
        "\n",
        "[Here](https://www.youtube.com/watch?v=9zrmUIlScdY&ab_channel=Weights%26Biases) you can find a video tutorial on how you can use sweeps in wandb and [here](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb) the corresponding colab.\n",
        "\n",
        "\n",
        "Random Search is a hyperparameter optimization technique that involves randomly sampling hyperparameter values within specified ranges or distributions. It explores the hyperparameter space by selecting combinations randomly, allowing for a broad search across various configurations without following a specific pattern or grid.\n",
        "\n",
        "\n",
        "Grid Search is a method where hyperparameter values are exhaustively tested across a predefined grid or set of values. It evaluates the model's performance for each combination of hyperparameters within the specified ranges, systematically covering the entire search space to find the optimal configuration.\n",
        "\n",
        "\n",
        "Perform hyperparameter search using wandb sweeps (more details [here](https://wandb.ai/wandb_fc/articles/reports/Running-Hyperparameter-Sweeps-to-Pick-the-Best-Model--Vmlldzo1NDQ0OTIy)) for the learning rate and at least one other parameter.\n",
        "- first perform a random search as a preliminary exploration to identify \"promising\" values for your hyperparameters;\n",
        "- then apply a grid search on this range, to perform a more focused investigation into promising regions for finer optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbejan-andrei-personal\u001b[0m (\u001b[33mdeiubejan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "}\n",
        "\n",
        "metric = {\n",
        "    'name': 'train_loss',\n",
        "    'goal': 'minimize'\n",
        "}\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        'values': ['adam', 'sgd']\n",
        "    },\n",
        "    'learning_rate': {\n",
        "        'distribution': 'uniform', \n",
        "        'min': 0.0001,\n",
        "        'max': 0.01\n",
        "    }\n",
        "}\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "parameters_dict.update({\n",
        "    'epochs': {\n",
        "        'value': 3\n",
        "    }\n",
        "})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: k2t1gctr\n",
            "Sweep URL: https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr\n"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='cv-dl-sweep3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_sweep():\n",
        "    wandb.init()\n",
        "    config = wandb.config\n",
        "\n",
        "    num_classes = 3\n",
        "    encoder_channels = [3, 64, 128, 256, 512, 1024]\n",
        "    decoder_channels = [1024, 512, 256, 128, 64]\n",
        "\n",
        "    model = MyUNet(num_classes, encoder_channels, decoder_channels)\n",
        "    model.to('cuda')\n",
        "\n",
        "    lr = config.learning_rate\n",
        "    if config.optimizer == \"sgd\":\n",
        "        optimizer = torch.optim.SGD(model.parameters(),\n",
        "                              lr=lr, momentum=0.9)\n",
        "    elif config.optimizer == \"adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(),\n",
        "                               lr=lr)\n",
        "    num_epochs = config.epochs\n",
        "\n",
        "    loss_fn = loss_fn_unet\n",
        "    train_losses, test_losses = train(unet_model, num_epochs, train_dataloader, test_dataloader, optimizer, loss_fn)\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0dkbwcmd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.002267430079193743\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>d:\\ML\\cv_dl\\wandb\\run-20240110_210303-0dkbwcmd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/0dkbwcmd' target=\"_blank\">valiant-sweep-1</a></strong> to <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/0dkbwcmd' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/0dkbwcmd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0:\n",
            "batch: 0 loss: 1.3681957721710205\n",
            "batch: 1 loss: 1.2198683023452759\n",
            "LOSS train 1.3034272193908691 valid 1.2366719245910645\n",
            "EPOCH 1:\n",
            "batch: 0 loss: 1.3081495761871338\n",
            "batch: 1 loss: 1.2883977890014648\n",
            "LOSS train 1.307039737701416 valid 1.2366671562194824\n",
            "EPOCH 2:\n",
            "batch: 0 loss: 1.3531968593597412\n",
            "batch: 1 loss: 1.2370600700378418\n",
            "LOSS train 1.299464225769043 valid 1.236671805381775\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–…â–ˆ</td></tr><tr><td>test_loss</td><td>â–ˆâ–â–ˆ</td></tr><tr><td>train_loss</td><td>â–…â–ˆâ–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_loss</td><td>1.23667</td></tr><tr><td>train_loss</td><td>1.29946</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">valiant-sweep-1</strong> at: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/0dkbwcmd' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/0dkbwcmd</a><br/>Synced 5 W&B file(s), 1 media file(s), 6 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240110_210303-0dkbwcmd\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4j1xcmpa with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.006014908978829932\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>d:\\ML\\cv_dl\\wandb\\run-20240110_210334-4j1xcmpa</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/4j1xcmpa' target=\"_blank\">crimson-sweep-2</a></strong> to <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/4j1xcmpa' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/4j1xcmpa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0:\n",
            "batch: 0 loss: 1.3106142282485962\n",
            "batch: 1 loss: 1.2856006622314453\n",
            "LOSS train 1.293941617012024 valid 1.236670732498169\n",
            "EPOCH 1:\n",
            "batch: 0 loss: 1.317772388458252\n",
            "batch: 1 loss: 1.2774510383605957\n",
            "LOSS train 1.2982491254806519 valid 1.2366726398468018\n",
            "EPOCH 2:\n",
            "batch: 0 loss: 1.2640273571014404\n",
            "batch: 1 loss: 1.338935375213623\n",
            "LOSS train 1.300243616104126 valid 1.2366694211959839\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–…â–ˆ</td></tr><tr><td>test_loss</td><td>â–„â–ˆâ–</td></tr><tr><td>train_loss</td><td>â–â–†â–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_loss</td><td>1.23667</td></tr><tr><td>train_loss</td><td>1.30024</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">crimson-sweep-2</strong> at: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/4j1xcmpa' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/4j1xcmpa</a><br/>Synced 5 W&B file(s), 1 media file(s), 6 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240110_210334-4j1xcmpa\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rxwveuhp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00521485675687738\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>d:\\ML\\cv_dl\\wandb\\run-20240110_210405-rxwveuhp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/rxwveuhp' target=\"_blank\">vital-sweep-3</a></strong> to <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/rxwveuhp' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/rxwveuhp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0:\n",
            "batch: 0 loss: 1.3092992305755615\n",
            "batch: 1 loss: 1.2870924472808838\n",
            "LOSS train 1.2983554601669312 valid 1.2366671562194824\n",
            "EPOCH 1:\n",
            "batch: 0 loss: 1.3761078119277954\n",
            "batch: 1 loss: 1.210420846939087\n",
            "LOSS train 1.3003973960876465 valid 1.2366628646850586\n",
            "EPOCH 2:\n",
            "batch: 0 loss: 1.3242220878601074\n",
            "batch: 1 loss: 1.26996910572052\n",
            "LOSS train 1.2988919019699097 valid 1.2366611957550049\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–…â–ˆ</td></tr><tr><td>test_loss</td><td>â–ˆâ–ƒâ–</td></tr><tr><td>train_loss</td><td>â–â–ˆâ–ƒ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_loss</td><td>1.23666</td></tr><tr><td>train_loss</td><td>1.29889</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vital-sweep-3</strong> at: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/rxwveuhp' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/rxwveuhp</a><br/>Synced 5 W&B file(s), 1 media file(s), 6 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240110_210405-rxwveuhp\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sxxuqg2t with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00624316223742304\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>d:\\ML\\cv_dl\\wandb\\run-20240110_210437-sxxuqg2t</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/sxxuqg2t' target=\"_blank\">deep-sweep-4</a></strong> to <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/sxxuqg2t' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/sxxuqg2t</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0:\n",
            "batch: 0 loss: 1.263850450515747\n",
            "batch: 1 loss: 1.3392689228057861\n",
            "LOSS train 1.2971761226654053 valid 1.2366572618484497\n",
            "EPOCH 1:\n",
            "batch: 0 loss: 1.3317407369613647\n",
            "batch: 1 loss: 1.2614479064941406\n",
            "LOSS train 1.299228549003601 valid 1.236659288406372\n",
            "EPOCH 2:\n",
            "batch: 0 loss: 1.3284153938293457\n",
            "batch: 1 loss: 1.2652504444122314\n",
            "LOSS train 1.2985496520996094 valid 1.2366623878479004\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–…â–ˆ</td></tr><tr><td>test_loss</td><td>â–â–„â–ˆ</td></tr><tr><td>train_loss</td><td>â–â–ˆâ–†</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_loss</td><td>1.23666</td></tr><tr><td>train_loss</td><td>1.29855</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deep-sweep-4</strong> at: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/sxxuqg2t' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/sxxuqg2t</a><br/>Synced 5 W&B file(s), 1 media file(s), 6 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240110_210437-sxxuqg2t\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mlmski1d with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.006595205479649615\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>d:\\ML\\cv_dl\\wandb\\run-20240110_210508-mlmski1d</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/mlmski1d' target=\"_blank\">royal-sweep-5</a></strong> to <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/sweeps/k2t1gctr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/mlmski1d' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/mlmski1d</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0:\n",
            "batch: 0 loss: 1.3867543935775757\n",
            "batch: 1 loss: 1.198582649230957\n",
            "LOSS train 1.3069093227386475 valid 1.2366628646850586\n",
            "EPOCH 1:\n",
            "batch: 0 loss: 1.26625657081604\n",
            "batch: 1 loss: 1.3358538150787354\n",
            "LOSS train 1.3039778470993042 valid 1.2366749048233032\n",
            "EPOCH 2:\n",
            "batch: 0 loss: 1.3358216285705566\n",
            "batch: 1 loss: 1.2568244934082031\n",
            "LOSS train 1.3053905963897705 valid 1.2366752624511719\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–…â–ˆ</td></tr><tr><td>test_loss</td><td>â–â–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–â–„</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_loss</td><td>1.23668</td></tr><tr><td>train_loss</td><td>1.30539</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">royal-sweep-5</strong> at: <a href='https://wandb.ai/deiubejan/cv-dl-sweep3/runs/mlmski1d' target=\"_blank\">https://wandb.ai/deiubejan/cv-dl-sweep3/runs/mlmski1d</a><br/>Synced 5 W&B file(s), 1 media file(s), 6 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240110_210508-mlmski1d\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.agent(sweep_id, train_sweep, count = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### FOR LAB 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.load('saved_model.pth')\n",
        "\n",
        "scripted_model = torch.jit.script(unet_model)\n",
        "scripted_model.save(\"unet_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "stupid_model.eval()\n",
        "\n",
        "# Create a sample input tensor (change according to your model's input requirements)\n",
        "example_input = torch.randn(1, 3, 250, 250)\n",
        "\n",
        "# Script the model\n",
        "scripted_model = torch.jit.script(stupid_model)\n",
        "\n",
        "# Save the scripted model to a file\n",
        "scripted_model.save(\"scripted_my_stupid_model.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
